{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da351cd-3355-45e9-b7f5-33f6f74233b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:42.482687Z",
     "iopub.status.busy": "2022-12-06T05:49:42.482494Z",
     "iopub.status.idle": "2022-12-06T05:49:42.491023Z",
     "shell.execute_reply": "2022-12-06T05:49:42.490298Z",
     "shell.execute_reply.started": "2022-12-06T05:49:42.482637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"CompuBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34544118-caec-4db6-99b1-2134a44f81d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:42.492887Z",
     "iopub.status.busy": "2022-12-06T05:49:42.492507Z",
     "iopub.status.idle": "2022-12-06T05:49:42.790122Z",
     "shell.execute_reply": "2022-12-06T05:49:42.789328Z",
     "shell.execute_reply.started": "2022-12-06T05:49:42.492868Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First you have to install the modified version of sentence-transformers\n",
    "# You can find this version if you git clone the CompuBERT Repo\n",
    "# https://github.com/MIR-MU/CompuBERT.git\n",
    "# All credit goes to MIR-MU team that developed CompuBERT\n",
    "\n",
    "# First you must install these packages (CompuBERT uses Tensorboard if wanted)\n",
    "# !pip install lxml annoy pathos gensim==3.4.0\n",
    "# !pip install git+https://github.com/hbldh/xmlr\n",
    "\n",
    "# This package also relies on a custom package arqmath_eval\n",
    "# Download the package via git below\n",
    "# !pip install --force-reinstall git+https://github.com/MIR-MU/ARQMath-eval\n",
    "\n",
    "# Uncomment these lines below to download the modified package\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# RESTART KERNEL AFTER INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc6b9e-4a4f-4474-908a-d205be0c0f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:42.794058Z",
     "iopub.status.busy": "2022-12-06T05:49:42.793673Z",
     "iopub.status.idle": "2022-12-06T05:49:48.796870Z",
     "shell.execute_reply": "2022-12-06T05:49:48.796113Z",
     "shell.execute_reply.started": "2022-12-06T05:49:42.794038Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models, losses, SentencesDataset\n",
    "from ARQMathCode.post_reader_record import DataReaderRecord\n",
    "from scipy.stats import zscore\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from question_answer.utils import examples_from_questions_tup\n",
    "from question_answer.utils import dataloader_from_examples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from preproc.question_answer.blank_substituer import BlankSubstituer\n",
    "import pickle\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from ArqmathEvaluator import ArqmathEvaluator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import MPNetModel, AutoModel, MPNetTokenizerFast\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments\n",
    "from transformers import DefaultDataCollator, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AdamW\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from datasets import load_metric\n",
    "from pytorch_optimizer import Nero, Adan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23362956-473d-4ab8-b9f7-1af65b6e6215",
   "metadata": {},
   "source": [
    "### Process Data Records from XML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f94dbb-416e-4065-9cd4-6e9b0221b3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:48.798960Z",
     "iopub.status.busy": "2022-12-06T05:49:48.798532Z",
     "iopub.status.idle": "2022-12-06T05:49:48.802425Z",
     "shell.execute_reply": "2022-12-06T05:49:48.801699Z",
     "shell.execute_reply.started": "2022-12-06T05:49:48.798940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ARQMath provides a function to read the XML files\n",
    "# This code runs the provided DataReaderRecord\n",
    "# Data from XML is stored in a DataReaderRecord object\n",
    "# Data is automatically loaded based on what files are found in the folder\n",
    "# Please use the CompuBERT augmented code for the ARQMath dataset\n",
    "# Found in their github repo: https://github.com/MIR-MU/CompuBERT/\n",
    "\n",
    "# # UNCOMMENT THIS CODE TO RUN\n",
    "# data_path = \"./data\"\n",
    "# data_records = DataReaderRecord(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e2be9b-d85c-41b7-9e22-d843a2b8cd12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:48.803616Z",
     "iopub.status.busy": "2022-12-06T05:49:48.803293Z",
     "iopub.status.idle": "2022-12-06T05:49:49.157142Z",
     "shell.execute_reply": "2022-12-06T05:49:49.156399Z",
     "shell.execute_reply.started": "2022-12-06T05:49:48.803597Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_records.post_parser.map_questions[5].body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37540a9-1c25-42d6-a2c1-ba7f8f7f127e",
   "metadata": {},
   "source": [
    "### Process Posts into Q+A Pairs and Apply Stemming/Pre-Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bf6f35-a11b-4f0f-adca-28b06cc7e4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:49.158361Z",
     "iopub.status.busy": "2022-12-06T05:49:49.158043Z",
     "iopub.status.idle": "2022-12-06T05:49:49.341677Z",
     "shell.execute_reply": "2022-12-06T05:49:49.340913Z",
     "shell.execute_reply.started": "2022-12-06T05:49:49.158344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcess Description: \\nThe Blank Substituter removes the <math> and other html tags from the text\\nThe postprocessor is found in the CompuBERT module.\\n\\nThis function call splits the data_records and preprocesses the text.\\nIt also creates a label or weight for each q+a pair.\\nThe weight is calculated via a normalized score based on the number of up\\nand down votes the answer received.\\n\\nNOTE: One change was made to the post processor, the labels had to be\\ncoerced into a float \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process Description: \n",
    "The Blank Substituter removes the <math> and other html tags from the text\n",
    "The postprocessor is found in the CompuBERT module.\n",
    "\n",
    "This function call splits the data_records and preprocesses the text.\n",
    "It also creates a label or weight for each q+a pair.\n",
    "The weight is calculated via a normalized score based on the number of up\n",
    "and down votes the answer received.\n",
    "\n",
    "NOTE: One change was made to the post processor, the labels had to be\n",
    "coerced into a float \n",
    "\"\"\"\n",
    "\n",
    "# # UNCOMMENT THIS CODE TO RUN\n",
    "# postprocessor = BlankSubstituer()\n",
    "# postproc_parser = postprocessor.process_parser(data_records.post_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787c486-a21e-4d52-88cc-af7c25b52d50",
   "metadata": {},
   "source": [
    "### Load pre-saved Data (Pickled)\n",
    "\n",
    "Simply for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4535638c-87cb-48b2-869f-91968884cdb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:49.342786Z",
     "iopub.status.busy": "2022-12-06T05:49:49.342570Z",
     "iopub.status.idle": "2022-12-06T05:49:49.588055Z",
     "shell.execute_reply": "2022-12-06T05:49:49.587215Z",
     "shell.execute_reply.started": "2022-12-06T05:49:49.342767Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('postprocessor.pickle', 'wb') as handle:\n",
    "#     _=pickle.dump(postproc_parser, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f8c487-f42f-4981-812d-5f3ae8a63b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:49.591388Z",
     "iopub.status.busy": "2022-12-06T05:49:49.591032Z",
     "iopub.status.idle": "2022-12-06T05:49:49.707982Z",
     "shell.execute_reply": "2022-12-06T05:49:49.707185Z",
     "shell.execute_reply.started": "2022-12-06T05:49:49.591365Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('postprocessor.pickle', 'rb') as handle:\n",
    "#     postproc_parser = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7641cd-18bc-4cbd-a463-fdfb0505fd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:49.709591Z",
     "iopub.status.busy": "2022-12-06T05:49:49.709215Z",
     "iopub.status.idle": "2022-12-06T05:49:49.925991Z",
     "shell.execute_reply": "2022-12-06T05:49:49.925221Z",
     "shell.execute_reply.started": "2022-12-06T05:49:49.709574Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This places all the q+a posts into a tuple of (question-text, answer-text)\n",
    "# This allows the encoder to vectorize and relate the q+a data samples\n",
    "# all_data = list(examples_from_questions_tup(postproc_parser.map_questions.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519b3948-679e-4b7d-9744-200a9827300e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:49.926981Z",
     "iopub.status.busy": "2022-12-06T05:49:49.926812Z",
     "iopub.status.idle": "2022-12-06T05:49:50.281777Z",
     "shell.execute_reply": "2022-12-06T05:49:50.280921Z",
     "shell.execute_reply.started": "2022-12-06T05:49:49.926962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"all_data.pickle\", \"wb\") as handle:\n",
    "#     _=pickle.dump(all_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "561e9479-43d0-4230-9d83-b7e708470424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:49:50.283075Z",
     "iopub.status.busy": "2022-12-06T05:49:50.282627Z",
     "iopub.status.idle": "2022-12-06T05:50:05.145369Z",
     "shell.execute_reply": "2022-12-06T05:50:05.144506Z",
     "shell.execute_reply.started": "2022-12-06T05:49:50.283058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"all_data.pickle\", \"rb\") as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "dataset_size = len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d619127-604d-4797-8abc-b885c73254a7",
   "metadata": {},
   "source": [
    "### Split Data into Dev and Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae4f3a2-37f1-4294-a5f5-d6d76e6e37ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:05.147101Z",
     "iopub.status.busy": "2022-12-06T05:50:05.146265Z",
     "iopub.status.idle": "2022-12-06T05:50:05.609300Z",
     "shell.execute_reply": "2022-12-06T05:50:05.608280Z",
     "shell.execute_reply.started": "2022-12-06T05:50:05.147077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We split the dataset into a test and dev set\n",
    "X_train, X_dev = train_test_split(all_data, \n",
    "                                  train_size=0.8, \n",
    "                                  random_state=22,\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04100546-096c-4873-ac9a-268e95238946",
   "metadata": {},
   "source": [
    "### Load Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0c841f-dbf8-4782-b059-5c1b034738d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:05.610357Z",
     "iopub.status.busy": "2022-12-06T05:50:05.610172Z",
     "iopub.status.idle": "2022-12-06T05:50:05.614282Z",
     "shell.execute_reply": "2022-12-06T05:50:05.613221Z",
     "shell.execute_reply.started": "2022-12-06T05:50:05.610340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # CompuBERT Base Model\n",
    "# device = \"cuda\"\n",
    "# model = SentenceTransformer(\"bert-base-wikipedia-sections-mean-tokens\", \n",
    "#                             device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d99a13-b00d-425e-bcbf-3c85bb1b825a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:05.615941Z",
     "iopub.status.busy": "2022-12-06T05:50:05.615416Z",
     "iopub.status.idle": "2022-12-06T05:50:15.248590Z",
     "shell.execute_reply": "2022-12-06T05:50:15.247835Z",
     "shell.execute_reply.started": "2022-12-06T05:50:05.615921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at AnReu/math_pretrained_bert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at AnReu/math_pretrained_bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This is a new updated model that is said to perform better than\n",
    "# the base model above\n",
    "device = \"cuda\"\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# tokenizer_name = \"AnReu/albert-for-arqmath-3\"\n",
    "tokenizer_name = \"AnReu/math_pretrained_bert\"\n",
    "model_name = \"AnReu/math_pretrained_bert\"\n",
    "\n",
    "# # This model was used in a more recent paper for ARQMath that had a much better\n",
    "# # performance. This model is pretrained and I wanted to work on training my own\n",
    "# # model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"AnReu/albert-for-arqmath-3\")\n",
    "\n",
    "# # Tokenizer: Tokenizers are a great way to preprocess data\n",
    "# # In reference to the paper above, a tokenizer was created for the ARQmath\n",
    "# # dataset. I use that tokenizer below here in some of my later testing\n",
    "# math_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Tokenizer from pretrained\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "_=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe0ef3-eaac-454a-870e-6a401ac3478d",
   "metadata": {},
   "source": [
    "### Create A Pytorch Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9070acbe-cb4a-4ecc-836f-21e3122e64b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:15.261809Z",
     "iopub.status.busy": "2022-12-06T05:50:15.261384Z",
     "iopub.status.idle": "2022-12-06T05:50:15.268572Z",
     "shell.execute_reply": "2022-12-06T05:50:15.267697Z",
     "shell.execute_reply.started": "2022-12-06T05:50:15.261786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(question, answer):\n",
    "    return tokenizer(question, answer,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=512)\n",
    "\n",
    "class MathIRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        tokens = list(map(lambda x: preprocess_data(x.texts[0], x.texts[1]), dataset))\n",
    "        labels = list(map(lambda x: float(x.label), dataset))\n",
    "        self.encodings = []\n",
    "        \n",
    "        for token, label_curr in zip(tokens, labels):\n",
    "            input_ids = token['input_ids']\n",
    "            token_type_ids = token[\"token_type_ids\"]\n",
    "            att_masks = token[\"attention_mask\"]\n",
    "            label = label_curr\n",
    "            encoding = {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(att_masks, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(label, dtype=torch.long) \n",
    "            }\n",
    "            self.encodings.append(encoding)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3bbbd5-5e4f-4fcf-9999-fca133176711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:15.269952Z",
     "iopub.status.busy": "2022-12-06T05:50:15.269572Z",
     "iopub.status.idle": "2022-12-06T05:50:15.548154Z",
     "shell.execute_reply": "2022-12-06T05:50:15.547237Z",
     "shell.execute_reply.started": "2022-12-06T05:50:15.269932Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subseting the Data\n",
    "# You can opt to only take a subset of the data by changing X_train\n",
    "# to X_train[0:n]\n",
    "X_train_set = X_train[0:30000]\n",
    "X_dev_set = X_dev[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8987812d-0585-47f3-acc8-e4f6239cd843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:15.549653Z",
     "iopub.status.busy": "2022-12-06T05:50:15.549280Z",
     "iopub.status.idle": "2022-12-06T05:50:53.703641Z",
     "shell.execute_reply": "2022-12-06T05:50:53.702710Z",
     "shell.execute_reply.started": "2022-12-06T05:50:15.549634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MathIRDataset(X_train_set)\n",
    "val_dataset = MathIRDataset(X_dev_set)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ceee092-470e-4bbe-979d-5f9416328e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T05:50:53.757072Z",
     "iopub.status.busy": "2022-12-06T05:50:53.756475Z",
     "iopub.status.idle": "2022-12-06T05:50:55.413198Z",
     "shell.execute_reply": "2022-12-06T05:50:55.412376Z",
     "shell.execute_reply.started": "2022-12-06T05:50:53.757050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del all_data\n",
    "del X_train\n",
    "del X_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f1958-4921-42cd-8f91-d6ffb6e109b1",
   "metadata": {},
   "source": [
    "### Inititialize the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f30bdf-df15-44d8-8c89-f48b006c7058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T06:18:11.748715Z",
     "iopub.status.busy": "2022-12-06T06:18:11.748316Z",
     "iopub.status.idle": "2022-12-06T06:18:13.050935Z",
     "shell.execute_reply": "2022-12-06T06:18:13.050144Z",
     "shell.execute_reply.started": "2022-12-06T06:18:11.748697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying out different Optimizers\n",
    "# optim = Adan(model.parameters(), lr=5e-7)\n",
    "# optim_sched = torch.optim.lr_scheduler.ExponentialLR(optim, 0.9)\n",
    "\n",
    "\n",
    "metrics = evaluate.combine([\"accuracy\", \"precision\", \"f1\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "_=model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f7219-602f-4cb8-a6f4-542898a09a35",
   "metadata": {},
   "source": [
    "### Initialize Training Parameters and Fine-Tune (Train) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103c75b9-276a-4993-8ef5-19100710b6c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T06:18:15.585735Z",
     "iopub.status.busy": "2022-12-06T06:18:15.585359Z",
     "iopub.status.idle": "2022-12-06T06:18:15.637493Z",
     "shell.execute_reply": "2022-12-06T06:18:15.636528Z",
     "shell.execute_reply.started": "2022-12-06T06:18:15.585717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dna/miniconda3/envs/math-ir/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250000\n",
      "  Number of trainable parameters = 108696578\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_511/401787444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/math-ir/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math-ir/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;31m# tr_loss is a tensor to avoid synchronization of TPUs through .item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_loss_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "num_epochs = 50\n",
    "\n",
    "model_name_path = model_name.replace(\"/\",\"_\")\n",
    "training_args = TrainingArguments(f\"./Math-IR-Adamw-5e-5-{model_name_path}-{num}\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  # warmup_steps = 500,\n",
    "                                  per_device_train_batch_size=6,\n",
    "                                  per_device_eval_batch_size=6,\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  seed=22,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  weight_decay = 0.1,\n",
    "                                  num_train_epochs=num_epochs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # optimizers=(optim, optim_sched),\n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c11f-036b-439d-a0d1-dc100ed4a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5d42ea-fadd-4c50-981d-1edcb2264d96",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Old Model Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0dacd-e6bf-4e7b-a124-7b6f04eccd4b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-05T07:28:05.894331Z",
     "iopub.status.idle": "2022-12-05T07:28:05.894561Z",
     "shell.execute_reply": "2022-12-05T07:28:05.894462Z",
     "shell.execute_reply.started": "2022-12-05T07:28:05.894451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is Unused code to train using the sentence-transformers module\n",
    "\n",
    "\n",
    "# Create the Sentence label dataset\n",
    "train_data = SentenceLabelDataset(X_train_set, 2)\n",
    "\n",
    "# # Create the sentences for the evaluator\n",
    "# # The dataset we use for the evaluator is the dev set\n",
    "# # The format for the evaluator changed since the last CompuBERT updated\n",
    "# # I augmented the code to account for this change in the new\n",
    "# # sentence-transformers package\n",
    "# sentences_1 = [tokenizer(x.texts[0]) for x in X_dev_set]\n",
    "# sentences_2 = [tokenizer(x.texts[1]) for x in X_dev_set]\n",
    "# scores = [float(x.label) for x in X_dev_set]\n",
    "\n",
    "# Load the data loader for pytorch training\n",
    "# train_loader = DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "# train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# # Init the evaluator\n",
    "# # The evaluator will automatically write results to a folder\n",
    "# evaluator = ArqmathEvaluator(model, sentences_1, sentences_2, scores,\n",
    "#                              batch_size=10,\n",
    "#                              post_parser_postproc=postproc_parser,\n",
    "#                              name = \"ARQMathEvaluator\",\n",
    "#                              show_progress_bar=True,\n",
    "#                              write_csv=True\n",
    "#                             )\n",
    "\n",
    "# # Run the experiment\n",
    "# experiment_num = 2\n",
    "# model.compile(optimizer=Adam(3e-5))\n",
    "# model.fit(train_objectives=[(train_loader, train_loss)],\n",
    "#           evaluator=evaluator,\n",
    "#           epochs=10,\n",
    "#           evaluation_steps=50,\n",
    "#           output_path=f\"Math-IR-System-Experiment-{experiment_num}\",\n",
    "#           optimizer_params={'lr': 2e-5, 'eps': 1e-6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a14ab-b347-418b-81f7-082506f1ed06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc440afb-6288-47b4-998e-09b8c9fdacdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
