pid	doc	score
729924	Let  A  be some  m \times n  real matrix. Let  \|A\|_1  be the  maximim absolute column sum norm ,  \|A \|_\infty  be the  maximim absolute row sum norm  and  \|A \|_2 = \sigma_{\max}  be the  2-norm  or  spectral norm  ( \sigma_{\max}  is the maximum singular value). We have the following fundamental inequalities:  (i) \quad \frac{1}{\sqrt{n}} \|A \|_\infty \leq \|A \|_2 \leq \sqrt{m} \|A \|_\infty   (ii) \quad \frac{1}{\sqrt{m}} \|A \|_1 \leq \|A \|_2 \leq \sqrt{n} \|A \|_1    \|A \|_2 \leq \sqrt{\|A \|_1 \|A \|_\infty} \quad (\text{some kind of HÃ¶lder's inequality}) .    So we see that if you impose additionally that also  \|A \|_\infty \leq 1  (i.e. row sum is also small) then your assertion holds.    However it's easy to construct (quadratic) matrices with small column sum but big row sum (and vice versa). And inequality  (i)  (or  (ii)  whatever you are starting with) forces the largest singular value to be big. More precisely the following matrix   \begin{pmatrix} 0.99 & 0.99 & 0.99\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix}   fulfilles your condition but has singular value  1.71473 .  	0.8672550320625305
268532	You can use simply the fact that  \|x\|_2\leq\|x\|_1\leq\sqrt{n}\|x\|_2  to get (upper bound in the numerator, lower bound in the denominator):   \|A\|_1=\max_{x\neq 0}\frac{\|Ax\|_1}{\|x\|_1} \leq \max_{x\neq 0}\frac{\sqrt{m}\|Ax\|_2}{\|x\|_2} = \sqrt{m}\|A\|_2.   Similarly for the other:   \|A\|_2=\max_{x\neq 0}\frac{\|Ax\|_2}{\|x\|_2} \leq  \max_{x\neq 0}\frac{\|Ax\|_1}{(\sqrt{n})^{-1}\|x\|_1} = \sqrt{n}\|A\|_1.   Hence   \frac{1}{\sqrt{n}}\|A\|_2\leq\|A\|_1\leq\sqrt{m}\|A\|_2.     The constants  \alpha=\frac{1}{\sqrt{n}}  and  \beta=\sqrt{m}  are  best  in the sense that they are the tightest constants such that the bounds hold for  all  matrices, that is, there exist matrices  A_{\alpha}  and  A_{\beta}  for which one of the bounds is attained:  \frac{1}{\sqrt{n}}\|A_{\alpha}\|_2=\|A_{\alpha}\|_1  and  \|A_{\beta}\|_1=\sqrt{m}\|A_{\beta}\|_2 . These matrices can be, e.g.:   A_{\alpha}=\begin{bmatrix} 1 & 1 & \cdots & 1 \\ 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots \\ 0 & 0 & \cdots & 0 \end{bmatrix}, \qquad A_{\beta}=\begin{bmatrix} 1 & 0 & \cdots & 0 \\ 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots \\ 1 & 0 & \cdots & 0  \end{bmatrix}.   	0.8651958703994751
467852	Assuming  A\in\mathbb{C}^{n\times n}  with columns  a_1,\ldots,a_n  and that you already know the  equivalence constants for vector  p -norms :      For  \|A\|_{\infty}\leq\sqrt{n}\|A\|_2 : We have for all  x\in\mathbb{C}^n ,  \|x\|_{\infty}\leq\|x\|_2\leq\sqrt{n}\|x\|_{\infty} , so   \|A\|_{\infty}=\max_{x\neq 0}\frac{\|Ax\|_{\infty}}{\|x\|_{\infty}} \leq\max_{x\neq 0}\frac{\|Ax\|_2}{\frac{1}{\sqrt{n}}\|x\|_2}=\sqrt{n}\max_{x\neq 0}\frac{\|Ax\|_2}{\|x\|_2}=\sqrt{n}\|A\|_2.   In fact, all equivalence relations for matrix  p -norms can be derived from the related equivalence relations for the vector  p -norms and using simply the definition of the operator norm.   For  \|A\|_F\leq\sqrt{n}\|A\|_2 :   \|A\|_F^2=\sum_{i=1}^n\|a_i\|_2^2=\sum_{i=1}^n\|Ae_i\|_2^2\leq\sum_{i=1}^n\|A\|_2^2\|e_i\|_2^2=n\|A\|_2^2.   The bound can be improved if the rank  r  of  A  is smaller than  n  by realizing that   \|A\|_F^2=\mathrm{trace}(A^*A)\leq r\rho(A^*A)=r\|A\|_2^2,   where we used the fact that the trace is equal to the sum of the eigenvalues, which, if  r=\mathrm{rank}(A) , contains only  r  nonzero terms. Hence  \|A\|_F\leq\sqrt{\mathrm{rank}(A)}\|A\|_2 .   For  \|A\|_F\leq\sqrt{n}\|A\|_1 : Using the fact that  \|a_i\|_2\leq\|a_i\|_1  and that  \|a_i\|_1\leq\|A\|_1  for all  i=1,\ldots,n ,    \|A\|_F^2=\sum_{i=1}^n\|a_i\|_2^2\leq\sum_{i=1}^n\|a_i\|_1^2\leq n\|A\|_1^2.     	0.8600489497184753
761684	I prove the special case  p=q=2 :    (1) Continuity of  T_{a}\colon L^2\to L^2, u\mapsto T_a(u):=a u  is equivalent to  \|a\|_{L^\infty}<\infty . (The direction that we need is trivial. The other one is not much harder)    (2) Assume we had  \|a\|_{L^\infty}=\infty . Take disjoint sets  A_n ,  n\in\mathbb{N} , such that  |a|>2^{n}  on  A_n  and  |A_n|\leq 1 . Define  u:=\sum \frac{1}{2^n\sqrt{|A_n|}}1_{A_n} . Then  \|u\|_{L^2}\leq \sum \frac{1}{2^n\sqrt{|A_n|}}\| 1_{A_n}\|_{L^2}\leq \sum 1/2^n=1 . But  \|au\|^2_{L^2}=\sum \int_{A^n}\frac{|a|^2}{2^{2n}|A_n|}\geq \sum 1=\infty , in contradiction to the assumption.  	0.8514751195907593
776275	If  a = \{a_n\} \in \ell^\infty , then  \|a\| = \sup\{|a_n|\} . Now   \begin{align}|f(a)| &=  \left|\sum_{n=0}^\infty \frac{(-1)^na_{n+1}}{\sqrt{n!}}\right| \\&\le \sum_{n=0}^\infty \frac{|a_{n+1}|}{\sqrt{n!}}\\&\le \sum_{n=0}^\infty \frac{\|a\|}{\sqrt{n!}} = M\|a\|\end{align}  where according to Wolfram Alpha  M \approx 3.46951  (maybe someone knows a closed form for this value, but I don't).  \sum_{n=0}^\infty (n!)^{-1/2}  can be proven to converge by the ratio test.    Obviously when  a = (1, -1, 1, -1, \ldots)  we have  |f(a)| = M\|a\| . Therefore  \|f\| = M .  	0.8433781266212463
543962	"The matrix  p -norm is the operator norm:   \|A\|_p=\max_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}. \tag{1}   The fact that   \tag{2} \|A\|_2=\rho(A^*A)\quad\text{and}\quad\|A\|_{\infty}=\max_i\sum_j|a_{ij}|   is just the consequence of (1). The  2 -norm and  \infty -norm satisfy   \tag{3} \|x\|_{\infty}\leq\|x\|_2\leq\sqrt{n}\|x\|_{\infty}   for any  n -vector  x  and the fact that  \|A\|_2\leq\sqrt{n}\|A\|_{\infty}  follows by plugging (3) to (1):   \|A\|_2=\max_{x\neq 0}\frac{\|Ax\|_2}{\|x\|_2}\leq \max_{x\neq 0}\frac{\sqrt{n}\|Ax\|_\infty}{\|x\|_2} \leq \max_{x\neq 0}\frac{\sqrt{n}\|Ax\|_\infty}{\|x\|_{\infty}}=\sqrt{n}\|A\|_{\infty}.     Assuming that you know nothing about (1) and take (2) as the definitions (such an approach in a class is a bit odd because it makes showing a lot of things quite complicated), you can proceed as follows:      You show that the matrix  \infty -norm is consistent with the vector  \infty -norm, that is,   \tag{4}\|Ax\|_\infty\leq\|A\|_\infty\|x\|_\infty   holds for all vectors  x . You can see that this is a trivial consequence of (1) but since you do not know about (1) you need to do it ""by hand"".   From (4), you have that  \|A\|_2^2=\rho(A^TA)\leq\|A^TA\|_\infty . Since there is an eigenvector  x  and an eigenvalue  \lambda  of  A^TA  such that  \lambda=\rho(A^TA)  (yes, all eigenvalues of  A^TA  are nonnegative but that is not important here),  A^TAx=\lambda x  gives us that  \|\lambda x\|_{\infty}=\|A^TAx\|_\infty  and hence   \lambda\|x\|_\infty=\|A^TAx\|_\infty\stackrel{(4)}\leq\|A^TA\|_\infty\|x\|_\infty,   which by cancelling the  \|x\|_\infty  gives  \rho(A^TA)=\lambda\leq\|A^TA\|_\infty .   Finally, one must show that  \|A^TA\|_\infty\leq n\|A\|_\infty^2 . This is easy:   \begin{split} \|A^TA\|_\infty&=\max_i\sum_j|(A^TA)_{ij}| =\max_i\sum_j\left|\sum_ka_{ik}a_{kj}\right| \leq \max_i\sum_j\sum_k\left|a_{ik}a_{kj}\right| \\& \leq\color{red}{\max_{i,j}|a_{ij}|} \color{blue}{\max_i\sum_j\sum_k\left|a_{ik}\right|}. \end{split}   In the last inequality, we bounded  |a_{kj}|  by the maximum over the absolute values of all entries in  A  (which is clearly OK). Now the red term can be bounded from above by  \|A\|_{\infty} , while the blue term equals  n\|A\|_\infty  (notice the terms in the sum over  j  no longer depend on  j ). Hence, indeed,  \|A^TA\|_\infty\leq n\|A\|_\infty^2  which together with the result of Item 2 gives   \|A\|_2^2=\rho(A^TA)\leq\|A^TA\|_\infty\leq n\|A\|_\infty^2.     "	0.8298523426055908
548646	We can get a quick bound on the operators by using some straighforward inequalities. Call our operator  T . Then  \|T\| = \sup_{\|x_n\|_\infty = 1} \|Tx\|_2 .    Take  \{ x_n \} \in l^\infty  and suppose that  \|x_n\|_\infty = 1 . We can see that  \| Tx_n \| = \sqrt{ \sum_{n=1}^\infty \frac{|x_n|^2}{n^2} } \le \left(\sup_n |x_n|\right) \sqrt{\sum_{n=1}^\infty \frac1{n^2}}=1 \cdot \pi/\sqrt{6}.    Thus  \|T\| \le \pi/\sqrt{6} , and we see that this is actually achieved when  x_n \equiv 1 . Thus  \|T\|=\pi/\sqrt{6} .    A similar analysis will give you the answer to the second question.  	0.8271564841270447
227083	If you've established a few basic properties (subadditivity, submultiplicativity, and continuity) of the norm, you can do it without having to bring in the  \sup  manipulation. Namely, for any  n , we have by subadditivity of the norm that   \| I+A+\frac{A^2}{2!}+\cdots +\frac{A^n}{n!} \| \leq \| I\| + \|A\|+\|\frac{A^2}{2!}\|+\cdots +\|\frac{A^n}{n!} \|    Then since we know  \| A^2\| \leq \| A\|^2 , we can pull all the exponents out:     \| I\| + \|A\|+\|\frac{A^2}{2!}\|+\cdots +\|\frac{A^n}{n!} \|  \leq \| I\| + \|A\|+\frac{1}{2!} \|A\|^2 +\cdots +\frac{1}{n!} \|A \|^n    Then since the norm is a continuous function, we can actually pass onto the limit as  n\to \infty  of the above inequality, and obtain   \|e^A \| = \|I+A+\cdots \| \leq \| I\| + \|A\|+\frac{1}{2!} \|A\|^2 +\cdots +\frac{1}{n!} \|A \|^n + \cdots = e^{\| A\|}   	0.8254461884498596
1101458	\begin{align*} |Tx(s)|&=|s|\left|\int_{0}^{a}x(t)dt\right|\leq|s|\int_{0}^{a}\|x\|_{\infty}dt\leq|a|\|x\|_{\infty}|a|=|a|^{2}\|x\|_{\infty}, \end{align*} so  \|Tx\|_{\infty}\leq|a|^{2}\|x\|_{\infty} , hence  \|T\|\leq|a|^{2} . Now put  x=\dfrac{a}{|a|} , then  T(a/|a|)(s)=s\displaystyle\int_{0}^{a}\dfrac{a}{|a|}dt=s|a| , and  \|T(a/|a|)\|_{\infty}=|a|^{2} , so  \|T\|=|a|^{2} .  	0.8228094577789307
1023020	\sigma_\text{min}\|x\|_2=\frac{\|x\|_2}{\|A^{-1}\|_2}=\frac{\|A^{-1}Ax\|_2}{\|A^{-1}\|_2}\leq\frac{\|A^{-1}\|_2\|Ax\|_2}{\|A^{-1}\|_2}=\|Ax\|_2    When  A  is singular its smallest singular value is zero and thus the inequality is trivial this case.  	0.8223761916160583
581003	You need to cut the study in two :    On  A=[-1,1] , you have  \|f\|_{1,A}^2 \leq 2\|f\|_{2,A}^2 \leq 2\|f\|_2^2  (Jensen inequality)    On  A^c=[-1,1]^c , you have     \|f\|_{1,A^c} = \left\| \frac{g}{x} \right\|_{1,A^c} \leq \|g\|_{2,A^c}\|\frac{1}{x}\|_{2,A^c} \leq \sqrt{2} \|g\|_2    Hence     \|f\|_1 \leq \sqrt{2}(\|f\|_2+\|g\|_2)    	0.822178840637207
690501	If  x \in \mathbb{R}^p , you have  \|x\|_2 \le \|x\|_1  \le \sqrt{p} \|x\|_2 .    To see this, note that  \|x\|_2^2 = \sum_i x_i^2 \le (\sum_i |x_i|)(\sum_i |x_i|) = \|x\|_1^2 , and the other side follows directly from the Cauchy-Schwartz inequality.    We have  A:\mathbb{R}^n \to \mathbb{R}^m .    Then  \|Ax\|_2 \le \|Ax\|_1 \le \|A\|_1 \|x\|_1 \le \sqrt{n} \|A\|_1 \|x\|_2 , and so  \|A\|_2 \le \sqrt{n} \|A\|_1 .    Similarly,   \|A x\|_1 \le \sqrt{m} \|A x\|_2 \le \sqrt{m} \|A\|_2 \|x\|_2 \le \sqrt{m} \|A\|_2 \|x\|_1 , and so  \|A\|_1 \le \sqrt{m} \|A\|_2 .  	0.8207961320877075
480640	I guess it holds for  \|X\|_{1}\leq \frac{1}{\|A\|_{1}} . Let  A  be a  m\times n  matrix. Then  A^{T}:\mathbb{R}^{m}\to\mathbb{R}^{n}  is a linear map. Now if we consider the  \ell_{\infty}  norm on both  \mathbb{R}^{m}  and  \mathbb{R}^{n} , then  \|A\|_{1}=\|A^{T}\|_{op} , where  \|\cdot\|_{op}  stands for the  operator norm . Therefore \begin{eqnarray} \|XA\|_{1}=\|A^{T}X^{T}\|_{op}\leq\|X^{T}\|_{op}\|A^{T}\|_{op}=\|X\|_{1}\|A\|_{1}. \end{eqnarray}  If  \|X\|_{1}\leq \frac{1}{\|A\|_{1}} , then we have  \|XA\|_{1}\leq 1 .  	0.8195104598999023
135473	This is wrong. In what follows, we assume that we are working with the operator norm.  If  A  is non-singular, it is easy to check that  P  is non-singular for all  P\in B\left(A;\frac{1}{\Vert A^{-1}\Vert}\right) .    The proof runs as follows. Let  P\in B\left(A,\frac{1}{\|A^{-1}\|}\right) . We shall prove that  P\in GL_n(\mathbb{R}) . For each  x\in\mathbb{R}^n , \begin{eqnarray*} \|x\|&=&\|A^{-1}(Ax)\|\leqslant \|A^{-1}\|\|Ax\|\\& \leqslant & \|A^{-1}\|\left(\|(A-P)x\|+\|Px\|\right)\leqslant \|A^{-1}\|\left(\|A-P\|\|x\|+\|Px\|\right). \end{eqnarray*} Thus,   \|x\|\leqslant \frac{\|A^{-1}\|}{1-\|A-P\|\|A^{-1}\|}\|Px\|,\text{ for all }x\in\mathbb{R}^n.   This implies that  P\in GL_n(\mathbb{R})  because if   Px=0 , we have  x=0 . Thus we have proved that   B\left(A,\frac{1}{\|A^{-1}\|}\right)\subset GL_n(\mathbb{R}) .  	0.8184606432914734
698211	Hint:    \frac{1}{\|A^{-1}\|\|A\|}=\left|\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\right|\leq 1 \ [\text{applicable only for the normal matrices}]      \frac{1}{\|A^{-1}\|\|A\|}=\frac{\sigma_{min}(A)}{\sigma_{max}(A)}\leq 1 \ [\text{true only when, } \|\cdot\| \text{ denotes } \|\cdot\|_2]  For any matrix  A, \ \kappa(A) \geq 1 .    [Please check the comments below for another approach to solve this problem.]  	0.8180593848228455
1105862	Your inequalities all go in the same direction, so you cannot possibly prove inequalities both ways.     You have the easy inequalities  \|x\|_2\leq\|x\|_1\leq\sqrt n\,\|x\|_2.  So you immediately get (from the definition, not the characterization you quote)  \tag{1}\bbox[5px,border:2px solid green]{\|A\|_{2,2}\leq\|A\|_{2,1}\leq\sqrt n\|A\|_{2,2}.}   These inequalities are sharp. For instance, if  A=E_{11} , then    \|Ax\|_1=\|(x_1,0,\ldots,0)\|_1=|x_1|=\|x\|_2.   So  \|A\|_{2,2}=\|A\|_{1,1} . And if  A=I_m , then  \|Ax\|_1=\|x\|_1 , so   \|A\|_{2,1}=\max\{\|x\|_1:\ \|x\|_2=1\}=\sqrt n,   while  \|A\|_{2,2}=1 . So  \|A\|_{2,1}=\sqrt n\|A\|_{2,2} .    Edit:  here is a short proof of the inequalities  (1) , in case they are not obvious.    You have, for any nonzero  x .    \frac{\|Ax\|_2}{\|x\|_2}\leq\frac{\|Ax\|_1}{\|x\|_2}\leq\|A\|_{2,1}.   Now you take supremum (forget about the term on the middle) and you obtain   \|A\|_{2,2}\leq\|A\|_{1,1}.   Similarly, start with    \frac{\|Ax\|_1}{\|x\|_2}\leq\sqrt{n}\,\frac{\|Ax\|_2}{\|x\|_2}\leq\sqrt{n}\,\|A\|_{2,2}.   Now forget the middle term and take supremum, to obtain   \|A\|_{2,1}\leq\sqrt{n}\,\|A\|_{2,2}.   	0.8164936900138855
870788	Consider the sequence  \xi  given by \begin{align} \xi_i = \frac{\operatorname{sgn}(a_i)|a_i|^{q-1}}{|| a||_q^{q-1}}. \end{align} Observe  \xi \in \ell^p , since \begin{align} \left(\sum^\infty_{i=1}\frac{|a_i|^{p(q-1)}}{||a||_q^q}\right)^{1/p} = \left(\sum^\infty_{i=1} \frac{|a_i|^q}{||a||_q^q} \right)^{1/p} = \frac{||a||_q^{q/p}}{||a||_q^{q/p}}<\infty. \end{align} Moreover, we have that \begin{align} f(\xi) = \sum^\infty_{n=1}\frac{|a_i|^q}{||a||^{q-1}}=||a||_q. \end{align}  	0.8128978610038757
354020	A connection between determinant and Frobenius norm results from the fact that  |\det A|  is the volume of the spat  A[0,1]^n  and that this is smaller than the volume of a rectangular box with the same side lengths.    If  A  has columns  A=(a_1,a_2,..., a_n)  then    |\det(A)| \le  \|a_1\|\,\|a_2\|\,....\,\|a_n\| \le  \left(\frac{\|a_1\|^2+\|a_2\|^2+....+\|a_n\|^2}{n}\right)^{\frac{n}{2}} = \left(\frac{\|A\|_F^2}{n}\right)^{\frac{n}{2}} =\frac{\|A\|_F^n}{n^{n/2}}   	0.8124208450317383
1197878	This is the Holder's inequality for  q=1  and  p=\infty  i.e.     \begin{align*} \|v u\|_2 &=\sqrt{\|v^2 u^2 \|_1}\\ &\leq    \sqrt{\| v^2\|_\infty}\sqrt{ \| u^2\|_1}\\ &=\|v\|_\infty \|u\|_2 \end{align*} And for the proof someone can see Martin's answer.    In certain cases the previous version of Holder's can be, easily, derived from the more popular one, i.e. when  1	0.8123541474342346
141144	(not doing all, but here's a few)      The key here is to notice that    \|A\|_F^2=\text{Tr}_n(A^TA).   Then   \|A\|_2^2=\lambda_\max(A^TA)\leq\text{Tr}_n(A^TA)\leq n\lambda_\max(A^TA),   so  \||A\|_2\leq\|A\|_F\leq\sqrt n\|A\|_2 .   Here, writing  e=(1,\ldots,1)^T\in\mathbb R^n ,   \|A\|_\infty=\max_i\sum_j|A_{ij}|=\|Ae\|_\infty\leq\|Ae\|_2\leq\|A\|_2\|e\|_2=\sqrt n\|A\|_2     	0.8119800686836243
179421	Referring to a previous  problem , for  ||A||<1 , we have     (I-A)^{-1} = \sum_{n=0}^{\infty}A^n       \implies  ||(I-A)^{-1}|| = ||\sum_{n=0}^{\infty}A^n||\leq \sum_{n=0}^{\infty}||A^n||\leq \sum_{n=0}^{\infty}||A||^n=\frac{1}{1-||A||}       \implies ||(I-A)^{-1}|| \leq \frac{1}{1-||A||}.     Note that, we have used the fact that      ||A^n||\leq ||A||^n .    	0.8109744787216187
1130305	You can compute straight away  \|a^2b\|_{L^2}^2 = \int a^3 ab^2 \leq \frac{1}{2} \int a^6 + a^2b^4 = C( \|a\|_{L^6}^6 + \|ab^2\|_{L^2}^2) \leq C( \|a\|_{L^6}^6 + \|a\|_{L^\infty}^2\|b^2\|_{L^2}^2).   Take the square root on both sides   \|ab^2\|_{L^2} \leq C \sqrt{\|a\|_{L^6}^6 + \|a\|_{L^\infty}^2\|b^2\|_{L^2}^2} \leq C(\|a\|_{L^6}^3 + \|a\|_{L^\infty}\|b^2\|_{L^2}).   Note that all we used where pointwise elementary inequality such as   ab \leq \frac{1}{2}(a^2+b^2)   and   \sqrt{a^2+b^2}\leq C (|a|+|b|).   For this reason this holds even on unbounded domains and not just on the torus.  	0.8091915845870972
402695	I don't know if I get your question correctly, but it seems like the induced norm is defined in the usual way, i.e.   \|A\|_1:=\sup_{x\ne 0}\frac{\|Ax\|_1}{\|x\|_1}   and the goal is to show that   C:=\max_{1\leqslant j\leqslant n}\sum_{i=1}^n \left|a_{ij}\right|=\sup_{x\ne 0}\frac{\|Ax\|_1}{\|x\|_1}  The inequality  \|A\|_1\leqslant C  says essentially that for every  x  you have   \frac{\|Ax\|_1}{\|x\|_1}\leqslant C   But you don't know yet if the  \sup  is a  \max . So if there exists an  x\ne 0  such that  C=\frac{\|Ax\|_1}{\|x\|_1}  , you're done.  	0.8086565732955933
141310	For  y \in \mathbb{R}^m  you have  \|y\|_2 = \sqrt{\sum_k y_k^2} \leq \sqrt{\sum_k \|y\|_\infty^2}= \sqrt{m} \|y\|_\infty .    Hence  \|Ax\|_2 \leq \sqrt{m} \|Ax\|_\infty , for all  x . Now suppose  \|x\|_\infty\leq 1 , then we have  \|Ax\|_2 \leq \sup_{\|x\|_\infty\leq 1} \sqrt{m} \|Ax\|_\infty = \sqrt{m} \|A\|_\infty . Now suppose  \|x\|_2\leq 1 . Then  we have  \|x\|_\infty \leq 1  and so  \|A\|_2 = \sup_{\|x\|_2\leq 1} \|Ax\|_2 \leq   \sqrt{m} \|A\|_\infty .    Now note that for any norm and  any  \sigma>0  we have  \sup_{\|x\|\leq \sigma} \|Ax\| = \sigma \|A\| . It is straightforward to show that if  y \in \mathbb{R}^m  you have  \|y\|_2 \leq \|y\|_1 . It is also straightforward to show that if  x \in \mathbb{R}^n  and   \|x\|_2\leq 1 , then  \|x\|_1 \leq \sqrt{n}  (ie,  B_2(0,1) \subset B_1(0,\sqrt{n}) ).    Hence we have  \|Ax\|_2 \leq \|Ax\|_1 . Now suppose  \|x\|_1 \leq \sqrt{n} , then we have  \|Ax\|_2 \leq \sup_{\|x\|_1 \leq \sqrt{n}}\|Ax\|_1 = \sqrt{n} \|A\|_1 , and since  B_2(0,1) \subset B_1(0,\sqrt{n}) , we have  \|A\|_2 = \sup_{\|x\|_2\leq 1} \|Ax\|_2 \leq \sqrt{n} \|A\|_1 .  	0.8081624507904053
1092530	Answer to the question:  for all  A\in\Bbb R^{m\times n}  we have,      \color{blue}{n^{-\frac1p}m^{-\frac1q} \|A\|_p \le  \|A\|_q \le  m^{\frac1p}n^{\frac1q} \|A\|_p }  See Below for explanatory details.      Hence take  C_2 =m^{\frac1p}n^{\frac1q}, ~~~~C_{1}^{-1}=n^{\frac1p}m^{\frac1q}        Lemma:  for all  x\in\Bbb R^d   We have,     \color{red}{d^{-\frac1q} \cdot\|x\|_q\le  \|x\|_p \leq d^{\frac1p} \cdot\|x\|_q\tag{I}}     Proof:    Indeed,  for  x\in\Bbb R^d   we get   \|x\|_\infty \le \|x\|_p\le  n^{\frac1p}\|x\|_\infty \quad\text{and}\quad\|x\|_\infty \le \|x\|_q\le  n^{\frac1q}\|x\|_\infty     Then we have,   \color{red}{d^{-\frac1q} \cdot\|x\|_q\le  \|x\|_p \leq d^{\frac1p} \cdot\|x\|_q\tag{I}}      Then, for  x\in\Bbb R^n  we have,  Ax\in \Bbb R^m  and hence,  \|Ax\|_p \overset{(I)}{\le} n^{\frac1p} \cdot\|Ax\|_q \le  m^{\frac1p} \|A\|_q\cdot\|x\|_q \overset{(I)}{\le}  m^{\frac1p}n^{\frac1q}  \cdot\|x\|_p    that is  \|A\|_p=\max_{x\neq0}\frac{\|Ax\|_p}{\|x\|_p} \le  m^{\frac1p}n^{\frac1q} \|A\|_q     That is   \color{blue}{ \|A\|_p \le  m^{\frac1p}n^{\frac1q} \|A\|_q }    By symmetry we have,      \color{blue}{ \|A\|_q \le  n^{\frac1p}m^{\frac1q} \|A\|_p }    That is      \color{blue}{n^{-\frac1p}m^{-\frac1q} \|A\|_p \le  \|A\|_q \le  m^{\frac1p}n^{\frac1q} \|A\|_p }    Hence take  C_2 =m^{\frac1p}n^{\frac1q}, ~~~~C_{1}^{-1}=n^{\frac1p}m^{\frac1q}  	0.8073988556861877
721844	Since  B  is singular there exists some  v\neq 0  with  \|v\|=1   such that  Bv=0 . Thus,  \|A-B\|_2^2\geq \|(A-B)v\|^2=\|Av\|^2=v^*A^*Av\geq \lambda_{min}(A^*A)\\ =\lambda_{min}\left[(A^{-1}(A^{-1})^*)^{-1}\right]=\frac{1}{\lambda_{max}(A^{-1}(A^{-1})^*)}=\frac{1}{\|A^{-1}\|_2^2}    EDIT: For a general induced  p -norm ( p>1 ):  Similarly there exists some  v\neq 0  with  \|v\|_p=1   such that  Bv=0 . Thus, for the induced matrix  p -norm  \|A-B\|_p\geq \|(A-B)v\|_p=\|Av\|_p\qquad\qquad(1)  Define now  q>0  such that  \frac{1}{p}+\frac{1}{q}=1 . Then using Holder inequality  \|Av\|_p\|A^{-1}\|_q\geq \|Av\|_p\frac{\|A^{-1}v\|_q}{\|v\|_q}\geq \|v\|_2^2\|v\|_{q}^{-1}\qquad (2)  Thus for a general  p -norm ( p>1 ) it holds true from (1), (2) that  \|A-B\|_p\geq \min_{v\in Ker(B), \|v\|_p=1} \left\{\|v\|_2^2\|v\|^{-1}_q\right\}\frac{1}{\|A^{-1}\|_q}  	0.8066763281822205
443311	All mentioned matrix norms are special cases of the matrix  p -norm:  \tag{a} \|A\|_p:=\max_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}.   If, for two  vector   p -norms  \|\cdot\|_p  and  \|\cdot\|_q  ( p,q\in[1,\infty] ), there are positive constants  \alpha  and  \beta  such that  \tag{b} \alpha\|x\|_q\leq\|x\|_p\leq\beta\|x\|_q   holds for all  x , then we have from (a) that   \|A\|_p=\max_{x\neq 0}\frac{\|Ax\|_p}{\|x\|_p}\begin{cases}\leq\max\limits_{x\neq 0}\frac{\beta\|Ax\|_q}{\alpha\|x\|_q}&=\frac{\beta}{\alpha}\|A\|_q\\ \geq\max_{x\neq 0}\frac{\alpha\|Ax\|_q}{\beta\|x\|_q}&=\frac{\alpha}{\beta}\|A\|_q. \end{cases}.   Hence,  \tag{c} \frac{\alpha}{\beta}\|A\|_q\leq\|A\|_p\leq\frac{\beta}{\alpha}\|A\|_q.   To get (1) and (2), use in (c) and (b) the well known relations  \tag{d} \|x\|_{\infty}\leq\|x\|_2\leq\sqrt{n}\|x\|_{\infty}, \quad \frac{1}{\sqrt{n}}\|x\|_1\leq\|x\|_2\leq\|x\|_1.        For (4), note that the 2-norm of  A  is the square root of the largest eigenvalue of  A^*A , that is, there is a  \lambda=\|A\|_2^2  and a nonzero  y  such that  A^*Ay=\lambda y . Let  \|\cdot\|  be  any  vector norm and  \|\cdot\|  its associated operator norm. The eigenvector  y  can be chosen such that  \|y\|=1 . Then   \|A\|_2^2=\lambda=\|A^*Ay\|\leq\|A^*\|\|A\|\|y\|=\|A\|\|A^*\|.  Hence for any operator matrix norm   \|A\|_2\leq\sqrt{\|A\|\|A^*\|}.   In particular,  \tag{e} \|A\|_2\leq\sqrt{\|A\|_1\|A^*\|_1}=\sqrt{\|A\|_1\|A\|_{\infty}}.        To show (3), let  e_i  be the  i th column of the  n\times n  identity matrix. Since  \|e_i\|_2=1 , (a) and (d) imply that   \|A\|_2\geq\|Ae_i\|_2\geq\|Ae_i\|_{\infty}=\max_{j}|a_{ij}|.   Since the above is valid for any  i ,  \|A\|_2\geq\max_{i,j}|a_{ij}|.  The second inequality can be easily obtained from (e) and the fact that    \left.\begin{array}{l} \|A\|_1\\\|A\|_{\infty} \end{array}\right\}\leq n\max_{i,j}|a_{ij}|.   	0.8066402077674866
952796	HINT: we must show that if  q\neq p  then these norms are not equivalent, that is doesnt exists  K\in(0,\infty)  such that    K\|f\|_q\le\|f\|_p,\quad\forall f\in C(I,\Bbb K)    or equivalently    \frac{\|f\|_q}{\|f\|_p}\le K^{-1}    for  f\neq 0 . Choosing  f(x):=a^x ,  q=2  and  p=1  and  I:=[0,1]  we have that it must be true that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}\le K^{-1},\quad\forall a>1    but the above is equivalent to say that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}=\frac{\sqrt{\frac{a^2-1}{2\ln a}}}{\frac{a-1}{\ln a}}=\sqrt{\frac{(a+1)\ln a}{(a-1)2}}\le K^{-1},\quad\forall a>1    what cannot be possible, hence  \|{\cdot}\|_2  and  \|{\cdot}\|_1  are not equivalent. We can generalize this result easily for any  q>p  and any interval  [\alpha,\beta] .  	0.8062635660171509
156628	K(A) = \|A\| \|A^{-1}\| , so the inequality in question is  \|A - B\| \ge \frac 1{\|A^{-1}\|} .    Pick a non-zero vector  x \in \ker(B) . Then  (A - B)x = Ax , so  \|A - B\| \ge \frac{\|Ax\|}{\|x\|} .    Next, let  y = Ax . Then  \frac{\|Ax\|}{\|x\|} = \frac{\|y\|}{\|A^{-1}y\|} \ge \frac{1}{\|A^{-1}\|}.  	0.8058899641036987
91275	I will assume that  d \asymp n^\alpha  with  \alpha < 1 , so that  d < n  asymptotically.  (If  \alpha > 1 , then  X'X  is not invertible for  n \ge 2 , and you have to be more careful in stating what you mean.) Then the singular values of  X  are in    (\sqrt{n} - \sqrt{d} -t,\sqrt{n} + \sqrt{d} +t)   with prob. at least  1 - 2\exp(-t^2/2) . It follows that the least singular value of  \frac1{\sqrt{n}} X  is bounded below by  1-C \sqrt{d/n}  w.h.p. Then, the largest singular value of  (\frac1n X'X)^{-1} , that is, its operator norm, is bounded above by  1/(1 -C \sqrt{d/n})^2 = 1 +C' \sqrt{d/n}  for  n  large enough.    On the other hand as you mentioned  \| \frac{1}{n} X'1\|_\infty \le C'' \sqrt{\frac{\log d}{n}}  w.h.p. and  \| \cdot\|_2 \le \sqrt{d} \| \cdot\|_\infty   over  \mathbb{R}^d .  Putting the pieces together, \begin{align*} \| (\frac1n X'X)^{-1} \frac{1}{n} X'1\|_\infty &\le   \| (\frac1n X'X)^{-1} \frac{1}{n} X'1\|_2 \\ &\le \| (\frac1n X'X)^{-1} \|_{\text{op}} \;\| \frac{1}{n} X'1\|_2 \\ &\le (1 + C'\sqrt{\frac{d}{n}})\; \sqrt{d} \| \frac{1}{n} X'1\|_\infty\\ &\le(1 + C'\sqrt{\frac{d}{n}})\; \sqrt{d} \;C''\sqrt{\frac{\log d}{n}} \\ &\le C'''\sqrt{\frac{d \log d}{n}} \end{align*} for large  n , w.h.p.    Please check, as I may have made a mistake.  	0.8049436807632446
1254915	Your function is not defined at  (0,0) . For the other points: fix  a=(x_0,y_0)  and let  \epsilon >0 . Observe that if  \delta < \|a\|/2  then for all  b=(x_1,y_1)  such that  \|a-b\|< \delta  we have  \frac{1}{(\|a\|-\frac{\|a\|}{2})^2 }=\frac{4}{\|a\|^2} \geq \frac{1}{\|b\|^2}.  Observe that  |y_0 - y_1|\leq \| a-b\|  and   |f(a)-f(b)| = \left| \frac{y_0}{\|a\|^2} - \frac{y_1}{\|b\|^2} \right| = \left| \frac{ y_0 \|b\|^2 - y_1 \| a\|^2}{\|a\|^2 \|b\|^2}\right| \leq \left| \frac{ |y_0|\cdot \left|\|b\|^2 - \| a\|^2 \right|+ |y_1 - y_0|\cdot \|a\|^2}{\|a\|^2 \|b\|^2}\right| \leq \frac{4}{\|a\|^4}\left( |y_0|\cdot \left|\|b\|^2 - \| a\|^2 \right|+ |y_1 - y_0|\cdot \|a\|^2 \right).   Choose   \delta < \min \left\{ \frac{2 \epsilon}{5\|a\|}, \frac{\|a\|}{2}, \frac{\epsilon}{\|a\|^2} \right\}.  	0.8043926954269409
267155	Recall that the supremum of  \frac{\|Ah\|}{\|h\|}  is by definition the norm of  A . Since for all  h , this is bounded by the constant  \|A^*A\|^{1/2} , taking the supremum over all  h  shows  \|A\|\le \|A^*A\|^{1/2} . If you repeat the argument on the first line and swap the positions of  A  and  A^* , you get similarly that  \|A^*\|\le \|A^*A\|^{1/2}  Multiplying these, you have  \|A^*\|\|A\|\le \|A^*A\| . But since in the general the norm of the product is at most the product of the norms, you also have   \|A^*A\|\le \|A^*\|\|A\| .     It follows that  \|A^*A\|= \|A^*\|\|A\| . In particular, both of the inequalities  \|A\|\le \|A^*A\|^{1/2}  and  \|A^*\|\le \|A^*A\|^{1/2}  must be equalities.   	0.8043228983879089
850252	"Here is a proof for the Euclidean norm. This approach is buried in SchÃ¤ffer. J., "" Norms and determinants of linear mappings "", Technical report, CMU, Department of Mathematical Sciences, 1970. He does not use the SVD, but the idea is essentially the same.    Let  A=U \Sigma V^*  be a singular value decomposition of  A , with  \Sigma=\operatorname{diag} (\sigma_1,...,\sigma_n) , and  \sigma_1\ge ... \ge\sigma_n .    Then  \|A\| = \sigma_1, \|A^{-1}\| = {1 \over \sigma_n} , and  |\det A| = \sigma_1 \cdots \sigma_n .    Hence  |\det A| \le \sigma_1 \cdots \sigma_{n-1} {1 \over \|A^{-1} \|} \le \|A\|^{n-1} {1 \over \|A^{-1} \|} , from which we obtain  |\det A| \|A^{-1} \| \le \|A\|^{n-1} .    The result is not true for general operator norms, for example, with  A=\begin{bmatrix} {1 \over 2} & 1 \\ 0 & 2 \end{bmatrix} , we have  \det A = 1 ,  \|A^{-1} \|_\infty = 3, \|A\|_\infty  = 2  and so  3=\|A^{-1} \|_\infty \not< \|A\|_\infty^{2-1} = 2 .    As an aside, it is worth noting the related  Hadamard's inequality ,  |\det A| \le \|Ae_1\| \cdots \|A e_n\|  (Euclidean norm).  "	0.8040179014205933
577698	Do you remember how we can prove Cauchy-Schwarz through interpolation?     Since  |xy|\leq\frac{x^2+y^2}{2}  we have:   \| f\cdot g\|_1 \leq \frac{\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2} \tag{1}   but the LHS is just the same if we replace  f  with  \lambda f  and  g  with  \frac{1}{\lambda}g , so:   \| f\cdot g\|_1 \leq \frac{\lambda^2\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2\lambda^2} \tag{2}   and by choosing  \lambda  in such a way the two terms in the RHS of  (2)  are equal, i.e.  \lambda=\sqrt{\frac{\|g\|_2}{\|f\|_2}} , we get:   \| f\cdot g\|_1 \leq \|f\|_2 \cdot \|g\|_2 \tag{3}  that is the usual Cauchy-Schwarz inequality.  If we start with the Young inequality:   |xy|\leq \frac{|x|^p}{p}+\frac{|y|^q}{q}\tag{4}   and follow exactly the same interpolation steps, we end with:   \| f\cdot g\|_1 \leq \|f\|_p\cdot \|g\|_q \tag{5}   that is the wanted Holder's inequality.  	0.8030720949172974
66166	"On the sum of Hilbert-Schmidt operators, "" \|(S+S')e_n\|_H^2 \leq \|Se_n\|_H^2 + \|S'e_n\|_H^2 "" is a little questionable! I propose the following approach instead: \begin{align*} \|S+S'\|_{HS} &= \bigg[ \sum_{n=0}^{\infty} \|(S+S')e_n \|_H^2 \bigg]^{\frac{1}{2}}  \leq \bigg[ \sum_{n=0}^{\infty} \bigg( \|Se_n\|_H + \| S'e_n \|_H \bigg)^2 \bigg]^{\frac{1}{2}} &\\  &\leq \bigg[ \sum_{n=0}^{\infty} \|Se_n\|_H^2 \bigg]^{\frac{1}{2}} +  \bigg[ \sum_{n=0}^{\infty} \|S'e_n\|_H^2 \bigg]^{\frac{1}{2}} = \|S\|_{HS} + \|S'\|_{HS} < \infty \end{align*}    The first inequality is the triangle inequality. The second inequality is Minkowski's inequality for infinite sums, where we view  (\|Se_n\|_H)_n  and  (\|S'e_n\|_H)_n  as sequences of real numbers. (working in a real Hilbert space)  "	0.8029131293296814
777747	You'll want to start by showing that  \|\rho\star f\|_{p}\le \|\rho\|_1\|f\|_{\rho} , which holds for  1 \le p \le \infty , but you don't need this for  p=\infty . For  p=1 , this is fairly standard. If you're not sure how to estimate that, let  q  be conjugate to  p  so that  \frac{1}{p}+\frac{1}{q}=1  and write        \left|\int \rho fdx\right| \le \int |\rho|^{\frac{1}{q}}\{|\rho|^{\frac{1}{p}}|f|\}dx   For any  R > 0 , \begin{align}    \|(\chi_{|x|\ge R}\rho_n)\star f\|_p & \le \|\chi_{|x|\ge R}\rho_n\|_1\|f\|_p      \\     & = \int_{|x|\ge R}|\rho_n|dx\|f\|_p \\     & = \int_{|x| \ge nR}|\rho|dx\|f\|_p\rightarrow 0 \mbox{ as } n\rightarrow\infty. \end{align} The remaining piece is  (\chi_{|x| \le R}\rho_n)\star f , which can be studied for continuous  f .  	0.8026512861251831
1150573	For the RHS (since, for all  i ,  |x_i|\leq \sup_j |x_j|:=\|x\|_{\infty} \ \Rightarrow \ x_i^2 \leq \|x\|_{\infty}^2 ) :  \begin{align*} \|x\|_2=\sqrt{\sum_i x_i^2}& \leq \sqrt{\sum_i \|x\|_{\infty}^2} \\ & =\sqrt{n \|x\|_{\infty}^2}=\sqrt{n} \|x\|_{\infty} \end{align*}  	0.8025016784667969
450584	Set  \rho:=\lim_{n\to\infty }\left|\frac{a_{n+1}}{a_n}\right| . If  \rho<1  then  0<\frac{\rho+1}{2}<1  and  \lim_{n\to\infty }\left(\frac{1+\rho}{2}\right)^n=0.    Since  \rho=\lim_{n\to\infty }\left|\frac{a_{n+1}}{a_n}\right|,  there exists an  N\in\mathbb N  such that   n\geq N\implies \left|\frac{a_{n+1}}{a_n}\right|\leq \frac{\rho+1}{2},  and so  |a_{n+1}|\leq\frac{\rho+1}{2}|a_n|  if  n\geq N . Recursively, we obtain  |a_n|\leq \left(\frac{1+\rho}{2}\right)|a_{n-1}|\leq   \left(\frac{1+\rho}{2}\right)^2|a_{n-2}|\leq\cdots\leq  \left(\frac{1+\rho}{2}\right)^{n-N}|a_{N}|  and so  \lim_{n\to\infty }|a_n|=0.  Moreover,  -|a_n|\leq a_n\leq |a_n|.  Therefore, by the  squeeze theorem , we obtain that  \lim_{n\to\infty }a_n=0.  	0.8024406433105469
316065	For general  A , it is not true even in finite dimension in the  1\times1  case: because that would imply the inequality  |\log(1+t)|\leq C|t| , and we can take  t=-1+\varepsilon  to get the (false) inequality  |\log\varepsilon|\leq C|1-\varepsilon|  for all  \varepsilon>0 .     For  A  positive, though, if the eigenvalue sequence of  A  is  \{\lambda_j\} , with  \lambda_j\geq0  for all  j ,   |\log\det(I+A)|=\sum_j\log(1+\lambda_j)\leq\sum_j\lambda_j=\|A\|_1.     If you know that  \|A\|<\delta<1 , then (using Plemelj's formula)   |\log\det(I+A)|=\left|\sum_{n=1}^\infty(-1)^{n-1}\frac{\mbox{Tr}(A^n)}n\right|\leq\sum_{n=1}^\infty\frac{|\mbox{Tr}(A^n)|}n \leq\sum_{n=1}^\infty\frac{\|A\|_1\,\|A\|^{n-1}}n\\ \leq\|A\|_1\,\sum_{n=1}^\infty\frac{\delta^{n-1}}n=\left(-\frac1\delta\,\log(1-\delta)\right)\,\|A\|_1.   	0.8023377060890198
1158129	Suppose you want     \frac1{\sqrt{n}}\|A\|_\infty = \|A\|_2 = \sqrt{m}\|A\|_\infty    Then we have  \|A\|_\infty=\sqrt{mn}\|A\|_\infty   and since  \|A\|_\infty >0  if  A \neq 0 , we have  \sqrt{mn}=1 , that is  m=n=1 .    When  m=n=1 , the equality holds.    Also, when  m \neq n , then the equality doesn't hold if  A \ne 0 .  	0.8021047115325928
268234	Not true for the spectral norm you use. Simple example:   A = \begin{bmatrix}1 & 1 \\ -1 & 1\end{bmatrix}, \quad \tilde{A} = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix},     \|A\|_2 = \sqrt{2}\approx 1.4142, \qquad  \|\tilde{A}\|_2 = \sqrt{\frac{\sqrt{5}+3}{2}}\approx 1.6180.   On the other hand, zeroing out entries of  A  to get  \tilde{A}  can give:      strict inequality for the Frobenius norm (2-norm of the vector of the singular values):  \|\tilde{A}\|_F < \|A\|_F ,   non-strict inequalities for some other norms, e.g.:  \|\tilde{A}\|_{\star} \leq \|A\|_{\star} , where  \star = 1,\infty  (usual matrix  p -norms) or  \star=M , where  \|A\|_M=\max_{i,j}|a_{ij}| .    	0.8020642995834351
1142591	"If   f(x):=A^{-1}x,   then   \frac{\|f(x+\delta x)-f(x)\|}{\|f(x)\|}/ \frac{\|\delta x\|}{\|x\|} = \frac{\|A^{-1}(x+\delta x)-A^{-1}x\|}{\|A^{-1}x\|}/ \frac{\|\delta x\|}{\|x\|} = \frac{\|A^{-1}\delta x\|}{\|\delta x\|} \frac{\|x\|}{\|A^{-1}x\|},   so   \lim_{\epsilon\rightarrow 0^+}\sup_{\|\delta x\|\leq\epsilon} \frac{\|A^{-1}\delta x\|}{\|\delta x\|} \frac{\|x\|}{\|A^{-1}x\|} = \|A^{-1}\| \frac{\|x\|}{\|A^{-1}x\|}.   Note that the condition number depends on the ""right-hand side""  x . To eliminate this dependency and thus to obtain the ""worst-case"" condition number, we want to maximize it over all nonzero  x  which gives the classical matrix condition number:   \sup_{x\neq 0} \|A^{-1}\| \frac{\|x\|}{\|A^{-1}x\|} = \|A^{-1}\| \sup_{y\neq 0} \frac{\|Ay\|}{\|y\|} =\|A\|\|A^{-1}\|.   "	0.8017475605010986
513041	Write  F(x) = \int_a^b 1_{[a,x]}(y)f(y)\, dy . By the Cauchy-Schwarz inequality,  |F(x)| \le  \|1_{[a,x]}\|_2\cdot\|f\|_2 = (x - a)^{1/2}\|f\|_2 \qquad (a < x < b).  Thus  \|F\|_2 \le  \|(x - a)^{1/2}\|_2\cdot \|f\|_2 = \frac{b-a}{\sqrt{2}}\|f\|_2 < \infty.    	0.8016550540924072
608560	Let  w \neq 0  be such that  (A+B)w = 0 . Then,    \left\| {\left| A^{-1}\right|} \right\| = \max_{v\neq 0} \frac{|A^{-1}v|}{|v|} \geq \frac{|A^{-1}Aw|}{|Aw|} = \frac{|w|}{|Aw|} = \left(\frac{|Bw|}{|w|}\right)^{-1} \geq \left(\max_{v\neq 0} \frac{|Bv|}{|v|}\right)^{-1} = \left\| {\left| B \right|} \right\|^{-1}  	0.8016509413719177
183624	As copper.hat already pointed out, the inequality  \|f\|_p \leq \mu(E)^{1-\frac{1}{p}} \cdot \|f\|_{\infty}  does not hold for all  p \geq 1 . Instead, it should read  \|f\|_p \leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty}    This follows from  \|f\|_p \leq \mu(E)^{\frac{1}{p}-\frac{1}{r}} \cdot \|f\|_r \qquad (r>p) by letting  r \to \infty , using that  \lim_{r \to \infty} \|f\|_r = \|f\|_{\infty}  (see for a  proof  here).       Edit: Actually, there is a rather quick (and direct) proof of the inequality:  \begin{align} \|f\|_p^p &= \int_E \underbrace{|f|^p}_{\stackrel{p \geq 1}{\leq} \|f\|_{\infty}^p} \, d\mu \leq \mu(E) \cdot \|f\|_{\infty}^p \\ \Rightarrow \|f\|_p &\leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty} \end{align}  	0.8016346096992493
394685	I think what you want to consider is what might  \|A^2\|  be, since  \|A\|^2  is just the square of the norm we have found. Now, in general  \|A^2\|\le\|A\|^2    But lets look at why this is:    \|A^2\|=\sup_{0\ne x\in X}\frac{\|A^2(x)\|}{\|x\|}=\sup_{0\ne x\in X}\frac{\|A(A(x))\|}{\|x\|}\le\sup_{0\ne x\in X}\frac{\|A\|\|(A(x))\|}{\|x\|}\le \sup_{0\ne x\in X}\frac{\|A(A(x))\|}{\|x\|}    \le\sup_{0\ne x\in X}\frac{\|A\|\|A\|\|x\|}{\|x\|}=\|A\|^2     Thus  \|A^2\|\le\|A\|^2 .  	0.8012963533401489
1252952	If  y\in\mathbb R^m  then  \|y\|_\infty \le \sqrt{y_1^2+\ldots+y_m^2}  and therefore,  \|Ax\|_\infty \le \|Ax\|_2  for all  x\in\mathbb R^n .    Moreover,  \|Ax\|_2\le \|A\|_2\|x\|_2\le \sqrt{n}\|A\|_2\|x\|_\infty . Hence, if  x\ne 0 ,  \frac{\|Ax\|_\infty}{\|x\|_\infty}\le \sqrt{n}\|A\|_2.  Taking the  \sup_{x\ne 0}  on the LHS yields the result.  	0.8012787699699402
1211799	The answer is yes.  The upper bound I come up with below is  \frac{1}{1 - \|A_0\|_2} .    It doesn't seem like there's any need to consider the matrix  A_0  itself.  In the below, we will take  M = \|A\|_0 \geq 0 , since no other information about  A_0  will be used.    Suppose that  M \geq 1 .  Then  \mathcal E  includes the identity matrix, and so we fail to have  \rho(A) < 1 .    Suppose that  M < 1 . We note that   \|(I-A)^{-1}\|_2 = \left\|\sum_{k=0}^\infty A^k\right\|_2 \leq  \sum_{k=0}^\infty \left\|A^k\right\|_2 \leq \sum_{k=0}^\infty \left\|A\right\|_2^k \leq \sum_{k=0}^\infty M^k = \frac{1}{1 - M}   	0.8010826110839844
353570	It is just an application of the Cauchy-Swartz inequality. For any  a>0   \begin{align} \int_\mathbb{R}|f|&=\int_{|x|\le a}|f|+\int_{|x|>a}|x\,f|\,\frac{1}{|x|}\\ &\le\Bigl(\int_{|x|\le a}|f|^2\Bigr)^{1/2}\Bigl(\int_{|x|\le a}1\Bigr)^{1/2}+ \Bigl(\int_{|x|>a}|x\,f|^2\Bigr)^{1/2}\Bigl(\int_{|x|>a}\frac{1}{|x|^2}\Bigr)^{1/2}\\ &\le\sqrt{2\,a}\,\|f\|_2+\frac{\sqrt2}{\sqrt{a}}\|x\,f\|_2. \end{align}  Choose  a=b^2\|x\,f\|_2/(2\,\|f\|_2)  to get   \|f\|_1\le\Bigl(b+\frac2b\Bigr)\sqrt{\strut\|f\|_2\,\|x\,f\|_2}\ .   Choose  b=\sqrt2  and square to get   \|f\|_1^2\le8\,\|f\|_2\,\|x\,f\|_2\ .   	0.8008970618247986
1137014	The operator norm is defined this way   \|A\|^* = \sup_{\|x\|=1} \|Ax\|.   This definition is in such a way that  \|A\|^*<\infty  if and only if  A  is continuous (we are talking about linear maps). Indeed   \|Ab\| = \|b\| \left\| A \frac{b}{\|b\|} \right\| \leq \|b\| \sup_{b\neq 0} \left\| A \frac{b}{\|b\|} \right\| = \|b\|\|A\|^*.   	0.8007674813270569
467853	First I want make a remark on the fact of not specifying the space on which we work(your matrices are square or not?), it is also important to give us  your norms definitions, so your question will be self-understood, especially if you consider the differences found in the notations  used in literature.    In case you are not restricted to search for a specific constant, you can always use the standard method found here :  Equivalence of Norms in Finite Dimension .    In your case ( I suppose that  A \in \mathbb{R}^{n\times n} )    ||A||_{F} \leq \sqrt{n} ||A||_{2}  is trivial  if you consider the definition by using the singular values.      ||A||_{F} = \sqrt{\sum_{i=1}^{n} \sigma_i(A)^2}  ( \sigma_i(A)  are the singular values of A, in decreasing order, in this case because A is square, singular values are simply the absolute value of eigenvalues)    ||A||_{F} \leq \sqrt{n \sigma_1(A)^2} = \sqrt{n}\sigma_1(A) = \sqrt{n} ||A||_{2}     after that, you can find  here  how you can prove that :    ||A||_2 \leq \sqrt {n} ||A||_1    with this two inequalities you get that     ||A||_{F} \leq n||A||_{1}    I have no idea haw you can get  \sqrt{n}  instead of  n  	0.7990503311157227
1038851	Hint. Let  N\geq |a| , then for  n>N , we have that \begin{align*} 0\leq \frac{|a|^n}{n!}&=\frac{|a|}{n}\cdot \frac{|a|^{n-N-1}}{(n-1)\cdots (N+1)}\cdot \frac{|a|^{N}}{N!}\\ &\leq \frac{|a|}{n}\cdot \left(\frac{|a|}{N+1}\right)^{n-N-1}\cdot \frac{|a|^{N}}{N!}\\ &\leq \frac{|a|}{n}\cdot \frac{|a|^{N}}{N!} \end{align*} then apply the Squeeze Theorem as  n\to \infty .  	0.7989110946655273
851001	This boils down to the question, whether for symmetric  A    \|Ax\|_2^2\le \|A\|_\infty \cdot x^TAx   holds for all  x .    This is not true without positive definiteness of  A  (take  A=-I ).    It is true for positive definite  A :   \|Ax\|_2^2 =\|A^{1/2}A^{1/2}x\|_2^2 \le \|A^{1/2}\|_2^2 \cdot \|A^{1/2}x\|_2^2 \le  \|A^{1/2}\|_2^2\cdot x^TAx.   Since  A^{1/2}  is symmetric positive definite   \|A^{1/2}\|_2^2 = \lambda_\max(A^{1/2})^2 = \lambda_\max(A) \le \|A\|_\infty,   which is the claim. Note that we can use any matrix norm on  \mathbb R^{n,n}  instead of  \|\cdot\|_\infty .  	0.798270046710968
750882	If  A  is nonsingular, then  AA^{-1} = I , so     1 = ||I|| = ||AA^{-1}|| \leqslant ||A||\cdot||A^{-1}||.     In general, then  1 \leqslant ||A||\cdot||A^{-1}|| \implies ||A||^{-1} \leqslant ||A^{-1}|| .      Equality is thus not necessarily guaranteed for arbitrary nonsingular  A ; however, the inequality above implies that  equality  may occur. Consider an example.     Example :     A = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}, A^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 0.5 \end{bmatrix}     \implies ||A||_{1,2,\infty} = 2    \implies ||A^{-1}||_{1,2,\infty} = 1    \implies \frac{1}{2} = ||A||_{1,2,\infty}^{-1} \neq ||A^{-1}||_{1,2,\infty} = 1 \implies ||A||^{-1} \neq ||A^{-1}||.    	0.7982600331306458
1221782	Let  \|\cdot\|  be a matrix norm.    It is known that the  spectral radius   r(A) = \lim_{n\to\infty} \|A^n\|^{\frac1n}  has the property  |\lambda| \le r(A)  for all  \lambda\in \sigma(A) .    Indeed, let  \lambda \in \mathbb{C}  such that  |\lambda| > r(A) .    Then  I - \frac1{\lambda} A  is invertible. Namely, check that the inverse is given by  \sum_{n=0}^\infty\frac1{\lambda^n}A^n .    This series converges absolutely because  \frac1{|\lambda|}  is less than the radius of convergence of the power series  \sum_{n=1}^\infty \|A\|^nx^n , which is  \frac1{\limsup_{n\to\infty} \|A^n\|^{\frac1n}} = \frac1{r(A)} .    Hence  \lambda I - A = \lambda\left(I - \frac1{\lambda} A\right)    is also invertible so  \lambda \notin \sigma(A) .    Now using submultiplicativity we get  \|A^n\| \le \|A\|^n  so    |\lambda| \le r(A) = \lim_{n\to\infty} \|A^n\|^{\frac1n} \le \lim_{n\to\infty} \|A\|^{n\cdot\frac1n} = \|A\|  	0.7979117035865784
1044050	Let  A=(a_{ij})  and  x=(x_i)^t\in\Bbb R^n . Then    \|Ax\|=\sqrt{\sum_i \left(\sum_j a_{ij}x_j\right)^2}\le     \mbox{(by Cauchy-Schwarz inequality)}    \sqrt{\sum_i \left(\sum_j a_{ij}^2\right)\left(\sum_j x_{j}^2\right)}=    \sqrt{\sum_i \left(\sum_j a_{ij}^2\right)}\|x\|\le    \sqrt{\sum_i \left(\sum_j \|A\|_\infty^2\right)}\|x\|=    \sqrt{n^2\|A\|_\infty^2}\|x\|=    n\|A\|_\infty\|x\|.    So  \| A \|/n \le \| A \|_\infty.      This bound is tight, for instance, for  A=J , where  J  is the  n\times n  matrix of all ones. Indeed, in this case if  x=(x_i)^t  then  \|Ax\|=\sqrt{n}\left|\sum x_i\right|\le   \mbox{(by the inequality between arythetic and quadratic means)}   n\sqrt{\sum x_i^2}=n\|x\|.  On the other hand, this inequality is tight because  \|A\bar1\|=n\|\bar1\| ,  where  \bar 1  is the n-dimensional vector of all ones.  	0.7973580956459045
676997	Since  \| \alpha v \|  =  \vert \alpha \vert \cdot \| v \|  (it's the property from definition of norm or length) then  \| \hat{e}_j(P) \|= \left \| \frac{\frac{\partial \vec{r}}{\partial q_i}}{ \left \| \frac{\partial \vec{r}}{\partial q_i} \right \|} \right \| = \frac{\left \| \frac{\partial \vec{r}}{\partial q_i} \right \|}{\left \| \frac{\partial \vec{r}}{\partial q_i} \right \|} = 1 .  	0.797207772731781
484563	The proofs use just easy matrix manipulations and the norm submultiplicativity. More general results can be found in the  work by van der Sluis .    Let  A=[a_1,\ldots,a_n] . We have  d_i=\|A\|_1/\|a_i\|_1 .  \newcommand{\cond}{\mathrm{cond}_1}  To show the second inequality, we have (denoting  [\cdot]_i  the  i th column of  \cdot ) as you already got:  \tag{1} \|AD\|_1=\max_{1\leq i\leq n}\|[AD]_i\|_1=\max_{1\leq i\leq n}\|a_i\|_1\frac{\|A\|_1}{\|a_i\|_1}=\|A\|_1.   For an arbitrary nonsingular and diagonal  C , we have   \begin{split} \|(AD)^{-1}\|_1&=\|D^{-1}CC^{-1}A^{-1}\|_1\leq\|D^{-1}C\|_1\|(AC)^{-1}\|_1 \\&=\frac{1}{\|A\|_1}\max_{1\leq i\leq n}(\|a_i\|_1|c_{ii}|)\|(AC)^{-1}\|_1=\frac{1}{\|A\|_1}\|AC\|_1\|(AC)^{-1}\|_1\\&=\frac{1}{\|A\|_1}\cond(AC). \end{split}   Hence  \tag{2} \cond(AD)=\|AD\|_1\|(AD)^{-1}\|_1\leq\cond(AC).   Note that (2) implies that   \cond(AD)=\min_{\substack{\text{$C$ diagonal}\\\text{nonsingular}}}\cond(AC).     We can show the second part of the first set of inequalities also using (2) with  C=I :   \cond(AD)\leq \cond(AI)=\cond(A).   For the other, we have from (1)   \begin{split} \cond(A)&=\|A\|_1\|A^{-1}\|_1=\|DA\|_1\|DD^{-1}A^{-1}\|_1\\&\leq\|DA\|_1\|D\|_{\infty}\|(AD)^{-1}\|_1=\|D\|_{\infty}\cond(AD). \end{split}   	0.7970713973045349
885240	First, you are missing a square root:  \|x\|_2=\sqrt{\int_{a}^{b}\|x(t)\|^2\ dt}    Since for all  t \in [a, b], \|x(t)\| \le \|x\|_\infty = \sup\{\|x(t)\| : t\in[a,b]\} , we have that  \int_{a}^{b}\|x(t)\|^2\ dt \le \int_{a}^{b}\|x\|_\infty^2\ dt = (b-a)\|x\|_\infty^2  and so  \|x\|_2 \le (\sqrt{b-a})\|x\|_\infty    This inequality cannot be improved upon, as it is an equality when  \|x(t)\|  is constant. So the question of which is larger depends on how far away  a  and  b  are from each other. When  b - a \ge 1 , then the supremum norm is always larger. when  b - a < 1 , then either norm can be larger, depending on the map  x .    For a more general comparison, see HÃ¶lder's inequality.  	0.796200156211853
1086029	Let  n\in\mathbb N  and let  e(n)\in\ell^p  be the sequence such that e(n)_k=\begin{cases}1&\text{ if }k=n\\0&\text{ otherwise.}\end{cases} Then  \bigl\|e(n)\bigr\|_p=1  and  \bigl|T\bigl(e(n)\bigr)\bigr|=|a_n| . Therefore,  (\forall n\in\mathbb{n}):\|T\|\geqslant|a_n| . Since this happens for each  n\in\mathbb N ,  \|T\|\geqslant\|a\|_\infty .    On the other hand, if  \|x\|_p=1 , then \bigl|T(x)\bigr|=\sqrt[p]{\sum_{n=0}^\infty|a_nx_n|^p}\leqslant\|a\|_\infty\sqrt[p]{\sum_{n=0}^\infty|x_n|^p}=\|a\|_\infty\|x\|_p. So,  \|T\|=\|a\|_\infty .  	0.7959557771682739
409536	When  $A^+x$  is nonzero, so is  $AA^+x$ . Therefore  \begin{aligned} \|A^+\|_2 &= \max\limits_{x\in\mathbb C^m\setminus0}\frac{\|A^+x\|_2}{\|x\|_2}\\ &\le\max\limits_{x\in\mathbb C^m\setminus0,\ A^+x\ne0}\frac{\|A^+x\|_2}{\|AA^+x+(I-AA^+)x\|_2}\\ &\le \max\limits_{x\in\mathbb C^m\setminus0,\ A^+x\ne0}\frac{\|A^+x\|_2}{\|AA^+x\|_2}\\ &\le \max\limits_{y\in\mathbb C^n\setminus0}\frac{\|y\|_2}{\|Ay\|_2}, \end{aligned}  If we replace  A  by  $A_1$ , all inequalities above become equalities. That is,  $$ \|A_1^{-1}\|_2 = \max\limits_{x\in\mathbb C^n\setminus0}\frac{\|A_1^{-1}x\|_2}{\|x\|_2} = \max\limits_{x\in\mathbb C^n\setminus0}\frac{\|A_1^{-1}x\|_2}{\|A_1A_1^{-1}x\|_2} = \max\limits_{y\in\mathbb C^n\setminus0}\frac{\|y\|_2}{\|A_1y\|_2}. $$  The result now follows because  \|A_1y\|_2\le\|Ay\|_2  for every vector  $y$ .  	0.795470654964447
1062200	Let   F_n(a):=\frac{a^n}{\prod_{i=1}^n(1+a^i)}.  Note that if  0<|a|<1   then   0<|F_n(a)|\leq \frac{|a|^n}{\prod_{i=1}^n(1-|a|^i)} \leq \frac{|a|^n}{1-\sum_{i=1}^n|a|^i} =|a|^n\left(1-\frac{|a|(1-|a|^{n+1})}{1-|a|}\right)^{-1}\to 0  as  n  goes to infinity. For  a=1 ,  0 1 , then  0<|1/a|<1  and for  n\to+\infty ,  |a|^{\frac{n^2-3n}{2}}\to +\infty , and   |F_n(a)|=\frac{|F_n(1/a)|}{|a|^{\frac{n^2-3n}{2}}}\to 0.  	0.7953842282295227
1198309	For simplicity, we write    P(A) = \int_{0}^{\infty} e^{(B-A)^*t}A^*Ae^{(B-A)t} \, dt.    Plugging  A = nI  and applying the substitution  u = nt , we obtain     P(nI) = n\int_{0}^{\infty} e^{\frac{1}{n}B^*u}e^{\frac{1}{n}Bu}e^{-2u}\,du    Now notice that   \left\| e^{\frac{1}{n}Bu} - I \right\| \leq e^{\frac{1}{n}\|B\|u} - 1   and the same inequality holds with  B^*  in place of  B , where,  \| \cdot \|  denotes the operator norm. (Since any two matrix norms are equivalent, using  \|\cdot\|  in place of  \|\cdot\|_F  poses no hassle.) Then    \begin{align*} \left\| \frac{1}{n}P(nI) - \frac{1}{2}I \right\| &\leq \int_{0}^{\infty} \left\| e^{\frac{1}{n}B^*u}e^{\frac{1}{n}Bu} - I \right\| e^{-2u} \, du \\ &\leq \int_{0}^{\infty} \left( e^{\frac{1}{n}(\|B\|+\|B^*\|)u} - 1 \right) e^{-2u} \, du \\ &= \frac{1}{2-\frac{1}{n}(\|B\|+\|B^*\|)} - \frac{1}{2}  = \mathcal{O}\left(\frac{1}{n}\right). \end{align*}    Thus for any non-zero vector  x ,    \begin{align*} \left| \langle x, P(nI) x \rangle - \frac{n}{2}\| x\|^2 \right| \leq n \left\| \frac{1}{n}P(nI) - \frac{1}{2}I \right\| \|x\|^2 = \mathcal{O}(\|x\|^2) \end{align*}    and therefore   \langle x, P(nI) x \rangle \geq \left(\frac{n}{2}-\mathcal{O}(1)\right)\|x\|^2   is unbounded as  n\to\infty .  	0.7952887415885925
571530	The operator norm is defined (equivalently) as any of :   \|A\| = \max_{\|x\|=1}\|Ax\|   \|A\| = \max_{\|x\|\leq1}\|Ax\|   \|A\| = \max_{\|x\|\neq0}\frac{\|Ax\|}{\|x\|}    Above, you test against  k = (0,1,1)  (I'm not sure what you mean by  e_1k_2+e_2k_3)  - you're multiplying vectors here, which makes little sense in context, and none at all without specifying if you are using the cross or dot product), which has (Euclidian) norm  \sqrt{2} , so if you wish to use it to test your operator norm, you need to use the third formula above, and  \frac{\|Ak\|_2}{\|k\|_2} = \frac{\|(1,1,0)\|_2}{\|(0,1,1)\|_2} = \frac{\sqrt{2}}{\sqrt{2}} = 1 , so you have a witness that  \|A\| \geq 1 . In order to prove that  \|A\| = 1 , you now need to show that  \|Ax\|_2 \leq \|x\|_2  for all  x \in\mathbb{R}^3  (or possibly faster, using the first formula,  \|Ax\|_2 \leq 1  for all  x \in \mathbb{R}^3  with  \|x\|_2 = 1 .    (NB: Throughout (except in the definition of the operator norm, which applies on any normed linear space) I've made the use of the Euclidian norm explicit by denoting it  \|\cdot\|_2  - this is not actually important).  	0.7949377298355103
758234	Following the hint, we have that   \|f^n\|_1 = \langle e^{i\varphi_n} , f^n \rangle_{L^2}=\langle g, \widehat{f}^n\rangle_{\ell^2} .   Here,  g\in\ell^2(\mathbb Z)  is (the Fourier transform of  e^{i\varphi_n}  and) an element of norm  1 , and  \widehat{f}^n  is a genuine  n th power. Thus, by Cauchy-Schwarz,   \|f^n\|_1^{1/n} \le \|\widehat{f}\|_{2n} ,   and we obtain the other inequality from the fact that if  a\in\ell^p  for some  p<\infty , then  \lim_{p\to\infty} \|a\|_p=\|a\|_{\infty}  (justifying the notation).    This is easy to prove (and will be in many texts). We can assume that  \|a\|_{\infty}=1 . Then   \sum_{|a_n|<1} |a_n|^p \to 0   by dominated convergence, so   1 \le \sum |a_n|^p \le C   for all large  p .  	0.7948787212371826
1027658	Hint. Note that  for all  n\geq 4|a|^2 , we have that  2|a|\leq \sqrt{n}  which imples  0\leq \left|\frac{a^n}{n^{n/2}}\right|=\left(\frac{|a|}{\sqrt{n}}\right)^n\leq \frac{1}{2^n}  then use the  Squeeze Theorem .  	0.7947379946708679
612036	Let  r(A)=max\{|\lambda|:\lambda\ is\ eigenvalue\ of\ A\} .  Then  ||A||_2=\sqrt{r(A^TA)}    Now, for any matrix norm  ||\cdot||  we have  r(A)\leq||A|| .    Thus  ||A||_2=\sqrt{r(A^TA)}\leq\sqrt{||A^TA||_\infty}\leq\sqrt{||A^T||_\infty||A||_\infty}=\sqrt{||A||_1||A||_\infty}  	0.794725775718689
899751	From your very definition of  W^{1,2} (\Bbb R)  you have that    |f(x) - f(y)| = \left| \int \limits _y ^x f'(z) \ \Bbb d z \right| = | \langle f', 1 \rangle _{L^2} | \le \| f' \| _{L^2} \ \| 1 \| _{L^2} = \\ \| f' \| _{L^2} \sqrt {\int \limits _y ^x 1^2 \ \Bbb d x} = \| f' \| _{L^2} \sqrt{|x-y|} ,    where I have used the Cauchy-Schwarz inequality in  L^2(\Bbb R) .  	0.7946922183036804
1056958	The inequality holds because for every nonzero vector  x ,   \frac{\sqrt{m}\,\|Ax\|_\infty}{\|x\|_\infty} \ge \frac{\sqrt{m}\,\|Ax\|_\infty}{\|x\|_2} \ge \frac{\|Ax\|_2}{\|x\|_2}.\tag{1}   So, for  \sqrt{m}\|A\|_\infty=\|A\|_2 , you need  \|x\|_2=\|x\|_\infty  and  \sqrt{m}\,\|Ax\|_\infty= \|Ax\|_2  at some  x  which maximises both the LHS and the RHS of  (1) . The former equality means that  x , up to a multiple, is the  k -th standard basis vector  e_k . In turn, the latter equality means that all entries of the  k -th column of  A  have the same magnitudes, and  \|A\|_\infty=|a_{ik}|  for every  i . However, for  e_k  to be a maximiser of  \|Ax\|_\infty/\|x\|_\infty , all other columns of  A  must be zero, otherwise for every  j\ne k  such that  Ae_j  is nonzero, we have  \|A(e_k+t e_j)\|_\infty>\|Ae_k\|_\infty  when  t\ne0  is sufficiently small and its sign is chosen appropriately.    In short,  A  has at most one nonzero column and all entries in this column have identical magnitudes.  	0.7943287491798401
1085669	Here's another elementary proof (which also works for general Banach algebras and nontheless avoids complex analysis) based on  Rickart's proof of the Gelfand formula    *First things first: the limit of  $\|A^n\|^{1/n}$  exists by virtue of the subadditivityof the sequence  $a_n=\log(\|A^n\|^{1/n})$  and Fekete's lemma. The limit equals  $\inf_{n\in \mathbb{N}}\|A^n\|^{1/n}$    *Next, it is easy to see that  $\rho(A) \leq \nu:=\lim_{n \to \infty}\|A^n\|^{1/n}$ . In the matrix-case, just act with  $A^n$  on the leading eigenvector and see what happens in order to arrive at this conclusion.    *So we still have to prove that  $\nu \leq \rho(A)$ .     First suppose that  $\nu=0$ . I will show that  $0$  must be an eigenvalue then, which suffices for our proof in this case. If  $0$  were not an eigenvalue of  $A$ , then  $A$  is invertible and taking the norm of  $I=A^n (A^{-1})^n$  yields  $1 \leq \|A^n\|^{1/n} \|(A^{-1})^n\|^{1/n} \leq \|A^n\|^{1/n}\|A^{-1}\|\to \nu \|A^{-1}\|$  which results in a contradiction against the assumption  $\nu=0$ .    From now on suppose that  $\nu>0$ . Assume that  $\rho(A)<\nu$  (anticipating a contradiction). The following identity, which holds for any  $q>\rho(A)$ , will come in handy,  $$\left(\frac{A^n}{q^n}-I\right)^{-1}=\frac{1}{n}\sum_{j=0}^{n-1} \left(\frac{A}{\omega_n^jq}-I\right)^{-1}\qquad (1)$$  where  $\omega_n\in \mathbb{C}$  is a primitive n'th root of unity. This identity can be verified by multiplying both sides by  $\frac{A^n}{q^n}-I=\frac{A^n}{(\omega_n^j q)^n}-I$ . Fix  $\varepsilon>0$  (we will later send it to 0). By our assumption that  $\rho(A) <\nu$ , both  $\nu$  and  $\nu_\varepsilon:=\frac{\nu}{1-\varepsilon}$  can substitute  $q$  in the identity (1). Hence  $$\left\|\left(\frac{A^n}{\nu_\varepsilon^n}-I\right)^{-1}-\left(\frac{A^n}{\nu^n}-I\right)^{-1}\right\| \leq \frac{1}{n}\sum_{j=0}^{n-1} \left\|\left(\frac{A}{\omega_n^j\nu_\varepsilon}-I\right)^{-1}-\left(\frac{A}{\omega_n^j\nu}-I\right)^{-1}\right\|$$   $$=\frac{1}{n}\sum_{j=0}^{n-1}|\nu_\varepsilon^{-1}-\nu^{-1}|\left\|\left(\frac{A}{\omega_n^j\nu_\varepsilon}-I\right)^{-1}A\left(\frac{A}{\omega_n^j\nu}-I\right)^{-1}\right\|$$   $$\leq \frac{\varepsilon}{n\nu}\sum_{j=0}^{n-1}\left\|\left(\frac{A}{\omega_n^j\nu_\varepsilon}-I\right)^{-1}\right\|\|A\|\left\|\left(\frac{A}{\omega_n^j\nu}-I\right)^{-1}\right\|$$  where I used a variant of the resolvent identity in the step in the middle. The key now is to find an  $M>0$  which bounds the individual terms in the final sum from above. To find  M  one establishes the continuity of the map  $\varphi: A\subset \mathbb{C} \to M^{m \times m}: z \mapsto \left(\frac{A}{z}-I\right)^{-1}$  where  $A$  is the (compact) annulus  $\{z \in \mathbb{C}:\,\nu\leq |z|\leq 2\nu\}$  (easy and therefore omitted). Compactness of  $A$  means that  $\|\varphi\|$  reaches a maximum  $N<+\infty$ . Wrapping everything together, we then find  $$\left\|\left(\frac{A^n}{\nu_\varepsilon^n}-I\right)^{-1}-\left(\frac{A^n}{\nu^n}-I\right)^{-1}\right\| \leq \frac{\varepsilon N^2 \|A\|}{\nu} = C \varepsilon\qquad (2)$$  where it is important that  $C$  does not depend on  n . Letting  $n \to \infty$ , we have  $\frac{\|A^n\|^{1/n}}{\nu_{\varepsilon}} \to 1-\varepsilon$  and therefore  $\frac{\|A^n\|}{\nu_{\varepsilon}^n} \to 0$  and therefore  $\frac{A^n}{\nu_{\varepsilon}^n} \to 0$  and therefore  $\left(\frac{A^n}{\nu^n}-I\right)^{-1} \to -I$  and therefore (since the right hand side of (2) can be made arbitrarily small) necessarily  $\left(\frac{A^n}{\nu^n}-I\right)^{-1}\to -I+O(\varepsilon)$  which requires that  $\frac{A^n}{\nu^n}=O(\varepsilon)$ . But that is impossible since  $\nu=\inf_{n\in \mathbb{N}}\|A^n\|^{1/n}$ . So we arrive at the anticipated contradiction.  	0.7938609719276428
632614	I have used such things before in work I've done.  We get:  \begin{align} &\sum_{t=1}^T \frac{1}{\sqrt{t}}(||x_t-a||^2 - ||x_{t+1}-a||^2) \\ &= ||x_1-a||^2 - \frac{1}{\sqrt{T}}||x_{T+1}-a||^2 + \sum_{t=2}^{T}||x_t-a||^2\left(\frac{1}{\sqrt{t}}-\frac{1}{\sqrt{t-1}}\right) \\ &\leq ||x_1-a||^2 - \frac{1}{\sqrt{T}}||x_{T+1}-a||^2 \end{align}  	0.7938284277915955
563205	"The third is equivalent to the second for the following reason: suppose that  x  is such that  \|x\| \leq 1 .  We then have   \|Ax\| =   \left\|\|x\|\cdot A\left(\frac{x}{\|x\|}\right)\right\| =  \|x\| \left\|A\frac{x}{\|x\|}\right\| \leq 1 \cdot \left\|A\frac{x}{\|x\|}\right\| \leq \max_{\|u\| = 1} \|Au\|   Of course, the other inequality holds because  \{x:\|x\|  = 1\} \subseteq \{x:\|x\| \leq 1\} .    What it comes down to is that in the third definition, we've ""pointlessly added"" the set  \{x:\|x\| < 1\}  to our search space from the second definition even though it's impossible to achieve a maximum on this interior.    Your new norm with the  5  will have the same result as defining   \|A\| = \max_{ \{ x \vert \Vert x \Vert = 5 \} } \Vert Ax \Vert  , which you can see yields a different result from the second.  "	0.793770968914032
661828	Inequality  \frac{1}{\sqrt{n}}\|x\|_1 \leq \|x\|_2  can be proven by Cauchy-Schwarz inequality:   \|x\|_1=\sum_{i=1}^n |x_i|\cdot 1 \leq \sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n 1^2}=\sqrt{n}\|x\|_2.  	0.79365074634552
1282614	It is :    \mathbf{\|(1-A)^{-1}\| \leq \frac{1}{1-\|A\|}}    But, also, as you proved by using the triangle inequality :    \begin{align*} \|(\mathbf{1}-\mathbf{A})^{-1}-\mathbf{1}\| &\leq\|(\mathbf{1}-\mathbf{A})^{-1}\|+\|\mathbf{1}\|\\  & \leq \mathbf{\frac{1}{1-\|A\|}} + \|\mathbf{1}\| \leq  \mathbf{\frac{\mathbf{\|A\|}}{1-\|A\|}} \end{align*}    Important note :  Since we are talking about matrices, try to be careful not to make things tangled with the number  1 \in \mathbb R  and the  \mathbf{1} \equiv \mathbf{I}  matrix which is essentialy the element  1  but in  C^{n \times n} .  	0.7936211824417114
1128179	Let the Riesz potential  I_\alpha f(x) =\int_{\Bbb R^n}\frac{f(y)}{|x-y|^{n-\alpha }}dy    Note  (I_\alpha f, g) = \int_{\Bbb R^n}\int_{\Bbb R^n}\frac{f(y)g(x)}{|x-y|^{n-\alpha }}dydx    and we know  by Riesz representation theorem that     \|I_\alpha f\|_{q'} =\sup_{\|g\|_q = 1} |(I_\alpha f, g)|  Where  \frac{1}{q}+\frac{1}{q'}=1.  Whereas by Hardy-Littlewood-Sobolev inequality we have     |(I_\alpha f, g)| \le C\| f\|_p \| g\|_{q}~~~~~\frac{1}{p}+\frac{\alpha}{n}+\frac{1}{q}=2.    Hence  \|I_\alpha f\|_{q'} =\sup_{\|g\|_q = 1} |(I_\alpha f, g)|\le C\| f\|_p ~~~~\frac{1}{p}+\frac{\alpha}{n}+1-\frac{1}{q'}=2.    Taking  q'=1 ,     \|I_\alpha f\|_{1} \le  C\| f\|_p ~~~~\frac{1}{p}+\frac{\alpha}{n}=2.  	0.7935835123062134
820086	Your approach works, with some care: we see that: \begin{align*} |a_{n+1}-\sqrt{2}|&=|\frac{a_n(1-\sqrt{2})+(2-\sqrt{2})}{a_n+1}| \\ &\le |\sqrt{2}-1||a_n-\sqrt{2}| \end{align*}Letting  |a_0-\sqrt{2}|=c , this proves that  |a_n-\sqrt{2}|\le c|\sqrt{2}-1|^n , which goes to  0 .  	0.7930483222007751
1015154	Thanks to those that have already responded, you were very helpful. Here I will give the solution I have come up with, with more exposition than is provided in some of the other responses.    First we need the following lemma:    Lemma :  \lim_{\|x\| \to \infty} f(x) = \infty . Some authors refer to this as  f  being  coercive .    Proof: Let  x_0 \in \mathbb{R}^n  and let  v  be a subgradient of  g  at  x_0 , i.e.  x_0 \in \partial g(x_0) . By equivalence of norms in finite-dimensional vector spaces, there exists a constant  c > 0  such that  \|x\|_2 \leq c \|x\|  for all  x \in \mathbb{R}^n . By Cauchy-Schwarz and the trinagle inequality, for  \|x\| > 0  we have     \begin{align*} \frac{| v^T(x - x_0) |}{\frac{m}{2}\|x\|^2} &\leq \frac{\|v\|_2 \|x - x_0\|_2}{\frac{m}{2}\|x\|^2} \\ &\leq \frac{\|v\|_2 \|x\|_2 + \|v\|_2 \|x_0\|_2}{\frac{m}{2}\|x\|^2} \\ &\leq \frac{c\|v\|_2 \|x\| + \|v\|_2 \|x_0\|_2}{\frac{m}{2}\|x\|^2} \\ &= \frac{2c\|v\|_2 \|x\|}{m} \frac{1}{\|x\|} + \frac{2\|v\|_2 \|x_0\|_2}{m} \frac{1}{\|x\|^2} \end{align*}     The far right hand side of this inequality  \to 0  as  \|x\| \to \infty , which implies that  v^T(x - x_0) + \frac{m}{2} \|x\|^2 \to \infty  as  \|x\| \to \infty . Now we use the definition of subgradient:     \begin{align*} v^T(x - x_0) &\leq g(x) - g(x_0) \\ v^T(x - x_0) + \frac{m}{2}\|x\|^2 &\leq g(x) + \frac{m}{2}\|x\|^2 - g(x_0) \\ v^T(x - x_0) + \frac{m}{2}\|x\|^2 + g(x_0) &\leq f(x) \end{align*}     The left hand side of this  \to \infty  as  \|x\| \to \infty , so we conclude that  f(x) \to \infty  as  \|x\| \to \infty .  \square    On to the main result. First, assume that  A  is unbounded. If it is bounded, then it is compact, and the result follows immediately. There are 2 mutually exclusive possibilities:    Case 1:  f  has a minimizer on  A , in which case it is unique (see  this  thread).    Case 2:  f  does not have a minimizer on  A .     Assume we have case 2. Let  f^\star := \inf_{x \in A} f(x) .  f^\star < \infty  by assumption. Let  (x_k)  be a sequence in  A  such that  f(x_k) \to f^\star . We now have two mutually exclusive subcases:    Subcase 2.1:  \sup_k \|x_k\| = d < \infty . Define  B_d := \{ x \in \mathbb{R}^n \ : \ \|x\| \leq d\} . Then for all  k ,  x_k \in \{ A \cap B_d \}  which is closed and bounded and hence compact. Therefore  (x_k)  converges in  A , i.e.  x_k \to x^\star  for some  x^\star \in A . Continuity of  f  then implies  f(x^\star) = f^\star , which is a contradiction.    Subcase 2.2:  \sup_k \|x_k\| = \infty . This implies  \|x_k\| \to \infty ,  and by the Lemma this implies  f(x_k) \to \infty  which implies  f^\star = \infty  which contradicts  f^\star < \infty .    Thus we conclude that Case 2 cannot occur, and therefore Case 1 must occur.    EDIT: After writing all of this out, it is clear that  f  strongly convex is a stronger assumption than we require.  f  strictly convex and coercive is sufficient for  f  to have a unique global minimum on the convex set  A .  	0.7921259999275208
354627	Suppose that  \frac14l\le a_n\le4l , which is equivalent to  \frac12\sqrt{l}\le\sqrt{a_n}\le2\sqrt{l} . Then,   \left|\,\sqrt{a_n}-\sqrt{l}\,\right|=\frac{\left|\,a_n-l\,\right|}{\sqrt{a_n}+\sqrt{l}}   Therefore,   \frac1{3\sqrt{l}}\,\left|\,a_n-l\,\right|\le\left|\,\sqrt{a_n}-\sqrt{l}\,\right|\le\frac2{3\sqrt{l}}\,\left|\,a_n-l\,\right|   These two inequalities, along with the definition of a limit, shows that   \lim_{n\to\infty}\sqrt{a_n}=\sqrt{\lim_{n\to\infty}a_n}        Comment on the Answer in the Question    There is no reason that the coefficient of  \left|\,a_n-l\,\right|  must be less than  1 .  In this case. all you need to show is that for a given  l , each side is less than a constant multiple of the other.     For example, say we know that   \left|\,\sqrt{a_n}-\sqrt{l}\,\right|\le100\left|\,a_n-l\,\right|   We can make the left side as small as we want simply by making the right side  \frac1{100}  the size, and we know we can do that because   \lim_{n\to\infty}\left|\,a_n-l\,\right|=0   	0.7919779419898987
872446	More generally, consider  1\leq r\leq p . Assume  x\neq 0 . From  |x_i|\leq \|x\|_p  we get  that  1= \sum_i \left( \frac{|x_i|}{\|x\|_p} \right)^p \leq \sum_i \left( \frac{|x_i|}{\|x\|_p} \right)^r = \left( \frac{\|x\|_r}{\|x\|_p}\right)^r  from which  \|x\|_p\leq \|x\|_r . I didn't see a natural way to involve HÃ¶lder inequalities in this estimate.  	0.79197096824646
662547	just calculate  Ax  you will have a clumn matrix  m\times 1  which all elements are  x_1 , thus  ||A||_2 = \sup_{ x \in \mathbb{R}^m } \frac{ ||Ax||_2}{||x||_2 }\leq\frac{ ââ\sqrt{m}|x_1|}{||x||_2 }\leq\frac{ \sqrt{m}||x||_2ââ}{||x||_2 }\leq ââ\sqrt{m} . Now let  x=(1,0,0,...,0)  then  Ax=(1,1,1,...,1)  thus  \frac{ ||Ax||_2}{||x||_2 }=\frac{ \sqrt{m}}{1 }=\sqrt{m}  thus  ||A||_2\geq \sqrt{m}  	0.7915951609611511
250951	From Holder's inequality,  \|f\|_n^n=\int_E|f|^n\leq\left(\int_E|f|^{n\frac{n+1}{n}}\right)^\frac{n}{n+1}\left(\int_E1\right)^\frac{1}{n+1}=\|f\|_{n+1}^n\mu(E)^\frac{1}{n+1}\Rightarrow \frac{\|f\|_{n+1}^{n+1}}{\|f\|_n^n}\geq \|f\|_{n+1}\mu(E)^{-\frac{1}{n+1}},  and also  \|f\|_{n+1}^{n+1}=\int_E|f|^{n+1}\leq\|f\|_{\infty}\int_E|f|^n=\|f\|_{\infty}\|f\|_n^n\Rightarrow \frac{\|f\|_{n+1}^{n+1}}{\|f\|_n^n}\leq \|f\|_{\infty}.    Now, let  n\to\infty .  	0.7914028167724609
1135472	You mean :   \frac{\|y-x\|}{\|x\|} \le  k(A)\frac{\|r\|}{\|b\|}    Solution    Ax=b     \|Ax\|=\|b\|     \|b\| \le \|A\| \|x\| , x \neq 0     \frac{1}{\|x\|} \le \frac{\|A\|}{\|b\|} , (i)     Now  \|y-x\| = \|y \|- \| A^{-1} b \| = \| A^{-1}Ay-A^{-1}b \|= \| A^{-1}(Ay-b) \|  = \| A^{-1}(r) \| \le \|  A^{-1}\| \|(r)\| , (ii)   So (i),(ii)  \to    \frac{\|y-x\|}{\|x\|} \le \frac{\|A\|\|A^{-1}\|\|r\|}{\|b\|}=k(A)\frac{\|r\|}{\|b\|}   	0.7913143634796143
1019709	We can get a quick upper bound as follows: let  \|x\|  denote the norm (Euclidean length) of a vector, and denote   \|(x_1,\dots,x_n)\|_\infty = \max_{i=1,\dots,n}|x_n|   It is well known that    \max_{x \neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty} = \max_{i=1,\dots,n}\sum_{j=1}^n |a_{ij}|    Thus, for a stochastic matrix  A , we have   \sigma_1(A) = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|} \leq \max_{x \neq 0} \frac{\sqrt{n} \|Ax\|_\infty}{\frac 1{\sqrt{n}}\|x\|_\infty} = n \max_{i=1,\dots,n}\sum_{j=1}^n |a_{ij}|  = n   I'm not sure if this bound is tight.       We can see that it is possible to get a maximum singular value of at least  \sqrt{n} .  In particular, it suffices to consider  A = \mathbf 1 e_1^T  where  \mathbf 1  is the column-vector of  1 s and  e_1  is the standard basis vector  (1,0,\dots,0)^T .   	0.7912835478782654
446096	Any induced norm satisfies  \|A\|\ge \rho(A) , where  \rho(A)  is the spectral radius of  A ,  i.e.  the largest absolute value of its eigenvalues. This fact is stated without proof  in Wikipedia ; here is a proof.        Any induced norm is of the form    \|A\|=\max_x\frac{\|Ax\|}{\|x\|}.  Let  x^*  be an eigenvector corresponding to the eigenvalue  \lambda^*  with the largest absolute value. Then  \|A\|\ge\frac{\|Ax^*\|}{\|x^*\|}=\frac{\|\lambda^*x^*\|}{\|x^*\|}=|\lambda^*|\frac{\|x^*\|}{\|x^*\|}=|\lambda^*|.      Let  A=\begin{bmatrix}1&0\\0&0\end{bmatrix} . Then your norm is  \|A\|=\dfrac1{\sqrt2}\|A\|_F=\dfrac1{\sqrt2}  while  \rho(A)=1 , contradicting the above inequality.  	0.7912057638168335
1237239	Since  M_{d \times d}(\mathbb C)  is a finite-dimensional vector space, any two norms on it are equivalent.  If  c \|\cdot\|_2 \le \|\cdot \|_1 \le C \|\cdot \|_2  where  \|\cdot\|_2  is submultiplicative, then  \| A^n \|_1 \le C \|A^n \|_2 \le C \|A\|_2^n  and    \|\exp(A) \|_1 = \|\sum_{n=1}^\infty A^n/n! \|_1 \le \sum_{n=1}^\infty C \|A\|_2^n/n! = C \exp(\|A\|_2)  	0.7907542586326599
877760	If  A  is positive definite then the eigenvalues of  A  are all positive. In particular, the smallest eigenvalue, say  \lambda_s , will be nonzero. Next observe \begin{align} \frac{1}{2}x^TAx-b^Tx=\frac{1}{2}x^TQ^TDQx-b^Tx=\geq\frac{\lambda_s}{2}\| Qx\|^2-\|b\|\|x\|=\frac{\lambda_s}{2}\|x\|^2-\|b\|\|x\|. \end{align} Thus as  \|x\|\rightarrow \infty  we see that  f  will also go to infinity.    For the converse direction, assume  A  is not positive definite then there is an eigen-direction corresponding to a non-positive eigenvalue, say  x . Then we see that if  \lambda<0  then \begin{align}  f(\alpha x)= \frac{\lambda\alpha^2}{2}\|x\|^2-\alpha b^Tx\leq \frac{\lambda\alpha^2}{2}\|x\|^2+|\alpha|\|b\|\| x\| \end{align} which goes to negative infinity as  |\alpha|\rightarrow \infty . For the case  \lambda=0 , we will leave it as an exercise for the reader.  	0.7905745506286621
958762	The necessary and sufficient condition is that the  spectral radius  of  A  is less than  1 . Equivalently, there exists a positive integer  n  and a submultiplicative matrix norm  \|\cdot \|  such that  \|A^n\|<1 .     If you are looking for an easy-to-check sufficient condition, it's reasonable to consider the  induced norms  for either  \ell^1 - or  \ell^\infty - vector norms. These are    \|A\|_1 = \max_{j}\sum_{i}|a_{ij}|\quad\text{ and }\quad \|A\|_\infty = \max_{i}\sum_{j}|a_{ij}|   If either of these norms is less than  1 , then the inequality  \|Ax\|_p\le \|A\|_p\|x\|_p  (with  p  being  1  or  \infty ) yields convergence to zero.  	0.7905314564704895
1104380	"I think that  E  is a Banach space and  T  is a linear and bounded operator on  E . I am right ?    Your problem is a spacial case of the following: if  A \in L(E)  is invertible, then     \frac{1}{||A^{-1}||}=\inf_{\Vert x\Vert=1}\Vert Ax\Vert.    We have    ||A^{-1}||=\sup_{x \ne 0}\frac{||A^{-1}x||}{||x||}=\sup_{x \ne 0}\frac{||A^{-1}Ax||}{||Ax||} .    The reason for the second  ""="" :  A(E)=E .    Hence  ||A^{-1}||=\sup_{x \ne 0}\frac{||x||}{||Ax||}=\sup_{||x||=1}\frac{1}{||Ax||} .    Can you take it from here ?  "	0.7905087471008301
1019710	The largest singular value  \sigma_1(A)  is equal to its spectral norm  \|A\|_2 , and for any matrix   A \in \mathbb{R} ^{n \times n} , it is known  that  \|A\|_2\le\sqrt{n}\|A\|_\infty   (see bottom of Wikipedia page for other examples of norm equivalences)    Now, for a stochastic matrix  A , it is always the case by definition that actually  \|A\|_\infty = 1 , so that  \|A\|_2\le\sqrt{n} .    Combining this fact with Omnomnomnom's construction, this upper bound cannot be generally improved.  	0.7904631495475769
792720	You correctly observed that  \|x\|_2\le \sqrt{\|x\|_1\|x\|_\infty}  is a special (easy) case of HÃ¶lder's inequality, which essentially amounts to  |x_i|^2\le \|x\|_\infty |x_i| . To prove the second part, first use the arithmetic-geometric means inequality. After that, use HÃ¶lder's inequality again:  \|x\|_1\le \sqrt{n}\|x\|_2  and observe that  \|x\|_\infty \le \|x\|_2 .  	0.7904146909713745
869181	Hint: Consider functions of the form \begin{align} f(x) = \frac{1}{|x-a|^\alpha} \end{align} where  a \in \Omega  and  \alpha>0 .    For instance  \mathbb{R}^n  where  n\geq 3 , let  q = 1  and  p=2 . If  \alpha = n/2 , then we see that \begin{align} \int_{\Omega} \frac{dx}{|x-a|^{n/2}} \leq \int_{B(a, r)} \frac{dx}{|x-a|^{n/2}} = \int^r_0 \int_{|x-a|=\tau} \frac{1}{|x-a|^{n/2}} dS d\tau = \int^r_0 \frac{dx}{\tau^{n/2-1}}<\infty \end{align} where  B(a, r) \supset \Omega  but \begin{align} \int_{\Omega} \frac{dx}{|x-a|^n} \geq \int_{B(a,r_2)} \frac{dx}{|x-a|^n} =\int^{r_2}_0\frac{1}{\tau}\ d\tau = \infty \end{align} where  \Omega \supset B(a, r_2) , which means  f \in L^1(\Omega)  but not in  L^2(\Omega) , i.e.  there doesn't exist constant  C>0  such that \begin{align} || f||_{L^2(\Omega)} \leq C|| f||_{L^1(\Omega)}. \end{align}   	0.7902071475982666
1113766	It might goes this way:  (1-\delta)K\le\frac{\|Ax\|}{\|x\|}\le(1+\delta)K\implies\min_x\frac{\|Ax\|}{\|x\|}\ge(1-\delta)K  and  \min_x\frac{\|Ax\|}{\|x\|}=\min\sigma_A . This is because  \min_x\frac{\|Ax\|}{\|x\|}=\min_x\frac{\|V\Sigma U^*x\|}{\|x\|}=\min_x\frac{\|\Sigma x\|}{\|x\|}=\min\sigma_A .    The last equality:  \frac{\|\Sigma x\|}{\|x\|}=\frac{\sqrt{\sum_i|\sigma_ix_i|^2}}{\sqrt{\sum_ix_i^2}}\ge\frac{\sqrt{\sum_i\min|\sigma_A|^2|x_i|^2}}{\sqrt{\sum_ix_i^2}}=\sigma_A  and  \frac{\|\Sigma e_1\|}{\|e_1\|}=\sigma_A .   	0.7901918292045593
156629	Let  x  be a nonzero vector in  \ker B  and let  X  be the square matrix in which every column is equal to  x . By a suitable scaling of  X , we may assume that  \|X\|=1 . Therefore \begin{align*} \kappa(A) &= \frac{\kappa(A)\|B-A\|}{\|B-A\|} = \frac{\|A\|\|A^{-1}\|\|B-A\|\|X\|}{\|B-A\|}\\ &\ge \frac{\|A\|\|A^{-1}(B-A)X\|}{\|B-A\|} = \frac{\|A\|\|0-X\|}{\|B-A\|} = \frac{\|A\|}{\|B-A\|}. \end{align*}  Note:  The above proof only makes use of the submultiplicativity of a matrix norm. We deliberately avoid using the inequality  \|M\|\ge\|Mx\|/\|x\| . While it is true that for  any  submultiplicative matrix norm  \|M\|  for square matrices, there  exists  a compatible vector norm  \|x\|  such that  \|M\|\ge\|Mx\|/\|x\|  for any  M  and any  x\not=0 , this fact is usually not taught in classes.  	0.79012131690979
1118674	We have \begin{align*} \|u_{n}\|_{L^{2}}&=\sup\left\{\left|\int u_{n}v\right|: v\in L^{2},~\|v\|_{L^{2}}\leq 1\right\}\\ &\geq\left|\int u_{n}v\right| \end{align*} for all such  v , then taking  \liminf , we have  \begin{align*} \liminf_{n}\|u_{n}\|_{L^{2}}\geq\left|\lim_{n}\int u_{n}v\right|=\left|\int uv\right|, \end{align*} once again the formula \begin{align*} \|u\|_{L^{2}}=\sup\left\{\left|\int uv\right|: v\in L^{2},~\|v\|_{L^{2}}\leq 1\right\}, \end{align*} the result follows.  	0.7898580431938171
560167	Since  f  is absolutely continuous with  f(0) = 0 , we can write    f(x) = \int_0^x f'(t)\, dt.    By Holder's inequality,    |f(x)| \le \left(\int_0^x 1^q \, dt\right)^{1/q} \left(\int_0^x |f'(t)|^p\, dt\right)^{1/p} = x^{1/q}\|f'\|_p.    Therefore     \|f\|_p = \|x^{1/q}\|_p \|f'\|_p = \left(\frac{1}{\frac{p}{q} + 1}\right)^{1/p} \|f'\|_p = \left(\frac{\frac{1}{p}}{\frac{1}{q} + \frac{1}{p}}\right)^{1/p}\|f'\|_p = \left(\frac{1}{p}\right)^{1/p}\|f'\|_p.    By Holder's inequality, we conclude    \|fg\|_1 \le \|f\|_p \|g\|_q = \left(\frac{1}{p}\right)^{1/p}\|f'\|_p \|g\|_q.  	0.7896632552146912
361626	First note that using the Cauchy-Schwarz inequality, we have (for nonzero  x  and  y )   \frac{y^TAx}{\|x\|_2\|y\|_2}\leq\frac{\|y\|_2\|Ax\|_2}{\|y\|_2\|x\|_2}=\frac{\|Ax\|_2}{\|x\|_2}.   If one picks  y=Ax , it gives   \left.\frac{y^TAx}{\|x\|_2\|y\|_2}\right|_{y=Ax}=\frac{x^TA^TAx}{\|x\|_2\|Ax\|_2} =\frac{\|Ax\|_2^2}{\|x\|_2\|Ax\|_2}=\frac{\|Ax\|_2}{\|x\|_2}.   Since the quotient can be bounded from above independently of  y  and this bound is attained by some  y , we have that   \max_{x,y}\frac{y^TAx}{\|x\|_2\|y\|_2}=\max_{x}\frac{\|Ax\|_2}{\|x\|_2}.   Now this is the standard variational characterisation of  \sigma_{\max}(A) :   \max_x\frac{\|Ax\|_2^2}{\|x\|_2^2}=\max_x\frac{x^T(A^TA)x}{x^Tx}=\lambda_{\max}(A^TA)=\sigma_{\max}^2(A).   	0.7896597981452942
590577	What you are doing is actually using Schwarz inequality to prove Schawarz inequality.    In the last step you need to use the Holder's Inequality as follows \begin{align} |\langle x,y\rangle|& = \biggl{|}\sum_\limits{n=1}^{\infty}\langle x,e_1\rangle \overline{\langle y,e_i\rangle}\biggr{|}\\ &\leq \sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|| \overline{\langle y,e_i\rangle}|\\ &\leq \sqrt{\sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|^2} \sqrt{\sum_\limits{n=1}^{\infty}|\langle y,e_1\rangle|^2} && \text{ (Holder's Inequality)}\\ &\leq \|x\|\|y\| && \text{(Bessel's Inequality)}. \end{align}    \textbf{EDIT:}  Sorry for this not helping answer. An easy way out is as follows:    Let  x\in X  and let  y\neq 0  be given. Then just take  e=\frac{y}{\|y\|} . This  \{e\}  only makes an orthonormal system. So, by Bessel's Inequality it will follow that  |\langle x,e\rangle|^2 \leq \|x\|^2 . Now just multiply both sides by  \|y\|  to get  |\langle x,y\rangle|^2\leq \|x\|\|y\| .  	0.7892969250679016
1015730	Now that you know how the elements of  L^p(0,1)^*  are you must show that:    \begin{equation} \underset{n \rightarrow \infty}{\text{lim}}{\int^1_0 x_n(t)y(t)dt}=0 \end{equation}    But, using Holder inequality we have:    \begin{equation} \left|{\int^1_0 x_n(t)y(t)dt}\right|\leq \int_0^1|x_n(t)y(t)|dt \leq \|x_n\|_p\|y\|_q \end{equation}    Now, suppose that  y\neq0 . So, to conclude, we must show that when  n \uparrow \infty ,   \|x_n\|_p \rightarrow 0 . But this is simple, because:    \begin{equation} \|x_n\|_p = \left(\int_0^1nt^{np}dt\right)^{\frac{1}{p}}=\left(\frac{t^{np+1}}{p+1}\right)^{\frac{1}{p}} \end{equation}    This clearly goes to zero because  t \in (0,1) !  	0.7878488302230835
523165	I finally figured it out; both separately...    Suppose it holds:  \omega(A^*A)\geq0    Positivity can be proven by Kadison's inequality:  \omega_E\left((A+a1)^*(A+a1)\right)=\omega(A^*A)+a\omega(A^*)+\overline{a}\omega(A)+|a|^2\|\omega\|\\\geq\omega(A^*A)-2|a|\omega(A^*A)^{\frac12}\|\omega\|^{\frac12}+|a|^2\|\omega\|=\left(\omega(A^*A)^{\frac12}-|a|\|\omega\|^{\frac12}\right)^2\geq0    Suppose it holds:  \lim\omega(E)=\|\omega\|    Boundedness can be proven by an approximate identity:  |\omega_E(A+a1)|=|\omega(A)+a\|\omega\||=\lim|\omega(AE)+a\omega(E)|\\\leq\limsup\|\omega\|\cdot\|aE+AE\|\leq\|\omega\|\cdot\|a1+A\|\cdot1   (Beware the limessuperior as the approximate identity alone diverges!)  	0.7876067161560059
879031	\begin{align} \||A|\|_2  &= \sup_{\|x\|_2 \le 1} \||A|x\|_2 \quad \mbox{(by definition of  \|\cdot\|_2  for matrices)} \\  &= \sup_{\|x\|_2 \le 1} \sqrt{\sum_{i=1}^m ||A_i|^Tx |^2} \quad \mbox{( A_i^T  is  i -th row of  A )} \\ &\le \sup_{\|x\|_2 \le 1} \sqrt{\sum_{i=1}^m \|A_i\|_2^2\|x\|_2^2} \quad \mbox{(by Cauchy-Schwarz inequality)} \\ &\le \sqrt{\sum_{i=1}^m \|A_i\|_2^2} \\ &= \sqrt{\sum_{i=1}^m\sum_{j=1}^n |A_{ij}|^2} \\ &= \sqrt{\mbox{trace}(A^*A)} \\ &= \sqrt{\sum_{i=1}^r \sigma_i^2} \quad \mbox{(by properties of trace,  \sigma_i  is singular value of  A )} \\ &\le \sqrt{r} \, \sigma_{\max} \quad \mbox{( \sigma_{\max}  is maximum singular value of  A )} \\ &= \sqrt{r} \, \|A\|_2, \quad \mbox{(follows from definition of  \sigma_{\max}  and  \|\cdot\|_2 )} \end{align} where  r  is the rank of  A , which in this case satisfies  r \le n .   	0.7875942587852478
