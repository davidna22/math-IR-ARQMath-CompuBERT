pid	doc	score
397878	\int x^{2}\ln(x)dx=x^{2}(x\ln(x)-x)-\int 2x^{2}\ln(x)+2\int x^{2}dx    Hence,    3\int x^{2}\ln(x)dx=x^{2}(x\ln(x)-x)+\frac{2}{3}x^{3}+C    so    \int x^{2}\ln(x)dx=\frac{1}{3}x^{2}(x\ln(x)-x)+\frac{2}{9}x^{3}+D=\frac{1}{3}x^{3}\ln(x)-\frac{1}{9}x^{3}+D  	0.9680376052856445
1129924	Indeed the norm in  c_0  is the  |\!|\cdot|\!|_{\infty}  norm. Clearly the norm of your functional is at most  1  because if  (a_j)\in c_0  is an element whose norm is at most  1 , then   \left|\sum_{j=1}^{\infty}\frac{a_j}{2^j}\right|\leq\sum_{j=1}^{\infty}\frac{1}{2^j}=1  to see that the norm is precisely  1 , let  \varepsilon>0 . Choose  N  sufficiently large so that  \sum_{j=1}^N\frac{1}{2^j}>1-\varepsilon . Let  a_0=(a_j)\in c_0  be the element whose first  N  entries are  1  and all the rest are zeros. Then  the norm of  \varphi(a_0)  is greater than  1-\varepsilon . Since  \varepsilon  was arbitrary, this proves that the norm of  \varphi  is not smaller than  1 .  	0.9676823616027832
668234	Neat problem. I don't have a complete solution, but I think a productive area would be to look at the prime factorizations of the numbers and see what happens to the exponents.    (2^2\cdot3^0, 2^1\cdot3^1, 2^0\cdot 3^2)    \to (2^1\cdot3^0, 2^2\cdot 3^1, 2^0\cdot3^2) \to (2^1\cdot3^0, 2^0\cdot3^1, 2^2\cdot 3^2) \to    or    \to(2^2\cdot3^0, 2^0\cdot3^1, 2^1\cdot3^2) \to(2^0\cdot3^0, 2^2\cdot 3^1, 2^1\cdot3^2) \to    (2^0\cdot3^0, 2^1\cdot3^1, 2^2\cdot 3^2)    In the first case the powers of 2 are  (2^2,2^1,2^0) , which get transformed into  (2^1,2^2,2^0)  and then  (2^1,2^0,2^2)  before ending up as  (2^0,2^1,2^2) . In the second case  (2^2,2^1,2^0)  becomes  (2^2,2^0,2^1)  and then  (2^0,2^2,2^1)  before ending up as  (2^0,2^1,2^2) . The exponents appear to be sorting themselves. The same thing happens to the powers of 3, and would happen to the powers of any other primes.    (Sorry this isn't a complete proof, but it looks like it could lead to one.)  	0.967046856880188
254051	I think this helps:  www.youtube.com/watch?v=xO8PZdQOpyg  to calculate  \frac{df}{dv}  if applying the chain rule with:    v=\frac{x+1}{x-1}  and  f=\text{arccot}(v)  and  \frac{df}{dx}=\frac{df}{dv}\frac{dv}{dx}    The calculation of  \frac{dv}{dx}  is left to your exercise.  	0.96689772605896
1234405	Use the binomial distribution. Let  X  be the number of students entering. Then  X\sim B(100,0.04) .    P(X\geq3)=1-P(X\leq2)=1-P(X=0)-P(X=1)-P(X=2)    We have  P(X=0)=0.96^{100}=0.0169   P(X=1)=100\times0.96^{99}\times0.04=0.0703   P(X=2)={{100}\choose{2}}0.96^{98}\times0.04^2=0.1450  Thus  P(X\geq3)=1-0.0169-0.0703-0.1450=0.7678 .  	0.9667444825172424
260956	A basic assumption of propositional logic goes something like that all of the connectives qualify as truth-functional, which in more conventional mathematical terms might get more specifically stated as that all connectives qualify as truth-operational.  This means that if we assign truth variables to any variables, there will exist some value V(a, ..., z) where V indicates a function of propositions.  For your example we have two truth-operations  \lor  such that     \lor (0, 0)=0, and    \lor (0, 1)= \lor (1, 0)= \lor (1, 1)=1.    \lnot (0)=1,  \lnot (1)=0.    You give a value to a symbol  \lor  or  \lnot  by using those functions or something similar.    By no means do you have to set-up truth tables in infix notation.  In infix notation the principle connective jumps all over the place, in the sense that it's position varies from well-formed formula to well-formed formula.  You can set up truth tables by writing all binary operations (those like  \lor  and  \land  which qualify as truth-operations of two truth values) after their arguments.  Or you could set up truth tables by writing all binary operations before their arguments.  With your example you could write:      p  q  ¬  ∨ ¬   p ∨   1  1  0  1  0  1  1   1  0  1  1  0  1  1   0  1  0  0  1  0  1   0  0  1  1  0  0  0     Or       p|  q| q ¬| p q ¬ ∨| p q ¬ ∨ ¬| p q ¬ ∨ ¬   p ∨   1|  1|   0|        1|          0|                1   1|  0|   1|        1|          0|                1   0|  1|   0|        0|          1|                1   0|  0|   1|        1|          0|                0   	0.9645787477493286
1015535	From your computation, the two definition are equivalent apart from a factor of  2 .   	0.9642890095710754
856189	This is not an answer. It's just too long and too illustrated for a comment.    I prefer to do most of my projective geometry over  \mathbb C , so I'll be viewing the problem from this point of view. What does it mean for a point  A  to be a focus of a given conic? It means that the lines joining  A  to the ideal circle points  I=[1:i:0]  and  J=[i:1:0]  are tangents to the conic. So when you are talking about a pair of two confocal conics, then a more general description would be speaking about a pair of conics sharing two tangents. The common focus  A  is the point where these tangents intersect. To make this easier to imagine, for the following picture I drew the points  I  and  J  at real positions. The brown lines are common tangents for the red and the green conic.       As Piquito pointed out, two conics will in general have four points of intersection. In the case of confocal ellipses it might well be that two of them have to be complex. But in the illustration above, all four of them are real.    If you pick the correct pair  P,Q , then the line joining their poles does indeed pass through  A , as you claimed. The same is true for the opposite pair of intersections, namely  R,S . Their poles lie on the same line as those for  P,Q . For other pairs of intersection, however, that is not the case. The other poles (red points) lie on two different lines (orange), four poles on each (not all of which are visible in the section above).    At this point I don't know how this may help you. Perhaps the most valuable property here might be the fact that the line joining the poles of  PQ  is concurrent with the lines  PS  and  QR . Since  R  and  S  are complex in your setup, that does not lead to an obvious construction, but it might help with a proof for your second question. The fact that you have to match things correctly may make an algebraic proof somewhat complicated, though.  	0.9640804529190063
80865	To prove the stated inequality (for normalized measure spaces): Let  X  be a finite measure space and  q \le p . We have using Hölder for  f \in L^p  \begin{align*}    \|f\|_q &= \bigl\||f|^q\bigr\|_1^{1/q}\\\            &= \bigl\|1 \cdot |f|^q\bigr\|_1^{1/q}\\\            &= \|1\|_{p/(p-q)} \bigl\||f|^q\bigr\|_{p/q}^{1/q}\\\            &= \mu(X)^{(p-q)/p} \|f\|_p \end{align*} If  \mu  is normalized, i. e.  \mu(X) = 1  (for example in  [0,1]  or  \mathbb T  with normalized arclength), then  \|f\|_q \le \|f\|_p  for every  q \le p . especially  \|f\|_{p-1} \le \|f\|_p .  	0.964065670967102
371638	We will define  5^k  as follows:   5^0  will mean 1, and if  i  is a positive integer,  5^i  will mean  5\cdot5^{i-1} .    Then we let  \Phi(k)  be the statement  a_k = 5^{k-1} .  We want to show that  \Phi(k)  holds for all  k\ge 1 , and we proceed by induction.    The base case is  \Phi(1) , the statement that  a_1 = 5^0 .  Since  a_1  and  5^0  are both defined to be 1, they are equal.    For the inductive step we suppose that  \Phi(k)  is already known to be true and we want to prove that  \Phi(k+1)  is true.  That is, we suppose  a_k = 5^{k-1}.  Multiplying both sides of this equation by 5, we obtain  5a_k = 5\cdot5^{k-1}.   The left side is equal to  a_{k+1}  by the definition we were given for the sequence, and the right side is equal to  5^k  by the definition we adopted for  5^k .  So substituting equals for equals we have  a_{k+1} = 5^k  which is precisely  \Phi(k+1) , and we are done.  	0.9624617099761963
1250252	Hint: First, let  s_x:=\frac 1 2 r_x  and choose finitely many of the intervals  (x−s_x,x+s_x)  that cover  C . Let  R  be the minimum of these values of  s_x .  	0.962037205696106
865997	\underline{A\; way\; without\; using\; an\; infinite\; geometric\; series}    Either you win on the first roll, or you don't, and the turn passes to your opponent, who is now exactly in the same position you were at the start of the game.    Thus if P(you eventually win)  =p,\quad p + \frac56p = 1 \to p = \frac6{11}     	0.9620344042778015
846096	We have  \int_{0}^{1}\frac{\log^{2}\left(1-x\right)}{x}dx\stackrel{x\rightarrow1-x}{=}\int_{0}^{1}\frac{\log^{2}\left(x\right)}{1-x}dx   \stackrel{DCT}{=}  \sum_{k\geq0}\int_{0}^{1}\log^{2}\left(x\right)x^{k}dx\stackrel{IBP}{=}  2\sum_{k\geq0}\frac{1}{\left(k+1\right)^{3}}=\color{red}{2\zeta\left(3\right)}.  	0.9617016315460205
250732	You can prove that  \{y=x\}  and  \{y=-x\}  are connected. Since they have a point in common, their union is. Can you try to argue this is the same, in essence, than showing the real line is connected?    First, let's obtain a slightly more useful equivalent to your definition. First, if  A,B  are open sets and  A\cap B=\varnothing , then they are separated. Now suppose  A,B  are separated sets, and  C=A\cup B . Then  C\setminus \bar A  and  C\setminus \bar B  are relatively open in  C . But  (A\cup B)\setminus \bar A=B  since  B\cap \bar A=\varnothing . Thus  C=A\cup B  and  A,B  are relatively open in  C . We can then just say that a set  C  is disconnected if it can be written as the disjoint union of relatively open subsets of  C .     Now, suppose  f:C\to C'  is continuous and onto. If  C'  is disconnected by  A,B , then  C  is disconnected by the open sets  f^{-1}(A) ,  f^{-1}(B) . Thus, by the contrapositive, if  C  is connected, so must be  C' . That is, the image of a connected set under a continuous function is connected.    Now consider the function  f:\Bbb R\to\Bbb R^2  given by  f(x)=(x,x) . Then your set is  f(\Bbb R) . I hope you know that  \Bbb R  is connected. You should see  f  is continuous:  \lVert f(x)-f(y)\rVert^2=2(x-y)^2    so if  x  and  y  are close, so is  (x,x)  and  (y,y) . Thus,  f(\Bbb R)  is connected.    It is also true that the union of connected sets with non-empty interesction is connected. And one can prove it as follows. First, we prove that  C  is connected if and only if the only continuous functions  f:C\to\{0,1\}   -  where  \{0,1\}  is given the discrete metric (topology), that is,  all  sets are open  -  are constant.    If  C=A\cup B  with  A,B  open, then we can define  f(a)=1  when  a\in A  and  f(b)=0  when  b\in B , to get non constant continuous function  f:C\to\{0,1\} . Conversely, if we had a  non-constant continuous function  f:C\to\{0,1\} , we could write  C=f^{-1}(\{0\})\cup f^{-1}(\{1\})  where this two sets are open and disjoint.    With this out of the way, suppose that we have a collection  \mathscr C  of connected sets, and they have a point in common, call it  c , that is  \bigcap\mathscr C\neq \varnothing . Consider a continuous function  f:\bigcup \mathscr C\to \{0,1\} . We will show that  f(x)=f(c)  for any  x\in\bigcup\mathscr C . Indeed, pick  x  in the union. Then this belongs to some of the sets in the collection, call this set  C_x . Then  f  as a function from  C_x  to  \{0,1\}  is still continuous, so, since  C_x  is connected, it must be constant. But  c\in C_x ; so we must have  f(x)=f(c) . Since this  x  was arbitrary, the claim follows.  	0.9616745114326477
1277760	I don't think it has a closed form. A series form solution to a more general integral in terms of upper incomplete gamma functions is:  \int \frac {at^4+2b}{t^8-be^t} dt = \sum_{k=0}^∞ \frac {1}{b^k(k+1)^{8k+1}}(\frac {a}{b}\frac {\Gamma [8k+5, (k+1)t]}{(k+1)^4} + 2\Gamma [8k+1,(k+1)t])  	0.9615047574043274
283970	"You are probably expected to write something like, when  z  is in  \mathbb C\setminus\{\frac13\} ,   \left|\frac{(1-3z)^{3/2}}{1-3z}\right|=|1-3z|^{1/2}\stackrel{z\to1/3}{\longrightarrow}0.   Unfortunately for this ""proof"", no continuous function  z\mapsto(1-3z)^{1/2}  can exist in a neighborhood of  z=\frac13 , likewise for  z\mapsto(1-3z)^{3/2} .  "	0.9613463282585144
71529	You could of course generalize your current measure    \begin{align}  S(X) = \frac{\frac{k^{(1/m)}}{k^{(1/n)}} -\frac{\|X\|_m}{\|X\|_n} } {\frac{k^{(1/m)}}{k^{(1/n)}}-1} \end{align}    while preserving your properties you specified.    An interesting special case could be  m = 1, n \to \infty , in which case the expression simplifies to    \begin{equation}  S(X) = \frac{k-\frac{\|X\|_1}{\|X\|_c}}{k-1} \end{equation}    where  c = \infty , (for some reason, mathjax refused to render when I inserted  \infty  directly in the fraction)  	0.9608859419822693
990577	So, suppose  p \nmid xyz . Then, from Fermat's theorem,   x^{p-1}\equiv y^{p-1} \equiv z^{p-1} \equiv1 (mod \ p)  which gives us a contradiction, as  x^{p-1}+y^{p-1}\equiv 2 (mod \ p) , while  z^{p-1}\equiv 1 (mod \ p) .  	0.9608449935913086
152796	Being a subset and being an element are different. For every set  A ,  \varnothing\subseteq A , but not necessarily  \varnothing\in A . As an example, we can consider  A=\left\{1,2,3,\{1,2\}\right\}\text{ and }B=\{1,2,3\}.  Then  \{1,2\}\in A  AND  \{1,2\}\subseteq A  whereas  \{1,2\}\subseteq B  but  \{1,2\}\notin B . Hopefully this clarifies that being an element and a being a subset are different things.  	0.9605357050895691
268908	Stein et al. - Complex Analysis    http://press.princeton.edu/chapters/s02_7563.pdf  	0.9605176448822021
482943	No. Any copy of  \mathbb{Z}  must be the ring generated by  1 , but in any infinite field of prime characteristic  p ,  \underbrace{1 + \cdots + 1}_p = 0,  and so the ring generated by  1  is not isomorphic to the integers.    Edit  Under the condition that the field have characteristic  0 , this  is  true, essentially because in such a field the above equation does not hold for any (nonzero)  p .  	0.9604347348213196
447670	To shift the graph 1 unit to the right, replace  x  by  x-1  to get  y=-\ln(-(x-1)) ,     which simplifies to  y=-\ln(1-x) .  	0.9604169726371765
962813	You can't, unless you restrict the operator to a bounded domain like a ball of finite radius. The term bounded has, for historical reasons, a special meaning for (linear) operators, which is de facto different from what you use for general functions. In other words, a linear operator which is bounded will not be bounded in the other sense, unless it is the zero operator.  	0.9601579904556274
1282704	This is related to very deep questions in number theory, and there's no simple answers.    From a naive point of view it seems reasonable. You can start with a fundamental domain for the fractional linear action of  PSL(2,\mathbb Z)  on the upper half plane; the conventional fundamental domain is given by intersecting  -\frac{1}{2} \le \text{Real}(z) < \frac{1}{2}  with  \text{abs}{z} \ge 1 . Then you can carefully choose coset representatives of  \Gamma(m)  in  PSL(2,\mathbb Z) , use them to translate the fundamental domain around, and hope that you chose those representatives carefully enough so that the union of those translated fundamental domains is a polygon which you can then glue up to get  \mathbb H^2 / \Gamma(m) . For the case  m=2  this can probably be done by hand. But the index of  \Gamma(m)  in  PSL(2,\mathbb Z)  grows uncomfortably fast: it is cubic in  m . So this method is not practical in general.    The general heading of your question is the topic of  congruence subgroups  of  PSL(2,\mathbb Z) . In that reference you'll see, for example, a link to a 2006 paper by Long, Machlachlan and Reid proving that there are only finitely many values of  m  such that the quotient surface  \mathbb H^2 / \Gamma(m)  has genus zero, meaning that it is homeomorphic to a sphere with some finite number of points removed. You'll also see described some amazing connections between the topology of  \mathbb H^2 / \Gamma(m)  (more properly, not that surface itself but a closely related quotient orbifold) and the prime factors of the order of the  monster group !  	0.9599092602729797
669094	"""Just"" solve this system:       \begin{align} |BC| &= b+c \\ |BD_1| &= b+d \\ |CD_2| &= c+d \\ |D_1 D_2| &= 2 d \\ |D_2 E| &= d + e \end{align}  where, say,   A = (0,0) \qquad B = (a+b,0) \qquad C = \frac{(a + c )\sqrt{2}}{2} (1,1)   D_1 = (a+2b,x) \qquad D_2 = (x,a+2b) \qquad E = (a+2b)(1,1)       Edit.  Numerically, taking  a=1 , we have   \begin{align} b &= 0.7037139\dots \\ c &= 0.5493113\dots \\  d &= 0.7792658\dots \\  e &= 0.3227824\dots \\  x &= 1.3053795\dots \end{align}     which, according to  Mathematica , is the only solution in positive real values.    Symbolically, well ... There's an unattractive degree-6 polynomial involved, and I don't really want to type it in right now. I'll revisit this later.  "	0.9598352909088135
505367	Let  f = \mathbf{x}^{T}B^{T}B\mathbf{x}-\mathbf{x}^{T}B^{T}\mathbf{b} - \mathbf{b}^TB\mathbf{x} . Notice that  f = \mathbf{x}^{T}B^{T}B\mathbf{x} -2\mathbf{b}^{T}B\mathbf{x} , because  \mathbf{x}^{T}B^{T}\mathbf{b} = \mathbf{b}^TB\mathbf{x} . In multivariate calculus*,  \nabla_{\mathbf{x}} ^{T} f = \frac{\partial f}{\partial\mathbf{x}} = 2\mathbf{x}^{T}B^{T}B -2\mathbf{b}^{T}B . It is a simple calculation.    However, if you don't understand the calculation before. You just follow the steps:    1) Calculate   \Delta f = f(\mathbf{x}+\mathbf{\Delta x})-f(\mathbf{x}) ;    2) Keep only the first order terms (in  \Delta \mathbf{x} ) of  \Delta f . Here, you will have been constructing the differential of  f  ( \partial f );    3) Change, thus,  \Delta \mathbf{x}  by  \partial \mathbf{x}  and you'll have  \partial f = (\cdot)\partial \mathbf{x} . The quantity  (\cdot)  is the derivative  \frac{\partial f}{\partial\mathbf{x}} .    In order to support you in vector and matrix calculus, I suggest you the books:    a)  Matrix Differential Calculus with Applications in Statistics and Econometrics (W.E. Shewhart and S. S. Wilks) ;    b)  Complex-Valued Matrix Derivatives (Are Hjørungnes) .    *depends on the notation used.  	0.9598090648651123
1280783	Here's a beginning: “Suppose that  a,b \in R^\times .”    Here's the end: “Therefore,  ab \in R^\times .    The middle is up to you.  For inspiration, you might try examples with  R=\mathbb{Z}  or  \mathbb{Q} .  What is the inverse of a product of numbers?  Is it expressible in terms of the inverses of the factors?  	0.9597285985946655
364533	Whether or not a series converges does not depend on if  \lim_{n\to\infty}a_n=0 , rather it is the converse that is true.    If a series converges, then  \lim_{n\to\infty}a_n=0 .    But  \lim_{n\to\infty}a_n=0  does not prove that a series converges.    Your series is an example of one that does not converge and approaches  0 .    Here is my explanation:    If a sum gets closer and closer to some limit, then its partial sums must increase by less and less.  Upon reaching the limit, the partial sums no longer increase and we must have the following: \lim_{n\to\infty}\sum_{i=1}^{n}{f(i)}=\lim_{n\to\infty}\sum_{i=1}^{n+1}{f(i)} Due to the nature of  \infty .    Furthermore, we have such:    \lim_{n\to\infty}\sum_{i=1}^{n}{f(i)}=\lim_{n\to\infty}\sum_{i=1}^{n+1}{f(i)} \lim_{n\to\infty}f(1)+f(2)+f(3)+\cdots f(n)=\lim_{n\to\infty}f(1)+f(2)+f(3)+\cdots f(n)+f(n+1) 0=\lim_{n\to\infty}f(n+1)    One can prove a convergent series has this property, but not that a series with this property is convergent.    This means that a series without this property must be divergent, because I just proved all convergent series have this property, meaning if it doesn't, it can't be convergent.    Also, (fun fact) a series' sum is not dependent on whether it converges.    For example: \sum_{n=1}^{\infty}n=-\frac12 If you don't believe, you can look it up.  It even has its own wiki page.  	0.9595434665679932
416245	"If a ring homomorphism is required to send  1_G  to  1_H , then  \phi(-1_G)+1_H=\phi(-1_G)+\phi(1_G)=\phi(1_G-1_G)=\phi(0)=0 , so  \phi(-1_G)=-1_H .    If  \phi  is merely a group homomorphism between the groups of units  G^\ast  and  H^\ast , then the answer is trivially ""no"": you could map  \phi(g)=1_H  for every  g\in G^\ast  to provide a counterexample.  "	0.9594707489013672
13090	For  p \geq 1  the generalized mean defines a norm, because it is the  \ell^{p} -norm only up to a factor  \sqrt[p]{n} .    However, if  p \lt 1 , the generalized mean (and also the  \ell^{p} -expression) don't define a norm because the set  \|x\|_{p} \leq 1  is not convex: if  x_{i} \geq 0  and  y_{i} \geq 0  for all  i  then  \|x + y\|_{p} \geq \|x\|_{p} + \|y\|_{p} !    If  p \leq q  then  \|x\|_{q} \leq \|x\|_{p} . To see this, note that both sides of the inequality are invariant by multiplication with a positive real number, so we may take without loss of generality an  x  with  \|x\|_{p} = 1 . Then  \|x\|_{q}^{q} = \sum_{j = 1}^{n} |x_{j}|^{q} \leq \sum_{j = 1}^{n} |x_{j}|^{p} = 1  this is because for  t \leq 1  and  p \leq q  we have  t^{q} \leq  t^{p} .    I don't understand what you ask about the sequence space.  	0.9593386650085449
316273	For  finite  groups there is an easier proof, using the following facts. The proof of these facts can be found in the spoiler. All groups here are finite. Recall that  \Phi(G)  is the set of  non-generators  of  G  and the intersection of all maximal subgroups of  G . And that a group is nilpotent if and only if all its maximal subgroups are normal (here is where we need finiteness).   Lemma 1  A group  G  is nilpotent if and only if  G' \subseteq \Phi(G) .   Lemma 2  If  N \unlhd G , then  \Phi(N) \unlhd \Phi(G) .  Now assume that  G/N'  and  N  are nilpotent. By Lemma 1, we have  N' \subseteq \Phi(N) , and by Lemma 2, it follows that  N' \unlhd \Phi(G) . If  M  is a maximal subgroup of  G . Then  \Phi(G) \subseteq M , whence  N' \subseteq M , implying  M/N'  is maximal in  G/N' . Since  G/N'  is nilpotent it follows that  M/N' \unlhd G/N'  and by the correspondence theorem this is equivalent to  M \unlhd G . So all maximal subgroups are normal, and  G  is nilpotent.        ! Proof of Lemma 1  If a group is nilpotent and let  M  be a maximal subgroup. Then  M  is normal and hence  G/M  has no non-trival subgroups. This can only be if  M  has prime index in  G , whence  G/M  is abelian and so  G' \subseteq M . Since  \Phi(G)  is the intersection of all maximal subgroup, we get  G' \subseteq \Phi(G) . Conversely, if  G' \subseteq \Phi(G) , then for any maximal subgroup  M  we have  G' \subseteq \Phi(G) \subseteq M , so  G' \subseteq M  and  M  is normal.      Proof of Lemma 2  Observe that  \Phi(N)  is characteristic in  N  and  N  is normal in  G , so  \Phi(N)  is normal in  G . Hence for a maximal subgroup  M  of  G ,  M\Phi(N)  is a subgroup containing  M . Assume  G=M\Phi(N) . By Dedekind Lemma, we get  N=N \cap M\Phi(N)=(N \cap M)\Phi(N)=N \cap M \subseteq M . Hence  \Phi(N) \subseteq N \subseteq M  and it follows that  G=M\Phi(N)=M  a contradiction. We conclude that  \Phi(N) \subseteq M  for any maximal  M , hence  \Phi(N) \subseteq \Phi(G)  as required.    	0.9591347575187683
975617	Hints:      If  n  is an integer, then  \lceil z\rceil\le n\iff z\le n.    \lceil x\rceil\cdot\lceil y\rceil  is an integer.    x\le\lceil x\rceil.  	0.9591052532196045
977698	Uniqueness is a desirable property of a solution but not required.    But continuity is much more important.    From a theoretical perspective    It is possible to distinguish between solutions based only on their domain.    For example, if you have two solutions    \{0,1\} \rightarrow u(t)    \{-1,2\}\rightarrow v(t)    with  u(t)=v(t)    that only differ in the domain, then  v(t)  is called an extension of  u(t) . Nevertheless, some mathematicians would call them separate solutions.    But this is often not done because then we would always have an infinite amount of possible solutions.    The same happens with your approach. Let us look at your specific problem. In an initial value problem our initial value would lie in  (-1,2)  or  (4,\infty)  but not in both.    So we could assign the interval where the initial value is not located the stationary solution.    Not very nice (but from a theoretical standpoint it would be feasible).    The derivative itself can be non continuous on the other hand. For example    u'(t)=\|u\|    is totally valid, even for  u(t)=0 .    From a numerical perspective    Many numerical approximation schemes rely on Taylor expansion of the form  u(t+\Delta t)=u(t)+hu'(t)+u''(\xi) .    If your function is discontinuous this property does not automatically hold anymore. A smaller step size would not automatically improve the solution anymore (if the algorithm is consistent) because a smaller  h  could land on undefined points.  	0.9590164422988892
768028	Assume that  n \neq 1 .    Let  p  be the smallest prime which divides  n . Write  n= p^k \cdot l .    Now, you know that   1 \equiv 2^n \equiv (2^l)^{p^k} \pmod p    Use the fact that  a^p \equiv a \pmod p  to conclude that   (2^l)^{p^k} \equiv 2^l \pmod{p} .    This shows that   2^l \equiv 1 \pmod{p}  You also know that   2^{p-1} \equiv 1 \pmod{p}    This gives that  2^{\gcd (l, p-1)}\equiv 1 \pmod{p} .    Now, if  \gcd(l, p-1) \neq 1  it is divisible by a prime  q . This prime divides  l  hence  n , and  q  also divides  p-1  and hence is smaller than  p . But this contradicts that  p  is the smallest prime dividing  n .    We can therefore conclude that  \gcd (l, p-1)=1 .     Thus  2^1 \equiv 1 \pmod{p} \Rightarrow p\mid 2-1=1 .     This is a contradiction.  	0.9589784741401672
1118457	If  x\in F_n , then the conjugacy class of  x  is in bijection with the set of cosets of  C_{F_n}(x)  in  F_n  where  C_{F_n}(x)  denotes the centralizer of  x .    Exercise :  C_{F_n}(x)  is infinite cyclic for all nonidentity  x\in F_n . ( Hint : subgroups of free groups are free.)    Hope this helps!  	0.9589348435401917
704661	We may suppose that  y\geq 0 . Now  x^4-4x^3+6x^2-2x+5=(x-1)^4+2x+4  Hence we have  (y-(x-1)^2)(y+(x-1)^2)=2(x+2)    This imply that  (x-1)^2\leq (x-1)^2+y\leq 2|x|+4  and it is easy to finish.   	0.9588611721992493
269232	Since  f_X  and  f_Y  are independent then the joint pdf will look like  f_{XY}(x,y)=f_X(x)f_Y(y) . Then we can perform the transoformation or variables from  Y  to  Z=g(Y) :  f_{XZ}(x,z)=f_X(x)f_Z(z)\left|\frac{\partial g(y)}{\partial y}\right| . The question  P[X < g(Y)]  prompts us to use the joint cdf of  X  and  Z :  F_{XZ}(x,z) .   So  P[X < Z]=F_X(z)=\int_{-\infty}^{-\infty}F_{XZ}(z,z')\mathrm dz' .  	0.9588593244552612
431410	This proof looks wrong to me. The claim that  \{ f(x): x \in (0, 1)\}  is bounded below seems wrong. Look at, say,  f(x) = -1/x^8  for  x \ne 0 , and  f(0) = 0 . Then that set is not bounded below.     The place where it really messes up is when you suppose that  f(x) > \int_0^1 ... . At this point,  x  is a specific number in the unit interval. But in the next line, you use it as a variable in a set-specification, and things get confused.     Edit, post-comments:  Once the proof was modified to suppose that  f(x) > \int_0^1 f(t) dt  for all  x \in (0, 1) , the remaining failure is in the last couple of steps. You say that  int_0^1 f(t) −C ~dt \le 0 , but  f(t)−C\ge 0  for all  t , and conclude that this means  f(t)−C=0 . Unless you're assuming  f  is continuous, this conclusion isn't justified. (Think of  C=0 , and  f(t)=0  except for  f(0)=1 , or  f(1/2) = 1 .)     I think that perhaps a better division would be into two cases: (1) there is a constant  C  such that  f(t) = C  almost everywhere, or (2) there is no such constant. The first case gets just a tiny bit trickier, but in the second case, you can conclude at the end that  f(t) = C  almost everywhere; it thus must be true at some point of  (0, 1) .  	0.9588227272033691
1251001	I think is that the question implies some assumptions. For instance, the fact that you arrive at an expected value of  7  is only because you assumed the probability of an individual taking a car to be identical to that of a fair coin; instead, just take  p=\frac{5}{7} , that leads you to an answer and I would guess that this is the one they are looking for.     However, as a side note, I wanted to mention that it's useful to think of concentration inequalities. These won't give you exact answers but can get you fairly tight bounds depending on the information you have at hand. For instance, you can use Markov's inequality here; that's not a great bound for this, so it's not particularly useful.   	0.9585137963294983
599079	The question asks what the angles are if they are obtuse, i.e. between 90 and 180 degrees. The notes provide the conversions, e.g. if you get sin(30) = 0.5 then you convert it to an obtuse angle through sin(30) = sin(180 - 30) from the first note.  	0.9584485292434692
295961	Proof:  Let  \mathcal B  be an  n -in-countable base for some  n \in N . Suppose not. So there is an uncountable open cover  \mathcal U_0  of elements of  \mathcal B  which has no countable subcover. Enumerate it as  \{B_\alpha: \alpha < \kappa \} .     Since  X  is star countable, there is a countable subset  A_0  such that  \operatorname{St}(A_0, \mathcal U_0) = X . Then there is some point  x_0 \in A_0  such that  \mathcal B_0 = \{B \in \mathcal U_0: x_0 \in B\}  is uncountable and  \mathcal B_0  hasn't a countable subcover of  \bigcup \mathcal B_0 , otherwise  \mathcal U_0  has a countable subcover of  X . Put  \alpha_0= \min \{\alpha < \kappa: x_0 \in B_\alpha \} . Let  \mathcal V_0=\{B_\alpha \in \mathcal U_0: x_0 \in B_\alpha, \alpha > \alpha_0\} ,  \mathcal U_1 = \{B_\alpha \setminus \{x_0\}: B_\alpha \in \mathcal V_0\} . Let  \mathcal W_1 = \mathcal U_1 \cup (\mathcal U_0 \setminus \mathcal V_0) \cup \{B_{\alpha_0}\} .    We can pick a countable subset  A_1  such that  \operatorname{St}(A_1, \mathcal W_1) = X . Then there is some point  x_1 \in A_1  such that  \mathcal B_1 = \{B \setminus \{x_0\}\in \mathcal U_1: x_1 \in B \setminus \{x_0\} \}  is uncountable and  \mathcal B_1  hasn't a countable subcover of  \bigcup \mathcal B_1 , otherwise  \mathcal B_0  has a countable subcover of  \bigcup \mathcal B_0 .  Put  \alpha_1= \min \{\alpha < \kappa: x_1 \in B_\alpha \setminus \{x_0\} \in \mathcal U_1 \} . Let  \mathcal V_1=\{B_\alpha \setminus \{x_0\} \in \mathcal U_1: x_1 \in B_\alpha \setminus \{x_0\}, \alpha > \alpha_1\} ,  \mathcal U_2 = \{B_\alpha \setminus \{x_0, x_1\}: B_\alpha \in \mathcal V_1\} . Let  \mathcal W_2 = \mathcal U_2 \cup (\mathcal U_1 \setminus \mathcal V_1) \cup \{B_{\alpha_1}\} \cup (\mathcal U_0 \setminus \mathcal V_0) \cup \{B_{\alpha_0}\} .    Continuing in this way, for each  k \le n-1 , we can choose a countable set  A_k , a point  x_k  of  A_k , an uncountable family  \mathcal U_k  and an uncountable subset  \mathcal B_k = \{B \setminus \{x_0,x_1,..., x_{k-1}\} \in \mathcal U_k : x_k \in B \setminus \{x_0,x_1,..., x_{k-1}\} \} . At the step  n-1 , we can get a subset  \{x_0, x_1,..., x_{n-1}\}  of  X  such that   \{x_0, x_1,..., x_{n-1}\}  is contained in uncountable many elements of  \mathcal B .     This is a contradiction!  	0.9584190249443054
1098265	f(x)=e^{x^2}  is a convex function growing really fast, hence in order to find an initial approximation of the positive solution  k  it is better to apply a substitution first.  \int_{0}^{k}\exp\left(x^2\right)\,dx =\frac{1}{2}\int_{0}^{k^2}\frac{e^x}{\sqrt{x}}\,dx =\frac{1}{2}\int_{1}^{\exp(k^2)}\frac{du}{\sqrt{\log u}}.  To find a numerical solution of    \int_{1}^{T}\frac{du}{\sqrt{\log u}} = 20 \tag{A}  is a bit easier. By integration by parts    \int_{1}^{T}u\cdot\frac{\frac{1}{u}\,du}{\sqrt{\log u}}=2T\sqrt{\log T}-\int_{1}^{T}\sqrt{\log u}\,du  hence  T_0=20\sqrt{\log 20}  is a not so bad approximation of a solution of  (A) .  By invoking Newton's method and the trapezoid method for numerical integration,   T_{n+1} = T_n - \sqrt{\log T_n}\left(-20+\int_{1}^{T_n}\frac{du}{\sqrt{\log u}}\right) \tag{B}  we get  T_1\approx 29.654  and  T_3\approx T_2\approx 29.706 , then   k\approx \sqrt{\log T_2} \approx 1.84157. \tag{C}  	0.9583341479301453
576898	You have to be weary of the definition of  aHa^{-1}=H  for  H  normal. It does not mean  aha^{-1}=h . It means there is  h'\in H  such that  aha^{-1}=h' . The first case means  h  is central, or commutes with all  a\in G . Now suppose  H  is normal. Then  aHa^{-1}=H , or  aha^{-1}=h' . Thus  ah=h'a , or  aH=Ha . The other direction is similar.  	0.9582816958427429
583624	"As said, there is an analytical solution in terms of Lambert function  x=-W(d)-\frac{c}{b}  where  d=\frac{a }{b}e^{-\frac{c}{b}}  If you do not want to (or cannot)  use Lambert function, then numerical methods are the way to go.    Probably the simplest should be Newton method which, starting from a ""reasonable"" guess  x_0 , will update it according to  x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}  For the example you give in comments  f(x)=5-12x - 88e^x   f'(x)=-12-88e^x  A look at the graph of the function shows that the solution is close to  -1.5 . So, let start iterating at  x_0=-1.5 ; the method will generates the following iterates :  -1.393646353 ,  -1.397047564 ,  -1.397051301  which is the solution for ten significant figures.  "	0.9581931829452515
315939	"The equation  x^2+y^2=0^2  has a single point as its solution. Whether that that means that a point ""can be viewed as a circle"" depends more on the exact meaning you assign to ""can be viewed as"" than on the nature of points and circles in themselves.    It certainly doesn't mean that all properties of circles are also properties of points. For example, a circle has the property that for every point on the circle there is exactly one line that passes through that point and not through other points on the circle. This is not true about a singleton set, of course.  "	0.9581659436225891
766865	Here's another way. The number of bytes with  n  zeroes is the coefficient of  x^n  in  (1+x)^8  Now for any polynomial  p(x) , the sum of the coefficients of even degree is  \frac{p(1)+p(-1)}{2} . Hence the answer is:    \frac{(1+1)^8+(1-1)^8}{2} =2^7  	0.9580414295196533
232772	You can use generating functions. Let  F_n  be the Fibbonacci sequence defined as  F_0=1\\F_1=1\\F_{n}=F_{n-1}+F_{n-2}\;\; ;\; n\geq 2    Let  F(x)=\sum_{n\geq 0}F_nx^n  be its generating function. Then  (1 - x - {x^2})F(x) = \sum\limits_{n \geqslant 0} {{F_n}} {x^n} - \sum\limits_{n \geqslant 0} {{F_n}} {x^{n + 1}} - \sum\limits_{n \geqslant 0} {{F_n}} {x^{n + 2}}   (1-x-x^2)F(x)=\sum_{n\geq 0}F_nx^n-\sum_{n\geq 1}F_{n-1}x^{n}-\sum_{n\geq 2}F_{n-2}x^{n}   (1 - x - {x^2})F(x) = {F_0} + {F_1}x - {F_0}x + \sum\limits_{n \geq 2} {\left( {{F_n} - {F_{n - 1}} - {F_{n - 2}}} \right)} {x^n}    And by the recursion the left hand side is  1+1-1+0  so  (1 - x - {x^2})F(x) = 1   F(x) = \frac{1}{{1 - x - {x^2}}}    Now use that  {x^2} + x - 1 = \left( {x+ \varphi } \right)\left( {x-{\varphi ^{ - 1}}} \right)  and the geometric expansion  \frac{1}{{a - x}} = \sum\limits_{k = 0}^\infty  {{a^{1 - k}}{x^k}}   plus simple fractions to get what you want.  	0.9580102562904358
101268	"I am not a Mathematica expert, but it seems as though the values for the level curves were selected such that the level curves represent five even steps from the peak at (0,0) to where the surface ""levels out.""    Regardless, let's look at the bivariate Gaussian distribution for  \rho = 0 , which implies that  X  and  Y  are uncorrelated.    You can write the PDF of this distribution as    f(x,y) = \frac{1}{2\pi\sigma_x\sigma_y} \exp \left(-\frac{1}{2}\left[\frac{x-\mu_x}{\sigma_x^2}+\frac{y-\mu_y}{\sigma_y^2}-\frac{2(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}\right]\right).    You can compute this surface in a straightforward manner, and use any contour curve generating algorithm to plot those curves.  "	0.9579982161521912
158762	You can bound  |x+3|  by taking some preliminary  \delta  of convenience.  For example, just say you will start with  \delta \le 1 .  Then  |x+3| \le 7 .  Since you just have to prove your error is less than  \epsilon , you can say  |x+3||x-3| \le 7 |x-3|  and continue.  Then report  \delta  as the smaller of the result of your calculation (which it will be here) or  1 .  	0.9579310417175293
1098211	Alternative approach: let us introduce the sequence  a_n = \frac{(2n)!}{n! n^n} . We have   \frac{a_{n+1}}{a_n} = \frac{(2n+2)(2n+1)n^n}{(n+1)(n+1)^{n+1}} = \frac{4\left(1-\frac{1}{2n+2}\right)}{\left(1+\frac{1}{n}\right)^n}  hence  \lim_{n\to +\infty}\frac{a_{n+1}}{a_n}=\color{red}{\large\frac{4}{e}} . Cesàro theorem then implies this is also the limit of  \sqrt[n]{a_n}  as  n\to +\infty .       Non-trivial corollary.  e>\frac{8}{3}  is a straightforward consequence of Jensen's inequality, since   \frac{4}{e}=\exp\int_{0}^{1}\log(1+x)\,dx <\int_{0}^{1}(1+x)\,dx=\frac{3}{2}.  	0.9578703045845032
702124	Yes, but in general only if  P  is unique (i.e. if it is normal).    That's because in general  \operatorname{ord}(\phi(g)) \mid \operatorname{ord}(g)  for every  g \in G . Since for every  \pi \in P  we have  \operatorname{ord}(\pi) = p^n  for some  n \geq 0 , it follows that  \operatorname{ord}(\phi(\pi)) = p^m  for some  0 \leq m \leq n . Thus  \operatorname{ord}(\phi(\pi))  lies in some  p -Sylow subgroup, but we can't guarantee in which one unless it is unique.  	0.9578378796577454
839850	a(a^{4n}-1)=a(a^{2n}-1)(a^{2n}+1)=a(a^{n}-1)(a^{n}+1)(a^{2n}+1) .    Assume  30 | a(a^{4n}-1) .    30 | a(a^{4(n+1)}-1)  if and only if  30 | a(a^{4(n+1)}-1)-a(a^{4n}-1) .    Try factoring:  a(a^{4(n+1)}-1)-a(a^{4n}-1) .  Think about how regardless of  a , you can show that one of the terms of the factorization must be divisible by 2, another divisible by 3, and another divisible by 5.  	0.9577962756156921
1041652	Your result is correct. The result from Mathematica is correct as well.    What is mysterious ?     Nothing because simplifying the Mathematica formula leads exactly to your formula.    u(x,t)=\frac{1}{4}\left(4t^3\ln(x)-6t^2\ln^2(x)+4t\ln^3(x)-\ln^4(x)+\left(\ln(e^{-t}x)\right)^4 +4\Phi(e^{-t}x) \right)    \left(\ln(e^{-t}x)\right)^4 = \left(-t+\ln(x)\right)^4 = t^4-4t\ln^3(x)+6t^2\ln^2(x)-4t\ln^3(x)+\ln^4(x)    After simplification :    u(x,t)=\frac{1}{4}\left(t^4 +4\Phi(e^{-t}x) \right) = \frac{t^4}{4}+\Phi(\frac{x}{e^t})  	0.9577630162239075
949755	Notice that you need  V  to be a Banach space (even in the case when  X  is compact).     In this case it is complete. If you have a Cauchy sequence,  f_n , in  C_B(X,V)  then it is straightforward to see that it is pointwise cauchy (ie that for each  x\in X ,  f_n(x)  is cauchy in  V . Then since  V  is a Banach space you get that there is a pointwise limit, and it is a standard  \epsilon/3  argument to show that the limit is continuous.     Bounded is a little trickier. To see this note that in any metric space, cauchy sequences are bounded. Thus there is an  M  such that  \|f_n\|\leq M  for all  n . By the definition of the norm this means that  |f_n(x)|\leq M  for all  n  and all  x , thus  \lim_{n\rightarrow\infty}f_n(x)\leq M  for all  x .   	0.9577411413192749
183643	\newcommand{\Cov}{\operatorname{Cov}}    You need  \Cov(Z,X)=0 , not  \Cov(X,Y)=0 .    Then   0=\Cov(Z,X)= \Cov(X+wY,X)=\Cov(X,X)+w\Cov(Y,X),   and recall that  \Cov(X,X)=1 , etc.    Also, you omitted to add that the pair  (X,Y)  is  jointly  normally distributed.  Without that, your conclusion that  X+wY  is necessarily normal is not justified.  	0.9577218890190125
1164213	Hint: To show two topologies are equivalent you need to show that one is a subset of another and vice versa, so that they both (as sets) are equal.  	0.9576398730278015
1227463	I first want to note, that in your first comment where you suggest a bijection from  \{0/1,0/2,0/3,\dots\}  is only correct if you regard  0/x  as a syntactical object. If you refer to a classical fractional representation of a rational number, then  \{0/1,0/2,0/3,\dots\}=\{0\}  which is most certainly not bijective with  \mathbb{N} .       The proof that for countable disjoint  A,B , there is a bijection between  A\cup B  and  A,B  respectively is fine. Let me add, that I think that the countable case is not really rich in terms of awaiting revelations, as a countable union of countable sets is again countable (as you showed for the case of a pair-union) and every countably infinite set is bijective with any other countably infinite set.    This is why I think there are many ways to arrive relatively immediate at your desired result of equipotency of  \mathbb{Q}  and  \mathbb{Q}^+ :      You may show that  \mathbb{Q}  and  \mathbb{Q}^+  are both countably infinite (e.g. via a diagonal argument a la Cantor), i.e. yielding bijections  f:\mathbb{N}\to\mathbb{Q}  and  g:\mathbb{N}\to\mathbb{Q}^+ . Then  h:\mathbb{Q}\to\mathbb{Q}^+ ,  q\to g(f^{-1}(q))  may be checked by you to be bijective.   You can proceed by splitting  \mathbb{Q}=\mathbb{Q}^-\cup(\mathbb{Q}^+\cup\{0\})  or  \mathbb{Q}=(\mathbb{Q}^-\cup\{0\})\cup\mathbb{Q}^+  and observe that they are both countably infinite and disjoint. Then you can apply your insight for pair-unions of disjoint countably infinite sets.         As I think these results are relatively immediate, to get a greater deal of new knowledge out of this question, maybe try as an exercise to generalize your results and the related insights as far as possible.  	0.957619309425354
550534	For case 1, first choose the value and suit of the card that is the pair ( 52 ).  Then choose the values/suits of the three unpaired cards ( _{51}C_3 ) and then choose which instance of each card goes in your hand ( 2^3=8 ).    For case 2, choose the value and suit of the two cards that are the pairs ( _{52}C_2 ) and then choose the card from the rest of the pile ( 100 ).    So the probability for case 1 is    P_1 = \frac{(52)(_{51}C_3)(8)}{_{104}C_5},    and for case 2 it's    P_2 = \frac{(_{52}C_2)(100)}{_{104}C_5}.  	0.9574840068817139
167419	Hint: See,  Own Lecture Notes Functional Analysis . Theorem 2.15.1, p17.  	0.9574557542800903
621753	There is an easier approach.     Note that  \Phi(x)  is a continuous increasing function going from  0  to  1 . Let  Y=\Phi(X) , so  Y  is in the interval  (0,1) . Then  F(y)=\Pr(Y \le y) =\Pr(\Phi(X) \le y) = y  so  f(y)=1  when  y \in (0,1)  and  E[Y]=\int_0^1 y \,f(y) \, dy =\frac12 .    This works for any continuous distribution.  	0.9573177695274353
43157	Well, actually it is well known that       \|A\| = \|A\|_{1\rightarrow\infty} = \max_{x\ne 0}\frac{\|Ax\|_{\infty}}{\|x\|_1}\,.   Thus, in particular, it is consistent with the corresponding norms:       \|Ax\|_{\infty} \le \|A\| \|x\|_1\,.   	0.9571772217750549
1046888	Your number is suspiciously close to  1-1/e .  The fraction of values represented exactly  k  times in your array should be close to  \exp(-1)/k! , so it looks like your program counted the number of distinct values in the array, rather than the number of values represented only once.  	0.9571467041969299
521759	Suppose that the series  a_n  is convergent. Then  a_n\to 0 , and there exists  N  such that  0 1-a_n>0 . We get  \displaystyle \frac{a_{n+1}}{a_N}>(1-a_N)\cdots(1-a_n) , and   \sum_{k=N}^{n}(-\log(1-a_k))\geq \log a_N-\log(a_{n+1})  Now  -\log(1-a_k)\sim a_k , the series are positive, hence the series  -\log(1-a_k)  is convergent. Hence there exists  M  such that  \log a_{n+1}\geq M  for all  n\geq N . This imply that  a_{n+1}  do not  \to 0  if  n\to +\infty , a contradiction. Hence the series  a_n  is divergent.  	0.9571452736854553
971530	Take  e^{x}=p .    Then your equation becomes  2p+\frac{7}{p}=9   2p^2-9p+7=0   p=\frac{9\pm\sqrt{81-56}}{4}=\frac{9\pm5}{4}=1,\frac{14}{4}   x=\ln \left(\frac{9\pm5}{4}\right)=0,\ln \frac{14}{4}    Hope this helps you.  	0.9571279883384705
247283	In this instance, the symbol is an  Iverson bracket , defined by  [\phi] = \begin{cases} 0 & \text{if}\ \phi\ \text{is false} \\ 1 & \text{if}\ \phi\ \text{is true} \end{cases}  where  \phi  is some formula.  	0.9571136236190796
1127404	(7 + \sqrt {48})^n + (7 - \sqrt {48})^n =  (7 + \sqrt {48})^n + \left( \frac{1}{7 + \sqrt {48}}\right)^n   is always an integer. Indeed, always an integer  m  such that  m \equiv 2 \pmod 4.  Including negative  n,  the values are   ..., 2702, 194, 14, 2, 14, 194, 2702,...      such that   a_{n+1} + a_{n-1} = 14 a_n     	0.9570361971855164
736962	As I commented, your approach is doomed because there are no homomorphisms from  \mathbb{Q}  to  \mathbb{Z} .  Here are a couple different possible approaches you could take.    Approach 1: Consider the localization  A_{(0)}  of the  \mathbb{Z} -module  A  obtained by inverting every nonzero element of  \mathbb{Z} .  Explicitly,  A_{(0)}  consists of equivalence classes of fractions  a/n  where  a\in A  and  n\in\mathbb{Z}\setminus\{0\} , where  a/n=b/m  iff there exists a nonzero  k\in\mathbb{Z}  such that  kma=knb .  Construct a bilinear map  A\times \mathbb{Q}\to A_{(0)} , and prove that it has the universal property of the tensor product, so in fact  A_{(0)}\cong A\otimes \mathbb{Q} .  Now use the explicit description of  A_{(0)}  in terms of fractions and the fact that  A  is torsion-free to show that  a\otimes 1=0  implies  a=0 .    Approach 2:  First, prove that the statement you want is true if  A  is a free  \mathbb{Z} -module.  Now note that if  a\otimes 1=0  in  A\otimes\mathbb{Q} , there is a finitely generated subgroup  A_0\subseteq A  such that  a\in A_0  and  a\otimes 1=0  in  A_0\otimes\mathbb{Q}  (to prove this, think about the explicit construction of the tensor product as a quotient of the free  \mathbb{Z} -module on formal expressions  a\otimes b ).  Now use the classification of finitely generated abelian groups to show  A_0  is free (this is the step where you need  A  to be torsion-free).  	0.9569337964057922
1159127	First we prove that if  c = 1  (if the graph is connected) we have at least  n - 1  edges.  A simple path with  n  vertices has  n - 1  edges, thus it's possible. If  \delta \geq 2 , then we have at least  n  edges, thus there is a node with  \delta = 1 . use induction on other vertices, so you have at least  n-2  edges on other vertices of the graph and  n - 1  edges on all vertices of the graph. Now, we have  c  connected components with  a_i  vertices on  i^{th}  component. thus we have at least  \Sigma_{i = 1}^c (a_i - 1) = n - c  	0.9568707346916199
612358	You can show that  x < 1 . Indeed, if  x \geq 1 \Rightarrow 3^x+x \geq 3^1+1 = 4 , and if  x \leq 0 \Rightarrow 3^x + x \leq 3^0 + 0 = 1 < 4 . For if  0 < x < 1 \Rightarrow 3^x + x < 3^1 + 1 = 4 .  	0.956832766532898
981003	I’m not at all familiar with this representation of an elliptic curve, but I think that when you homogenize, to the equation  x^2z^2+y^2z^2=z^4+dx^2y^2 , things may get a bit clearer. The points  (1,0,0)  and  (0,1,0)  on the line at infinity are singular, seems to me, and even before writing the addition-rule projectively, I’ll wager that the sum of any two nonsingular points will again be nonsingular. But then I haven’t checked this.    EDIT : After looking more closely, I realize that the situation is complicated enough that it requires careful description. For typographical and esthetic reasons, I’m changing your description somewhat. The condition that  d  be a nonsquare means that the points at infinity are not  k -rational, where  k  is the constant field (characteristic  \ne2  necessarily). This makes sense arithmetically, but geometrically it does not. So, in my description of the curve, I’ll use the equation  x^2+y^2=1+D^2x^2y^2  instead. It makes all the formulas simpler. I should say that I have the real picture in mind, with  D>1 , and this is how it looks:    Start with a U-shaped curve, open upwards, centered on the positive  y -axis, with its vertex (lowest point) at  (0,1) , and with asymptotes  x=\pm1/D . Then take this, and rotate it  \pi/2  repeatedly, till you have four of these, opening out in all four directions. This is your quartic curve.    For this curve, I want to take a different but isomorphic law of combination: \begin{align} (x,y)+(x',y')&=\left(\frac{xx'-yy'}{1-D^2xx'yy'},\frac{xy'+x'y}{1+D^2xx'yy'}\right)\\ \Bbb O&=(1,0)\\ -(x,y)&=(x,-y)\,, \end{align} so that the neutral point  \Bbb O  is the vertex of the U-shaped curve that sits straddling the positive  x -axis. I’ve made this change only because your original law had the sine addition formula in the numerator of the  x -coordinate, and the negative of the cosine formula in the  y -coordinate, and that annoyed me. We can call the other three vertices  \Bbb P_{\pi/2} ,  \Bbb P_\pi , and  \Bbb P_{3\pi/2} .    Now, when you look at the equation for the curve, you see that its highest-degree form is just  x^2y^2 , and this tells you how the curve intersects the line at infinity: at  (1,0,0)  and  (0,1,0) . But the exponents tell you that these points are two-fold intersections with that line. Indeed, they’re the intersections with the four asymptotes I mentioned above.    Very  crudely, we can desingularize our curve by designating four points at infinity  \Bbb I^\rightarrow_+ ,  \Bbb I^\rightarrow_- ,  \Bbb I^\uparrow_+ , and  \Bbb I^\uparrow_- . The first of these,  \Bbb I^\rightarrow_+ , can be thought of as  (\infty,1/D)  or maybe the limit of  (t,1/D)  as  t\to\infty , even though these points are not on the curve. In a similar way,  \Bbb I^\rightarrow_-  is  (\infty,-1/D) , and you can guess the others. Let’s see how this works out by seeing what the sum  \Bbb I^\rightarrow_++\Bbb I^\rightarrow_+  will turn out to be. The formula gives   \left(\frac{t^2-1/D^2}{1-D^2t^2/D^2},\frac{2t/D}{1+D^2t^2/D^2}\right) \longrightarrow (-1,0)=\Bbb P_\pi\,.     You easily check, using the formulas, that  [2]\bigl(\Bbb P_\pi\bigr)=\Bbb O , which shows that  \Bbb I^\rightarrow_+  is a four-division point on our curve. Furthermore, you also see that  [2]\bigl(\Bbb P_{\pi/2}\bigr)=\Bbb P_\pi .    Without working (!), we’ve found eight rational points on our curve: all division points, and since there are too many  4 -division points for a cyclic group, you see that the torsion is at least  C_2\oplus C_4 , where  C_n  means cyclic of order  n .    Now, finally , to answer your question about infinities, it’s clear. Any time your addition formula leads you to a point on the line at infinity, it’ll be of form  (\infty,\pm1/D)  or  (\pm1/D,\infty) , where here, “ \infty ” refers to an expression that blows up with zero in the denominator. But if  D=\sqrt d  is not  k -rational, then your formulas with coefficients in  k  can not lead you to  1/\sqrt d .  	0.95682692527771
423990	"First of all, this is not how equality is  defined . This is a property that equality has.    In modern first-order logic equality is primitive. We don't define it, we just know that two things are equal if and only if they are the same thing. So two numbers are equal if and only if they are the same number.    For example,  0.\overline 9=1  because both of these are in fact the same number.    So what about  \frac12  and  \frac24 ? Why are they equal? Well, recall that when we say  x=\frac ab  we really mean that  x  is the unique number such that  bx=a . And so we have that:  x=\frac12\iff 2x=1\iff 2\cdot2x=2\cdot1\iff 4x=2\iff x=\frac24  Therefore the two fractional representations of  \frac12  are the same. But why does it surprise us that there are several ways of writing the same number? It shouldn't, after all,  0  is both the unique number such that  1-1  equals to it, as well the smallest number such that  x^2=x , and the unique rational number  x  such that  \sin \pi=x .    There are many ways of writing an expression which evaluates to the same number. And that's fine. But here's the kicker: expressions are not actual numbers, they are string in the language, and whether or not two expressions are equal may (or may not) depend on the interpretation of the symbols involved in these expressions.    So perhaps a better question would be ""What determines if two expressions have the same value in  \Bbb R "", but that question is vague since it doesn't specify the language, and how it might be interpreted. And in different languages we may be able to express different things (e.g. compare what you can express in the language which only includes  \leq  and what you can express in the language which also includes  0,1,+,\cdot  but doesn't include  \leq ).  "	0.9568090438842773
1215349	This is not the case: consider the following matrix,     M=   \begin{bmatrix}     1 & 1 & 0 \\     0 & 0 & 0 \\     0 & 0 & 0   \end{bmatrix}     Now,  Me_1 = (1,0,0)  wich is  not  in  R(M) = \langle (1,1,0) \rangle .  	0.9567233920097351
368432	@Jimmy R. Wondering how important is the original assumption:  b<0	0.956555962562561
1127344	You wish to show that for each  \epsilon > 0  there exists a number  N=N(\epsilon) \in \mathbb{N}  such that for all  n > N ,   \left|\dfrac{2}{n^2}-0\right| = \left|\dfrac{2}{n^2}\right| < \epsilon  Working backwards, we obtain  n^2>\dfrac{2}{\epsilon}\implies \sqrt{n^2}=|n|=n>\sqrt{\dfrac{2}{\epsilon}}  So, take  N  to be   N = \left\lceil\sqrt{\dfrac{2}{\epsilon}}\ \right\rceil  where the ceiling function  \lceil \cdot \rceil  is used.        Proof . Let  \epsilon > 0  be given. Choose  N = \left\lceil\sqrt{\dfrac{2}{\epsilon}}\ \right\rceil . Then for all  n > N ,    \left|\dfrac{2}{n^2}-0\right|=\left|\dfrac{2}{n^2}\right|<\left|\dfrac{2}{N^2}\right|=\left|\dfrac{2}{2/\epsilon}\right|=\epsilon\text{.}    	0.956528902053833
950172	Gaussian elimination: \begin{align} \left[\begin{array}{ccc|c} 1 & -1 & 3 & -5 \\ 5 & 2 & -6 & \alpha \\ 2 & -1 & \alpha & -6 \end{array}\right] &\to \left[\begin{array}{ccc|c} 1 & -1 & 3 & -5 \\ 0 & 7 & -21 & \alpha+25 \\ 0 & 1 & \alpha-6 & 4 \end{array}\right] &&\begin{aligned} R_2&\gets R_2-5R_1 \\ R_3&\gets R_3-2R_1\end{aligned} \\[6px]&\to \left[\begin{array}{ccc|c} 1 & -1 & 3 & -5 \\ 0 & 1 & -3 & (\alpha+25)/7 \\ 0 & 1 & \alpha-6 & 4 \end{array}\right] &&R_2\gets \tfrac{1}{7}R_2 \\[6px]&\to \left[\begin{array}{ccc|c} 1 & -1 & 3 & -5 \\ 0 & 1 & -3 & (\alpha+25)/7 \\ 0 & 0 & \alpha-3 & (3-\alpha)/7 \end{array}\right] &&R_3\gets R_3-R_2 \end{align} If  \alpha\ne3 , the system has unique solution.    If  \alpha=3 , the system has infinitely many solutions.  	0.9564502239227295
560133	Since    0 < \frac{x^{\alpha}}{x + x^2} = \frac{x^{\alpha - 1}}{1 + x} < x^{\alpha -1}, \quad x > 0    and  \int_0^1 x^{\alpha - 1}\, dx = \frac{1}{\alpha} < \infty , by the comparison test,  \int_0^1 \frac{x^\alpha}{x + x^2}\, dx  convergees.  	0.9563927054405212
563785	Let  x_1   denote the number of times 1 has been picked,  x_2  denote the number of times 2 has been picked and so on upto  x_{54}  where each of these vary from  0  to  n .    Calculation of total cases.    Total number of draws is  n .    Hence,  x_1+x_2+\dots +x_{54} = n.     This has number of solutions  \binom{n+53}{53}  as you should know.    Therefore, total number of cases =  \binom{n+53}{53}    Calculation of favorable cases    In this case, all those cases when any  x_i = 0   have to be removed.    Hence the equation becomes  x_1 + x_2 + \dots + x_{54} = n-54 \space\space\space  (by supplying 1 to each  x_i )    This has number of solutions  \binom{n-1}{53} .    So favorable cases =   \binom{n-1}{53}    Hence, probability  P(n)  =   \frac{\binom{n-1}{53}}{\binom{n+53}{53}}     You can also check that  \lim_{n \to \infty} P(n) = 1  .  	0.9563659429550171
1242078	Since  A  is rank one and Hermitian, then we can write \begin{equation}  A = aa^H \end{equation} So the equation is now    \begin{align} z^H \left( B^H + \alpha A^H \right)^{-1} aa^H \ \left( B + \alpha A \right)^{-1} z =c \end{align}  which is  \begin{equation}  \vert a^H(B + \alpha A)^{-1}z \vert^2 = c \end{equation} Let's solve  \begin{equation}  a^H(B + \alpha A)^{-1}z = \sqrt{c}e^{i \theta} = d \end{equation} for  \theta \in \mathbb{R} . This means in your solution, you will have a free  \theta  parameter. Now, let's apply the  Matrix Inversion Lemma  and since  B  is invertible, we can say: \begin{equation}  (B + \alpha aa^H)^{-1}  =  B^{-1} - \alpha \frac{B^{-1}AB^{-1}}{1 + \alpha a^H B^{-1} a} \end{equation} Plugging this we get \begin{equation}  a^H(B^{-1} - \alpha \frac{B^{-1}AB^{-1}}{1 + \alpha a^H B^{-1} a})z =  d \end{equation} which is  \begin{equation}  a^H B^{-1} z   -  \alpha \frac{a^HB^{-1}AB^{-1}z}{1 + \alpha a^H B^{-1} a}  =   d \end{equation} which is  \begin{equation}  [a^H B^{-1} z][1 + \alpha a^H B^{-1} a] - \alpha[a^HB^{-1}AB^{-1}z]  =   d[1 + \alpha a^H B^{-1} a] \end{equation} Let  \begin{align}  \beta_1 &= a^H B^{-1} z \\  \beta_2 &= a^H B^{-1} a\\  \beta_3 &= a^HB^{-1}AB^{-1}z = \beta_2\beta_1 \end{align} Rearranging, we get  \alpha=\frac{\beta_1 - d}{d\beta_2}  	0.9561598896980286
1184172	If you split  x^{k+1}-x^{k}  as  (x^{k+1}-x^{*})- (x^{k}-x^{*})  you see that the limit is  |r-1| , not 1. Perhaps you have stated the definition wrongly.   	0.9561312198638916
193075	You are asking for homomorphisms from the multiplicative group  \mathbb{Q}_p^*  to the additive group of real numbers.    It is known that  \mathbb{Q}_p^*  is a direct product of three subgroups. The infinite cyclic subgroup generated by  p , the cyclic subgroup of order  p-1  consisting of non-zero Teichmüller elements (generated by  \zeta , a primitive root of unity of order  p-1 ), the group  1+p\mathbb{Z}_p  of  p -adic integers congruent to  1 .    A homomorphism from a direct product of groups is determined, if we know its restriction to the constituent groups. The infinite cyclic group is easy to handle, as we can select the image of  p  to by any real number  a  we want, and define  f_a(p^n)=na . The finite cyclic group is also easy to handle: the additive groups has darn few elements of a finite order, so such a homomorphism has to be trivial.    The last factor,  U:=1+p\mathbb{Z}_p , is more interesting. It is actually a bit scary to think about classifying the homomorphisms from  U  to  \mathbb{R} . For one, I can't write down a single non-trivial one. Yet the following argument shows that they exist. Consider the subgroup  K=\langle 1+p\rangle \le U . This is an infinite cyclic group, so we get a homomorphism  \phi_b  from  K  to  \mathbb{R}  simply by sending  1+p  to an arbitrary real number  b , and extending that. Because the additive group of  \mathbb{R}  is  divisible , it is an  injective object  in the category of abelian groups (see e.g. Hilton-Stammbach). Therefore there is a homomorphism  g_b:U\to\mathbb{R}  such that the restriction of  g_b  to  K  is equal to  \phi_b . The proof of this involves a typical use of Zorn's lemma, so if you belong to the church that does not believe in the axiom of choice, then I cannot convince you that such a homomorphism exists.    Given the above there will other such functions. Namely those of the form   f(p^n\zeta^j u)= na + g_b(u).   If  g_b  is the trivial homomorphism, then you get a scalar multiple of the logarithm of the  p -adic value. Otherwise you get something else.  	0.956122100353241
104864	The code is on line 919 of  lib/gprdperm.gi  and is very easy to understand.    It rewrites the normal subgroup in its regular action (so  K  acting on  |K|  points), and then of course the complement subgroup acts on the normal subgroup giving the semidirect product of  H/C_H(K)  with  K . If  C_H(K) \neq 1 , then it does the subdirect product smooshy thing with  H 's original rep: that is  H \ltimes K  acts on  X \dot\cup K  where  H  acts on  X  as usual,  K  centralizes  X , and  H,K  have the previously described action on  K .    In particular, this method is impractical with  S_{20} as the normal subgroup.    If K  acts on  Y  and  |H|  is small, then  H \ltimes K  can act on  H \times Y  as well, but this is not implemented.    There is a special method implemented on line 961 to catch a nice case: if K  acts on  Y , and if the action of  H  on  K  lifts to a homomorphism from  H  to  \operatorname{Sym}(Y) , then one can view  H  more compactly as a subgroup of  \operatorname{Sym}(X \dot\cup Y) . This method is not guaranteed to succeed even if applicable if  C_{\operatorname{Sym}(Y)}(K)$ is large, as the method to check if the homomorphism lifts is just to choose a random section and see if it a homomorphism.  	0.9560655951499939
1012600	We can just view this as applying the map   f(x) = x+t   and asking is  f(A)  open when  A  is.     Now  f  has the inverse   g(x) = x-t.     The inverse image of an open set under a continuous mapping is continuous so    g^{-1}(A)   is open and we are done.    How do we prove: The inverse image of an open set under a continuous mapping is continuous . This is actually a definition of continuity...   	0.9560604095458984
1271677	Theorem:  Let  G  be a group and  H\le G  be a subgroup of  G . If  [G:H]=2 , Then  H \vartriangleleft G  is a normal subgroup of  G .    The converse of the above theorem is not always true. (An obvious example would be  G\vartriangleleft G .)    Another example is  \langle a^2 \rangle \vartriangleleft D_8 . First, verify that  Z(D_8) = \{1,a^2\} . And then, in particular, you can conclude that  \langle a^2 \rangle=\{1,a^2\}  is a normal subgroup of  D_8 .  	0.9560231566429138
938214	The right side is the factorized form of the left.    The factorized form always exists if the field you're working in is  algebraically closed . In particular, since the complex numbers are algebraically closed, every polynomial  p(x)  can be factored into some constant times factors of the form  x-r_i  where  r_i  is a root of  p(x) .     p(x) = c(x-r_1)(x-r_2)\cdots(x-r_n)     Where  p(x)  is of degree  n . Moreover, this factorization is unique up to the ordering of the linear factors  x-r_i .  	0.9558638334274292
976080	"I left about three wrong comments due to mixing up  p_1  and  p_2 , so let me say it the right way this time.    It is not a Banach space, in general.  Take  E = (0,1)  with Lebesgue measure, for instance, and  p_1 = 2 ,  p_2 = 4 .  The function  f(x) = x^{-1/3}  is in  L^2  but not in  L^4 .  Let  f_n  be a sequence in  L^4  which converges, in  L^2  norm, to  f ; for instance,  f_n = \max(f, n) .  This sequence is Cauchy in  L^2  norm, but it does not converge in  L^2  norm to any element of  L^4 .    One can show it is only a Banach space if  E  is a finite union of atoms.  In such a case  L^{p}(E)  is finite dimensional for all  p  and so all norms are equivalent.    For practical purposes, you should expect that a given vector space has at most one ""reasonable"" Banach norm, up to equivalence. A second norm on a Banach space is either equivalent to the first one, or is incomplete, or is some pathological non-constructive axiom of choice monster.  You can read more about this in Schechter,  Handbook of Analysis and its Foundations . In this case the norms are clearly not equivalent (unless, as mentioned,  L^p(E)  is finite dimensional), and they are both very explicit, so we expect they can't both be complete.  "	0.9558252096176147
777420	Suppose you  have: \begin{align} \Pr(X = 0 \mid k=0) & = 2/3 & & & \Pr(X=1\mid k=0) & = 1/3 \\[10pt] \Pr(X = 0 \mid k=1) & = 1/2 & & & \Pr(X=1\mid k=1) & = 1/2 \end{align} If you want the marginal distribution of  X , you  average  the two rows above, with weight equal to  \Pr(k=0)  for the first row and to  \Pr(k=1)  for the second row.  Let's suppose those weights are each  1/2 .  Then the joint distribution is: \begin{align} \Pr(X=0\ \&\ k=0) & = 1/3 & & & \Pr(X=1\ \&\ k=0) & 1/6 \\[10pt] \Pr(X=0\ \&\ k=1) & = 1/4 & & & \Pr(X=1\ \&\ k=1) & 1/4 \end{align} Then rather than averaging, you sum, to get \begin{align} \Pr(X=0) = & \frac 7 {12}, & & & \Pr(X=1) & = \frac 5 {12}. \end{align}    In one case you have a conditional distribution given  k , and you find an average; in the other case you have a joint distribution, and you find the sum.  Either way you get the same marginal distribution.    In the situation you describe, the von Mises distribution is a conditional distribution given the value of  \kappa .    The average should be a weighted average with weights given by the distribution of  \kappa .  	0.955813467502594
915698	Let  C  be the cycles of length  4  in a graph. Per your definition, the cycles of length  4  are linearly dependent if and only if there exists a nonzero function  f: C \to \mathbb{R}  such that on every edge  e  we have  \sum_{e \in C} f(C) = 0 .     Then it becomes clear that Nate's answer is correct. In fact if you take the  5 \times 5  grid, and glue the top edge to the bottom and left edge to the right, then we see the  4  cycles exactly correspond to cells in our grid. We can assign  \pm1  to each cycle in a checkerboard fashion. Since we glued the boundaries we have each edge is part of exactly two 4 cycles, and by our assignment one of these has  +1  and the other  -1 . Thus the cycles are linearly dependent.   	0.9557976722717285
405098	You mean to take the inverse. write your equation as     y=0.03(x-18)^3 +60  then   y-60=0.03(x-18)^3   \frac{y-60}{0.03}=(x-18)^3   \sqrt[3]{\frac{y-60}{0.03}}=x-18   \sqrt[3]{\frac{y-60}{0.03}}+18=x    so     g(y)=\sqrt[3]{\frac{y-60}{0.03}}+18  	0.9557787179946899
698231	Let's assume your whole equation is    ∇⋅(f(1−f)∇f)=g   Given some approximation  f  you want to compute the Newton step  s  so that  f+s  solves the equation up to errors  O(\|s\|^2) . Extracting the linear factors in  s  gives   ∇⋅([f(1−f)+(1-2f)s]∇(f+s))+O(\|s\|^2) \\ =∇⋅(f(1−f)∇f)+∇⋅((1-2f)s∇f+f(1−f)∇s)+O(\|s\|^2)   Thus you would get to solve    ∇⋅((1-2f)∇f·s+f(1−f)∇s) = g-∇⋅(f(1−f)∇f)   	0.9557642936706543
1142389	Guide:     \begin{bmatrix} x_{n} \\ y_{n}\end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 2 & 2\end{bmatrix} \begin{bmatrix} x_{n-1} \\ y_{n-1} \end{bmatrix}        Find a relationship between  \begin{bmatrix} x_{n} \\ y_{n}\end{bmatrix}  and  \begin{bmatrix} x_{n-2} \\ y_{n-2}\end{bmatrix} .    Find a relationship between  \begin{bmatrix} x_{n} \\ y_{n}\end{bmatrix}  and  \begin{bmatrix} x_{0} \\ y_{0}\end{bmatrix} .    Raising power of  \begin{bmatrix} 1 & 3 \\ 2 & 2\end{bmatrix}  might help. Diagonalization might help.    	0.9557396173477173
231466	"A wide (resp. tall) matrix  A  has  no  right (resp. left) inverses if and only if it has deficient row (resp. column) rank, i.e. if and only if the rows (resp. columns) of  A  are linearly dependent. Owing to the dependency between rows/columns, what you mean by  ""uniform distribution""  is unclear.    At any rate, here are some methods that you may consider. Without loss of generality, suppose  A  is an  m\times n  wide matrix (so that  m"	0.9557133913040161
542948	I don't think this is standard notation and could be difficult to read correctly as a result.    It would be clearer to say  P = (a_0, a_1, \dots, a_n) \text{ where } a_i \in O \text{ for each } i .    Then you can just say  Q(a_i) = i . Then it's clear that if  a = a_i  then (assuming  Q  is well-defined)  Q(a) = Q(a_i) = i .    If  Q  is only defined on the elements of  O  in the sequence  P  you can just define that to be its domain.        Q  is a function on  \{a_1, \dots, a_n\} ....      or something like that.    If all your sequences are going to be finite, it might also be clearer to talk about them as tuples.  	0.9556559920310974
542406	It is probably pa precision thing. Computers do not have infinite space and can therefore represent only a finite set of numbers exactly. Any operation you do on the numbers which results in a number that the computer cannot represent will result in rounding.     This means that there exists some  smallest number  which is still greater than  1  and is still representable by the computer, and if the result of  1+\frac1n  is close enough to  1 , the computer will make an estimation that  1+\frac1n = 1 , therefore,  \left(1+\frac1n\right)^n = 1^n=1.  	0.9556549787521362
129907	Obviusly  (\{\emptyset\},d)  defines a metric space, the trivial one. Or you can say vacuously. You can check that the rules of a metric space are satisfied.  	0.955588161945343
405085	This is essentially the problem of identifying  (\Omega, F, \Bbb{P})  with the space  ([0, 1], \mathcal{B}, \mathrm{Leb})  equipped with the Lebesbue measure on Borel sets. (So if you can take it granted, then there is nothing to prove.)    A direct approach is also possible, but we give a more neat approach using characteristic functions:    \begin{align*} \Bbb{E}\exp\left\{ it \sum_{n=1}^{\infty} \frac{X_{n}}{2^{n}} \right\} &= \prod_{n=1}^{\infty} \Bbb{E} \exp\left\{ \frac{it X_{n}}{2^{n}} \right\}  = \prod_{n=1}^{\infty} \Bbb{E} \exp\left\{ \frac{it X_{n}}{2^{n}} \right\} \\ &= \prod_{n=1}^{\infty} \left( \frac{1 + \exp\{ 2^{-n}it \}}{2} \right). \end{align*}    By noting that     (1 - \exp\{2^{-N}it\}) \prod_{n=1}^{N} \left( \frac{1 + \exp\{ 2^{-n}it \}}{2} \right) = \frac{1 - e^{it}}{2^{N}},     it follows that     \prod_{n=1}^{\infty} \left( \frac{1 + \exp\{ 2^{-n}it \}}{2} \right) = \lim_{N\to\infty} \frac{1 - e^{it}}{2^{N}(1 - \exp\{2^{-N}it\})} = \frac{e^{it} - 1}{it},     which is exactly the characteristic function for the uniform distribution on  [0, 1] . Therefore the proof is complete.       Here is a more direct approach. Let  D_{m} = \{ k/2^{m} : 0 \leq k \leq 2^{m} \}  and  D = \cup_{m=1}^{\infty} D_{m}  be the set of dyadic numbers on  [0, 1] . Also we put     Y = \sum_{n=1}^{\infty} \frac{X_{n+1}}{2^{n}}.     Then by the right-continuity of  x \mapsto \Bbb{P}(X \leq x) , it suffices to prove that  \Bbb{P}(X \leq x) = x  for dyadic  x \in D . We prove that this is true for  x \in D_{m}  for all  m  by induction.      When  m = 1 , only the non-trivial case arises when  x = 1/2 . But in this case, we have   \Bbb{P}(X \leq 1/2) = \Bbb{P}(X_{1} = 0) + \Bbb{P}(X_{1} = 1 \text{ & } X_{2} = X_{3} = \cdots = 0 ) = 1/2.    Assume that the claim holds for  x \in D_{m} . Then for  x \in D_{m+1} \setminus D_{m} , we have either  x \in (0, 1/2)  or  x \in (1/2, 1) . By noting that  X_{0} \! \perp\!\!\perp \! Y  and  Y \stackrel{\text{law}}{=} X , in the former case   \Bbb{P}(X \leq x) = \Bbb{P}(X_{1} = 0 \text{ & } Y \leq 2x) = \Bbb{P}(X_{1} = 0) \Bbb{P}(X \leq 2x) = \tfrac{1}{2}(2x) = x   and in the latter case \begin{align*} \Bbb{P}(X \leq x) &= \Bbb{P}(X_{1} = 0) + \Bbb{P}(X_{1} = 1 \text{ & } Y \leq 2x-1) \\ &= \tfrac{1}{2} + \Bbb{P}(X_{1} = 1)\Bbb{P}(X \leq 2x-1) \\ &= \tfrac{1}{2} + \tfrac{1}{2}(2x-1)  = x. \end{align*}      Therefore the conclusion follows by mathematical induction.  	0.9555462002754211
369035	F  induces the map  g  iff it respects the relation given by  h , i.e. if  F(x,t)=F(y,s)  whenever  h(x,t)=xt=ys=h(y,s) . But these products are equal iff  t=s=0  (or  (x,t)=(y,s) ), and in this case  F(x,0)=e_x=F(y,0) . means that  F  factors as  gh , and  g  is continuous since  D^2  has the final topology for  h . Also if  z\in\partial D^2 , then  z=h(z,1) , so  f(z)=F(z,1)=gh(z,1)=g(z) .    For the other direction (where  g:D^2\to X  is given), you only need to compose.    In the end, it all boils down to a homeomorphism  S^1\times I/S\times\{0\}\cong D^2  since the maps starting at the left term are exactly the homotopies  F  from  F|_{S^1\times\{1\}}  to a constant map at  F(x,0) .  	0.9554879069328308
360491	Another approach is this. Let  S  be the set of all lines. Then one can construct a surjection  [0,2\pi)\times \Bbb R^2\to S , as follows: pick a point in  \Bbb R^2 ; and an angle, and obtain a line. Of course any line can obtained in this way. (Note this is not an injection!). But we also have  an injection  \Bbb R\to S  by taking a real number  r  to the horizontal line  y=r . Thus, we have  \mathfrak c=|\Bbb R|\leqslant |S|\leqslant |[0,2\pi)\times \Bbb R^2|=\mathfrak c^3=\mathfrak c  	0.9554258584976196
435849	If  (f_x(t),f_y(t),f_z(t))  denotes the position of an object in space at time  t , then  \langle f'_x(t),f'_y(t),f'_z(t)\rangle  returns the rate of change of the object with respect to time, i.e. velocity.  Thus, it's a vector pointing in  the direction of motion.    For example, if the functions  (f_x(t),f_y(t),f_z(t)) = (t\cos(t), t\sin(t), t),  parametrize a helix with growing diameter. The motion along this curve with its velocity vector looks like so:     	0.9552144408226013
528059	What you have proved is that for a fixed  t\in\mathbb{R} , given  \epsilon>0  there is a polynomial  p  such that  |f(t)-p(t)|<\epsilon . But the Weierstrass approximation theorem is about uniform approximation;  |f(t)-p(t)|<\epsilon  must hold for all  t  on a given set.    It is true that given an interval  [-i,i]  and  \epsilon>0  there is a polynomial  p  such that  |f(t)-p(t)|<\epsilon  for all  t\in[-i,i] . But  p  depends on  \epsilon   and   i .    You can see that the theorem does not hold on unbounded sets by considering for example  f(t)=e^t . Suppose that there is a polynomial  p  such that  |e^t-p(t)|<1  for all  t\in\mathbb{R} . Let  n  be the degree of  p . Then we would have  e^t\le p(t)+1  and   \lim_{t\to+\infty}\frac{e^t}{t^{n+1}}=0.   	0.9551623463630676
165133	There is surely an elementary method, but you can use some Galois theory. I am following (by heart, hopefully correctly) the approach of Kaplansky in his  Fields and Rings .    Let  E = F(x_1,...,x_4) , and  K = F(x_1,...,x_4)^G  be the fixed field under  G = S_4 . Then  \operatorname{Gal}(E/K) \cong S_4 . Note that  G  permutes the  y_i  in all 6 possible ways. In particular,  h(x) = (x - y_1)(x - y_2)(x - y_3) \in K[x] ,  L  is the splitting field of  h(x)  over  K , and  \operatorname{Gal}(L/K) \cong S_3 . Now note that the Klein four group fixes each  y_i , and the Galois correspondence will tell you that  L  is the fixed field of the Klein four group.  	0.9551308155059814
