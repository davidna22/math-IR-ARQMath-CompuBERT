pid	doc	score
1197878	This is the Holder's inequality for  q=1  and  p=\infty  i.e.     \begin{align*} \|v u\|_2 &=\sqrt{\|v^2 u^2 \|_1}\\ &\leq    \sqrt{\| v^2\|_\infty}\sqrt{ \| u^2\|_1}\\ &=\|v\|_\infty \|u\|_2 \end{align*} And for the proof someone can see Martin's answer.    In certain cases the previous version of Holder's can be, easily, derived from the more popular one, i.e. when  1	0.9680376052856445
729924	Let  A  be some  m \times n  real matrix. Let  \|A\|_1  be the  maximim absolute column sum norm ,  \|A \|_\infty  be the  maximim absolute row sum norm  and  \|A \|_2 = \sigma_{\max}  be the  2-norm  or  spectral norm  ( \sigma_{\max}  is the maximum singular value). We have the following fundamental inequalities:  (i) \quad \frac{1}{\sqrt{n}} \|A \|_\infty \leq \|A \|_2 \leq \sqrt{m} \|A \|_\infty   (ii) \quad \frac{1}{\sqrt{m}} \|A \|_1 \leq \|A \|_2 \leq \sqrt{n} \|A \|_1    \|A \|_2 \leq \sqrt{\|A \|_1 \|A \|_\infty} \quad (\text{some kind of Hölder's inequality}) .    So we see that if you impose additionally that also  \|A \|_\infty \leq 1  (i.e. row sum is also small) then your assertion holds.    However it's easy to construct (quadratic) matrices with small column sum but big row sum (and vice versa). And inequality  (i)  (or  (ii)  whatever you are starting with) forces the largest singular value to be big. More precisely the following matrix   \begin{pmatrix} 0.99 & 0.99 & 0.99\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix}   fulfilles your condition but has singular value  1.71473 .  	0.9676823616027832
268234	Not true for the spectral norm you use. Simple example:   A = \begin{bmatrix}1 & 1 \\ -1 & 1\end{bmatrix}, \quad \tilde{A} = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix},     \|A\|_2 = \sqrt{2}\approx 1.4142, \qquad  \|\tilde{A}\|_2 = \sqrt{\frac{\sqrt{5}+3}{2}}\approx 1.6180.   On the other hand, zeroing out entries of  A  to get  \tilde{A}  can give:      strict inequality for the Frobenius norm (2-norm of the vector of the singular values):  \|\tilde{A}\|_F < \|A\|_F ,   non-strict inequalities for some other norms, e.g.:  \|\tilde{A}\|_{\star} \leq \|A\|_{\star} , where  \star = 1,\infty  (usual matrix  p -norms) or  \star=M , where  \|A\|_M=\max_{i,j}|a_{ij}| .    	0.967046856880188
1054051	A\in\mathbb{R}^{n\times n} , symmetric and  e:=[1, 1, \ldots, 1]^{\top} . Now,    \begin{equation} e^{\top}Ae=\sum_{i,j}A_{i,j}=:c \end{equation}    and,    \begin{align} Aee^{\top}A=\alpha\alpha^{\top} \end{align}    where  \alpha:=Ae  denotes the vector of row (or column) sums of  A .    So,    \begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}=\|A-c\alpha\alpha^{\top}\|_{F} \end{equation}    One way to proceed can be to apply triangle inequality. For that, note that for a rank-1 symmetric matrix  xx^{\top} ,  \|xx^{\top}\|_{F}=\|x\|_{2}^{2} . This is because for any matrix, the Frobenius norm is the  2 -norm of the vector of singular values, and  xx^{\top}  has just one non-zero eigenvalue, namely,  \|x\|_{2}^{2} . This gives    \begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}\le \|A\|_{F}+\frac{1}{c}\|\alpha\|_{2}^{2}. \end{equation}    Is this the kind of characterization you are looking for?  	0.96689772605896
834405	Yes; apply Holder's inequality (you can also use Jensen if you like): \begin{align*} \|f\|_{L^p}=\|f\cdot1\|_{L^p}\leq\|f\|_{L^q}\|1\|_{1/(1/p-1/q)}=\|f\|_{L^q}\mu(X)^{1/(1/p-1/q)}\leq\|f\|_{L^q} \end{align*} since we assumed  \mu(X)<1 , and  1/(1/p-1/q)\geq0  from  p\leq q .    Edit:  Please note I used a slightly generalized version of the Holder inequality for the case where a product is being estimated in  L^p  for  p  not necessarily  1 . This is described  here .  	0.9667444825172424
1060956	I'm posting this as an answer, since it doesn't fit in a comment, and seems to be pretty darn close to what is required.    I get the following bound: \begin{align} \|f-g\|_p^p  &= \| |f-g|^p \|_1 \\  &\le \| |f-g| |f-g|^{p-1} \| \\  &= \| f-g \|_2 \| |f-g|^{p-1} \|_2 \tag{by Hölder} \\  &= \|f-g\|_2 \left( \int |f-g|^{2p-2} \right)^{\frac{1}{2}} \\  &\le \|f-g\|_2 \left( \int (|f|-|g|)^{2p-2} \right)^{\frac{1}{2}} \tag{triangle inequality} \\  &\le \|f-g\|_2 \left( \int (2C)^{2p-2} \right)^{\frac{1}{2}} \\  &= (2C)^{p-1} \sqrt{\mu(X)} \|f-g\|_2 \end{align} This differs from the desired bound in three ways:  (1) the exponent on  C  is  p-1 , not  p-2  (this could be a clerical error on my part), (2) the extra 2 in the constant, and (3) it appears to me that the inequality also depends on the measure of the space.  	0.9645787477493286
615535	Partial results:    We define   \DeclareMathOperator{\tr}{trace} \|A\|_1 = 1'|A|1 = \sum_{i,j}|a_{ij}|\\ \|A\|_2 = \sqrt{\tr(A'A)} = \sqrt{\sum_{i,j}|a_{ij}|^2}    These are  the entrywise  1 -norm and entrywise  2 -norm (AKA Frobenius norm) respectively.  Both are submultiplicative matrix norms.    We note that for any matrix  A ,   \|A\|_2 \leq \|A\|_1 \leq n \|A\|_2   By the information given, we note that  \|A\|_2^2 = \tr(A'A) = o(n) . In particular, this means that  \|A\|_2 = o(\sqrt{n}) .  Now, because the norms are submultiplicative, we also have   \|A^3\|_2 \leq \|A\|_2^3 = o(n^{3/2})   From there, we have   1'A^31 = \|A^3\|_1 \leq n \|A^3\|_2 = o(n^{5/2})=o(n^{2.5})   Which may be the best we can do.       The information is  1'A1 = O(n)  doesn't let us do better directly.  In particular, we have   \|A^3\|_1 \leq \|A\|_1^3 = O(n^3)   	0.9642890095710754
456189	This follows from  Hölder's inequality . Letting  r  denote the Hölder conjugate of  \frac{q}{p} , we have  \|f\|_p^p = \|f^p\|_1 \leq \|f^p\|_{q/p}\|1\|_r = \|f^p\|_{q/p} = \|f\|_q^p  so  \|f\|_p \leq \|f\|_q . More generally, if  (X, \mathcal{M}, \mu)  is a finite measure space, the same proof shows that  \|f\|_p \leq \|f\|_q\mu(X)^{\frac{1}{p}-\frac{1}{q}} .  	0.9640804529190063
80865	To prove the stated inequality (for normalized measure spaces): Let  X  be a finite measure space and  q \le p . We have using Hölder for  f \in L^p  \begin{align*}    \|f\|_q &= \bigl\||f|^q\bigr\|_1^{1/q}\\\            &= \bigl\|1 \cdot |f|^q\bigr\|_1^{1/q}\\\            &= \|1\|_{p/(p-q)} \bigl\||f|^q\bigr\|_{p/q}^{1/q}\\\            &= \mu(X)^{(p-q)/p} \|f\|_p \end{align*} If  \mu  is normalized, i. e.  \mu(X) = 1  (for example in  [0,1]  or  \mathbb T  with normalized arclength), then  \|f\|_q \le \|f\|_p  for every  q \le p . especially  \|f\|_{p-1} \le \|f\|_p .  	0.964065670967102
1171638	Observe we have that \begin{align} \|Ax\|_2^2= x^TA^TAx \end{align} where  A^TA  is diagonalizable since it's symmetric. Then we see that \begin{align} x^TA^TAx = y^TDy \end{align} where  D  consists of the square of the singular values. Hence, we get that \begin{align} \|Ax\|_2^2 = \lambda_1^2y_1^2+\lambda_2^2y_2^2. \end{align} In the case when  \|x\|_2=1  then we also have that  \|y\|_2=1 . Hence it follows \begin{align} \|Ax\|_2^2 \leq \max_{1\le i \le 2}\lambda_i^2 \ \ \implies \ \ \|Ax\|_2 \leq \max_{1 \le i \le 2}\lambda_i. \end{align}    Note that I say  singular value , not  eigenvalue .    Additional : Consider \begin{align} A= \begin{pmatrix} 0 & 1\\ 0 & 0 \end{pmatrix} \end{align} then we see that  \begin{align} \sup_{\|x\|_2=1}\|Ax\|_2 = 1 \end{align} but the eigenvalues of  A  are  0 . Moreover, observe that \begin{align} A^TA= \begin{pmatrix} 0 & 0\\ 0 & 1 \end{pmatrix} \end{align} which means the singular values of  A  are equal to  1  and  0 .  	0.9624617099761963
850252	"Here is a proof for the Euclidean norm. This approach is buried in Schäffer. J., "" Norms and determinants of linear mappings "", Technical report, CMU, Department of Mathematical Sciences, 1970. He does not use the SVD, but the idea is essentially the same.    Let  A=U \Sigma V^*  be a singular value decomposition of  A , with  \Sigma=\operatorname{diag} (\sigma_1,...,\sigma_n) , and  \sigma_1\ge ... \ge\sigma_n .    Then  \|A\| = \sigma_1, \|A^{-1}\| = {1 \over \sigma_n} , and  |\det A| = \sigma_1 \cdots \sigma_n .    Hence  |\det A| \le \sigma_1 \cdots \sigma_{n-1} {1 \over \|A^{-1} \|} \le \|A\|^{n-1} {1 \over \|A^{-1} \|} , from which we obtain  |\det A| \|A^{-1} \| \le \|A\|^{n-1} .    The result is not true for general operator norms, for example, with  A=\begin{bmatrix} {1 \over 2} & 1 \\ 0 & 2 \end{bmatrix} , we have  \det A = 1 ,  \|A^{-1} \|_\infty = 3, \|A\|_\infty  = 2  and so  3=\|A^{-1} \|_\infty \not< \|A\|_\infty^{2-1} = 2 .    As an aside, it is worth noting the related  Hadamard's inequality ,  |\det A| \le \|Ae_1\| \cdots \|A e_n\|  (Euclidean norm).  "	0.962037205696106
465997	It is maybe a too big cannon to use (or prove) the continuity of eigenvalues. It is well known that the (matrix) norms are continuous. This follows from the triangle inequality:   \|A\|\leq\|B\|+\|A-B\|, \quad \|B\|\leq\|A\|+\|A-B\|\quad\Rightarrow \quad |\|A\|-\|B\||\leq\|A-B\|.   Therefore, if  A  and  B  are close then  \|A\|  and  \|B\|  are close as well.    Then use the fact that  \|A\|=\rho(A)  when  \|\cdot\|:=\|\cdot\|_2  is the spectral norm and  A  is SPD.    You can relate the componentwise closeness to the normwise closeness by realizing that   \|A\|_2=\max_{\|x\|_2=1}\|Ax\|_2\leq n\max_{1\leq i,j\leq n}|a_{ij}|=:n\|A\|_{\max}   (or by simply showing that  \|A\|_{\max}  is a norm and use the fact that norms on a finite-dimensional space are equivalent). Therefore, if  |a_{ij}-b_{ij}|<\epsilon  for all  i,j=1,\ldots,n  and some  \epsilon>0 , we have  |\|A\|_2-\|B\|_2|<\delta  with  \delta:=\epsilon/n .  	0.9620344042778015
446096	Any induced norm satisfies  \|A\|\ge \rho(A) , where  \rho(A)  is the spectral radius of  A ,  i.e.  the largest absolute value of its eigenvalues. This fact is stated without proof  in Wikipedia ; here is a proof.        Any induced norm is of the form    \|A\|=\max_x\frac{\|Ax\|}{\|x\|}.  Let  x^*  be an eigenvector corresponding to the eigenvalue  \lambda^*  with the largest absolute value. Then  \|A\|\ge\frac{\|Ax^*\|}{\|x^*\|}=\frac{\|\lambda^*x^*\|}{\|x^*\|}=|\lambda^*|\frac{\|x^*\|}{\|x^*\|}=|\lambda^*|.      Let  A=\begin{bmatrix}1&0\\0&0\end{bmatrix} . Then your norm is  \|A\|=\dfrac1{\sqrt2}\|A\|_F=\dfrac1{\sqrt2}  while  \rho(A)=1 , contradicting the above inequality.  	0.9617016315460205
1050732	For the identity map  I      \|I\|_{op}=\sup \{|L(x)| : |x|=1 \}=\sup \{|x| : |x|=1 \}=1.     \|I\|_{tr} = \sum_{i=1}^{n}\sum_{j=1}^{n}(a_{i,j})^2= \sum_{i=1}^{n}\sum_{j=1}^{n}(\delta_{i,j})^2=n    (I recall that  \delta_{i,j}=1  if  i=j  and  0 , otherwise).    I guess that there should be taken a root from  \|L\|_{tr}  in order to make it norm, that is to assure that  \|\lambda L\|_{tr}=|\lambda|\|L\|_{tr}  for each real  \lambda , because in the present state we have  \|\lambda L\|_{tr}= |\lambda|^2\|L\|_{tr} . In particular,  \|\lambda I\|_{op}=|\lambda| , whereas  \|\lambda I\|_{tr}=|\lambda|^2 , so  0<|\lambda|<1  the left part inequality fails too. But for corrected norm  \|L\|_{tr}=\sqrt{\sum_{i=1}^{n}\sum_{j=1}^{m}(a_{i,j})^2}  the inequality   \|L\|_{op} \leq \|L\|_{tr}   indeed follows from Cauchy-Schwartz inequality.  	0.9616745114326477
877760	If  A  is positive definite then the eigenvalues of  A  are all positive. In particular, the smallest eigenvalue, say  \lambda_s , will be nonzero. Next observe \begin{align} \frac{1}{2}x^TAx-b^Tx=\frac{1}{2}x^TQ^TDQx-b^Tx=\geq\frac{\lambda_s}{2}\| Qx\|^2-\|b\|\|x\|=\frac{\lambda_s}{2}\|x\|^2-\|b\|\|x\|. \end{align} Thus as  \|x\|\rightarrow \infty  we see that  f  will also go to infinity.    For the converse direction, assume  A  is not positive definite then there is an eigen-direction corresponding to a non-positive eigenvalue, say  x . Then we see that if  \lambda<0  then \begin{align}  f(\alpha x)= \frac{\lambda\alpha^2}{2}\|x\|^2-\alpha b^Tx\leq \frac{\lambda\alpha^2}{2}\|x\|^2+|\alpha|\|b\|\| x\| \end{align} which goes to negative infinity as  |\alpha|\rightarrow \infty . For the case  \lambda=0 , we will leave it as an exercise for the reader.  	0.9615047574043274
1083970	I mistakenly thought  \|\cdot\|  was the  \ell^2 -norm. See below the line for the general situation.    Hint:  \|Ax\|^2=\left\|\begin{bmatrix}\lambda_1 x_1 \\ \vdots \\ \lambda_n x_n\end{bmatrix}\right\|^2 = \lambda_1^2 x_1^2 + \cdots + \lambda_n^2 x_n^2 \le (\max_i \lambda_i^2) (x_1^2 + \cdots + x_n^2) = (\max_i \lambda_i^2) \|x\|^2.  By looking at the definition of  \|A\| , can you now compute  \|A\| ? Computing  \|A^{-1}\|  is similar, since it is also a diagonal matrix.       General situation:      For any  [submultiplicative] matrix norm   \|\cdot\| , we have  \|A\| \ge \max_i |\lambda_i| . (See below.)   Since subordinate norms are submultiplicative matrix norms, this inequality holds in the setting of your question.   Moreover, by considering  x  being the standard basis vectors, we see that we actually have the equality  \|A\| = \max_i |\lambda_i| . Can you conclude from here?      Proof of Claim 1: Let  \|\cdot \|  be a [submultiplicative] matrix norm. Let  x  be a  \lambda_i -eigenvector, and let  X  be the  n \times n  matrix whose columns are all  x . Then  |\lambda_i| \|X\| = \|\lambda_i X\| = \|A X\| \le \|A\| \|X\|.  	0.9613463282585144
71529	You could of course generalize your current measure    \begin{align}  S(X) = \frac{\frac{k^{(1/m)}}{k^{(1/n)}} -\frac{\|X\|_m}{\|X\|_n} } {\frac{k^{(1/m)}}{k^{(1/n)}}-1} \end{align}    while preserving your properties you specified.    An interesting special case could be  m = 1, n \to \infty , in which case the expression simplifies to    \begin{equation}  S(X) = \frac{k-\frac{\|X\|_1}{\|X\|_c}}{k-1} \end{equation}    where  c = \infty , (for some reason, mathjax refused to render when I inserted  \infty  directly in the fraction)  	0.9608859419822693
590577	What you are doing is actually using Schwarz inequality to prove Schawarz inequality.    In the last step you need to use the Holder's Inequality as follows \begin{align} |\langle x,y\rangle|& = \biggl{|}\sum_\limits{n=1}^{\infty}\langle x,e_1\rangle \overline{\langle y,e_i\rangle}\biggr{|}\\ &\leq \sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|| \overline{\langle y,e_i\rangle}|\\ &\leq \sqrt{\sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|^2} \sqrt{\sum_\limits{n=1}^{\infty}|\langle y,e_1\rangle|^2} && \text{ (Holder's Inequality)}\\ &\leq \|x\|\|y\| && \text{(Bessel's Inequality)}. \end{align}    \textbf{EDIT:}  Sorry for this not helping answer. An easy way out is as follows:    Let  x\in X  and let  y\neq 0  be given. Then just take  e=\frac{y}{\|y\|} . This  \{e\}  only makes an orthonormal system. So, by Bessel's Inequality it will follow that  |\langle x,e\rangle|^2 \leq \|x\|^2 . Now just multiply both sides by  \|y\|  to get  |\langle x,y\rangle|^2\leq \|x\|\|y\| .  	0.9608449935913086
952796	HINT: we must show that if  q\neq p  then these norms are not equivalent, that is doesnt exists  K\in(0,\infty)  such that    K\|f\|_q\le\|f\|_p,\quad\forall f\in C(I,\Bbb K)    or equivalently    \frac{\|f\|_q}{\|f\|_p}\le K^{-1}    for  f\neq 0 . Choosing  f(x):=a^x ,  q=2  and  p=1  and  I:=[0,1]  we have that it must be true that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}\le K^{-1},\quad\forall a>1    but the above is equivalent to say that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}=\frac{\sqrt{\frac{a^2-1}{2\ln a}}}{\frac{a-1}{\ln a}}=\sqrt{\frac{(a+1)\ln a}{(a-1)2}}\le K^{-1},\quad\forall a>1    what cannot be possible, hence  \|{\cdot}\|_2  and  \|{\cdot}\|_1  are not equivalent. We can generalize this result easily for any  q>p  and any interval  [\alpha,\beta] .  	0.9605357050895691
1068908	It is valid but as noted the Gershgorin theorem is quite an overkill. A more elementary approach follows.    Let  A=D-B , where  D  is the diagonal part of  A . Let  A  be row diagonally dominant, that is,  \tag{1} |a_{ii}|>\sum_{j\neq i}|a_{ij}|\;\iff\; 1>\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|, \quad i=1,\ldots,n.   The latter inequality is equivalent to  \tag{2}1>\|D^{-1}B\|_\infty.    If  A  was singular,  Ax=0  and hence  x=D^{-1}Bx  for some nonzero  x . We would have   \|x\|_\infty=\|D^{-1}Bx\|_\infty\leq\|D^{-1}B\|_\infty\|x\|_\infty   and, dividing by  \|x\|_\infty ,   1\leq\|D^{-1}B\|_\infty,   which contradicts (2).  	0.9605176448822021
1282943	As @Alonso Delfín mentioned, the desired conclusion is wrong. Actually this question is related to the so-called dual norm. By Hölder inequality, for  a,x\in\mathbb{R}^n , there holds   \sum |a_ix_i|\leq\|a\|_{\ell^p}\|x\|_{\ell^q},   where  p^{-1}+q^{-1}=1  for  1\leq p,q\leq\infty  and for each  p,q  and  a  there exists  x  establishing the equality. Therefore we have   \|\langle a,\cdot\rangle\|_{\ell^p}=\|a\|_{\ell^q},   and that's why it is called the dual norm. As your case, it actually should be   \|\langle a,\cdot\rangle\|_{\ell^\infty}=\|a\|_{\ell^1}.   	0.9604347348213196
1247670	"Though not an answer to the question, I would like to mention that the inequality  \|B\|_F \leq \|B\|_{\mathrm{op}}  is not correct, it should be  \|B\|_{\mathrm{op}}\leq\|B\|_{F} . A quick way to see this, is to use the equations   \|B\|^2_{\mathrm{op}}  = \lambda_{\max}(B^TB)= \sigma^2_{\max}(B)   and   \|B\|_F^2 = \operatorname{trace}(B^T B) = \sum_{i=1}^{\min\{m,n\}} \sigma^2_i (B),  where  \lambda_i  and  \sigma_i  stand for an eigenvalue and singular value respectively.    Reference:  ""Matrix norm"" on Wikipedia .  "	0.9604169726371765
562813	I'm assuming you are talking about functions on  \mathbb{R}^n .    So you are asking whether  \|f*g\|_\infty\le \|f\|_1\|g\|_1    You can already see by scaling that this can't be true. Namely, replace  f  by  f_\lambda(x)=f(\lambda x)  and  g  by  g_\lambda  ( \lambda>0 ). Also let's say that  f,g  are such that  \frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1  (just a normalization). We have    \lambda^{-n} \|f*g\|_\infty=\|f_\lambda*g_\lambda\|_\infty \le \|f_\lambda\|_1 \|g_\lambda\|_1=\lambda^{-2n} \|f\|_1 \|g\|_1    Then,  \lambda^n \le \frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1    For  \lambda>1  this is a contradiction!    But   \|f*g\|_\infty\le \|f\|_1 \|g\|_\infty  is true (simply pull out the  L^\infty  norm).    In general, there is  Young's convolution inequality :    \|f*g\|_r\le \|f\|_p \|g\|_q    if  1+\frac1r=\frac1p+\frac1q ,  1\le p,q,r\le \infty .  	0.9601579904556274
882704	You can also prove it by relating spectral radius with matrix norm. For a linear operator  A ,  \operatorname{spr}(A) \le \|A\|  where  \begin{align} \|A\| &= \underset{\|x\|=1}{\sup} \|Ax\| \\ \operatorname{spr}(A) &= \max\left\lbrace |\lambda|: \lambda \text{ is an eigenvalue of } A \right\rbrace \end{align}   The spectral radius  \operatorname{spr}(A)  and  \|A\|  are equal iff the operator  A  is  normal .    So  \|A\| < 1 \Rightarrow \operatorname{spr}(A) < 1 \Rightarrow   all of the eigenvalues of  A  have absolute value smaller than 1.    See Exercise I.2.6 of  Rajendra Bhatia's Matrix Analysis  where he calls it the  Neumann Series .   	0.9599092602729797
269094	Notice that from  Hölder's inequality , valid for  p \geq 1 ,     \|f\|_1 = \|f \cdot 1\|_1 \leq \|f\|_p \|1\|_q = \|f\|_p,   where  \frac{1}{p} + \frac{1}{q} = 1 , since  \|1\|_q = 1 .    But this implies that the identity     \begin{array}{rrl}     \mathrm{id}: &(C([0,1], \|\cdot\|_p) &\rightarrow &(C([0,1], \|\cdot\|_1)     \\                  &x &\mapsto &x   \end{array}   is continuous. And since  A = \mathrm{id}^{-1}(B(0,1)) , and  B(0,1)  is open in the  1 -norm, it follows that  A  is open.       Edit:  added observation from Pedro Tamaroff that Hölder is valid for  p \geq 1 .  	0.9598352909088135
105367	This got too long for a comment:    Using the definition of the  vecto  p  norm        \|\mathbf{x}\|_p := \bigg( \sum_{i=1}^n |x_i|^p \bigg)^{1/p},   you can combine 3.) and 4.) like the following:    Let    \left \| A \right \| _p = \max \limits _{x \ne 0} \frac{\left \| A x\right \| _p}{\left \| x\right \| _p}.    So you get back 3.) with  p=2 . In the case of  p=1  and  p=\infty , the norms can be computed as:       \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} |,    which is simply the maximum absolute column sum of the matrix.    \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} |,    which is simply the maximum absolute row sum of the matrix      Here I'm not sure, which of both you mean, but none of them is a vector as already pointed out in the comments (taken from  Matrix norm/Induced_norm , which also provides some examples, that might help with the interpretation).  	0.9598090648651123
880783	To show that  f \in L^p(\mathbb{R}^n)  is in fact a tempered distribution, we need to verify whether  f  acting on Schwatz class test functions produces a finite number, i.e. for  \phi \in \mathcal{S}(\mathbb{R}^n)   \begin{equation}   |\langle f,\phi \rangle | < +\infty \end{equation}  To see this, observe that by Schwartz inequality,    \begin{eqnarray}  |\langle f,\phi \rangle |&=& \bigg|\int f \phi \bigg|      \nonumber \\                          &\leq&  \int \big|f \phi \big|  \nonumber \\                          &\le& \bigg(\int \big|f \big|^p\bigg)^{1/p} \bigg(\int                              \big| \phi \big|^q\bigg)^{1/q} \text{ where } \frac{1}{p} + \frac{1}{q}=1\\                          &=& C_f || \phi||_{L^q} \text{ where } C_f=||f||_{L^p}\\                          &<& +\infty \text{ as } \phi \in L^q \end{eqnarray}  	0.9597285985946655
1164533	We start with  \|ax-by\|^2  since it is a non-negative number, with the hope to derive the Cauchy-Schwarz inequality from a trivial inequality. For instance, by just picking  a  and  b  as  1  we have  0 \leq \|x-y\|^2 = \|x\|^2+\|y\|^2 -2\langle x,y\rangle \tag{1}  from which it follows that   \langle x,y\rangle \leq \frac{\|x\|^2+\|y\|^2}{2}\qquad\text{(trivial inequality)}.\tag{2}  Now we may exploit a symmetry: if we pick some  \lambda>0  and replace  x  by  \lambda x  and  y  by  \frac{1}{\lambda}y  the LHS of  (2)  is unchanged. In particular   \forall \lambda>0,\qquad\langle x,y\rangle \leq \frac{\lambda^2\|x\|^2+\frac{1}{\lambda^2}\|y\|^2}{2}\qquad\text{(less trivial inequality)}.\tag{3}  Given some  \|x\|,\|y\|>0 , we may pick  \lambda>0  such that the RHS of  (3)  is minimal.  By the AM-GM inequality the RHS of  (3)  is  \geq\|x\|\|y\| , but equality is attained if  \lambda=\sqrt{\frac{\|y\|}{\|x\|}} .   \langle x,y\rangle \leq \|x\|\|y\| \tag{4}   Is the Cauchy-Schwarz inequality. By performing a backtracking  in the outlined proof we have that equality holds iff  x,y  are linearly dependent. We also have that  (4)  holds for  any  positive definite inner product  \langle \cdot,\cdot\rangle , where  \|x\|  is defined through  \|x\|^2=\langle x,x\rangle .  	0.9595434665679932
1216245	Suppose   A, B \in \mathbb{R}^{n \times n}  then we know the singular value decomposition exists for both of these. So, the following is true.     A = U \Sigma V^{T}        with   U,V, \Sigma \in \mathbb{R}^{n \times n}   then we know  \| A \|_{F} \leq   \|U \| \|\Sigma\| \|V^{T} \|    but  U, V   are orthogonal which means their norm is 1. So, we have    \|A \|_{F}  \leq \|\Sigma\|_{F}    and   \Sigma   is diagonal matrix. The norm of   \Sigma   is   \| S \|_{F} = max_{i} |\sigma_{i}|   that is it is the maximum singular value which is the top entry incidently.    So, you have some bounds. From here then      \|A+B\| \leq \| A \| +\|B\|       \sigma_{A+B} \leq \sigma_{A} + \sigma_{B}   	0.9594707489013672
13090	For  p \geq 1  the generalized mean defines a norm, because it is the  \ell^{p} -norm only up to a factor  \sqrt[p]{n} .    However, if  p \lt 1 , the generalized mean (and also the  \ell^{p} -expression) don't define a norm because the set  \|x\|_{p} \leq 1  is not convex: if  x_{i} \geq 0  and  y_{i} \geq 0  for all  i  then  \|x + y\|_{p} \geq \|x\|_{p} + \|y\|_{p} !    If  p \leq q  then  \|x\|_{q} \leq \|x\|_{p} . To see this, note that both sides of the inequality are invariant by multiplication with a positive real number, so we may take without loss of generality an  x  with  \|x\|_{p} = 1 . Then  \|x\|_{q}^{q} = \sum_{j = 1}^{n} |x_{j}|^{q} \leq \sum_{j = 1}^{n} |x_{j}|^{p} = 1  this is because for  t \leq 1  and  p \leq q  we have  t^{q} \leq  t^{p} .    I don't understand what you ask about the sequence space.  	0.9593386650085449
1116273	Your estimate is not naive in general. With  p=1 ,  q=2 , take    A=\begin{bmatrix}  1&0&\cdots&0\\  1&0&\cdots&0\\  \vdots&\vdots&\ddots&\vdots\\ 1&0&\cdots&0\\  \end{bmatrix},   It is well-known that   \|A\|_1=\max\{\|A_j\|_1:\ j\},\ \ \ \|A\|_2=\|A^*A\|_2^{1/2}=\max\sigma(A^*A)^{1/2},  where  A_j  denotes the  j^{\rm th}  column of  A , and  \sigma(B)  is the  spectrum  (i.e., the list of eigenvalues).     Thus  \|A\|_1=n , while  \|A\|_2=\|A^*A\|_2^{1/2}=\left\|\begin{bmatrix} n&0&\cdots&0\\0&0&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&0\end{bmatrix}\right\|^{1/2}_2=n^{1/2}.    So   \|A\|_1=n^{1/1-1/2}\,\|A\|_2.   	0.9591347575187683
575617	"I have question about the same proof. In the book it mentioned     ""Set  σ_{1}=\|A\|_{2} .By a compactness argument, there must be vectors  v_{1} \in C^n  and  u_{1} \in C^{m}  with  \|v_{1}\|_{2}=\|u_{1}\|_{2}=1  and  Av_{1}=σ_{1} .""  Since it was defined earlier that  \|A\|_{2}=sup_{\|v\|_{2}=1} \|Av\|_{2}  and   σ_{1}\geqslantσ_{2}\geqslant…  , as semi-axis principle, I can follow why  σ_{1}=\|A\|_{2}  but how it came to the conclusion that there is  u_{1}\in C^{m}  with  \|u_{1}\|_{2}=1  and  Av_{1}=σ_{1}u_1 .  "	0.9591052532196045
577698	Do you remember how we can prove Cauchy-Schwarz through interpolation?     Since  |xy|\leq\frac{x^2+y^2}{2}  we have:   \| f\cdot g\|_1 \leq \frac{\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2} \tag{1}   but the LHS is just the same if we replace  f  with  \lambda f  and  g  with  \frac{1}{\lambda}g , so:   \| f\cdot g\|_1 \leq \frac{\lambda^2\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2\lambda^2} \tag{2}   and by choosing  \lambda  in such a way the two terms in the RHS of  (2)  are equal, i.e.  \lambda=\sqrt{\frac{\|g\|_2}{\|f\|_2}} , we get:   \| f\cdot g\|_1 \leq \|f\|_2 \cdot \|g\|_2 \tag{3}  that is the usual Cauchy-Schwarz inequality.  If we start with the Young inequality:   |xy|\leq \frac{|x|^p}{p}+\frac{|y|^q}{q}\tag{4}   and follow exactly the same interpolation steps, we end with:   \| f\cdot g\|_1 \leq \|f\|_p\cdot \|g\|_q \tag{5}   that is the wanted Holder's inequality.  	0.9590164422988892
368028	As with most things the proof isn't tricky when you know how!    Define two norms for  x \in \mathbb{R^n}  as follows:     \|x\|_1 := \sum_{i=1}^n |x_i|    and      \|x\|_2 := (\sum_{i=1}^n |x_i|^2)^{1/2}.    For  x,y \in \mathbb{R^n}  I will define  xy:= x \cdot y  as a convenient short hand.     To prove your required inequality, it suffices (by the triangle inequality) to show that     \forall x,y \in \mathbb{R^n} \ \ \|xy\|_1 \leq \|x\|_2\|y\|_2.    To this end, note that     \forall a,b \in \mathbb{R}_{+} \quad 0 \leq \|(ax+by)^2\|_1 = a^2\|x^2\|_1 + 2ab\|xy\|_1 + b^2\|y^2\|_1.    Dividing through by  b^2 , and setting  \lambda:= a/b  results in this inequality:    \forall \lambda>0 \quad 0 \leq \lambda^2\|x^2\|_1 + 2\lambda\|xy\|_1 + b^2\|y^2\|_1.    We can conclude that there are no positive real roots of this quadratic in  \lambda . Therefore, we know that the determinant must be non-positive, i.e.     (2\cdot \|xy\|_1)^2 \leq 4\|x^2\|_1 \|y^2\|_1.    Dividing both sides by  4  and taking square roots gives:    \|xy\|_1 \leq \|x^2\|_1^{1/2} \|y^2\|_1^{1/2} = \|x\|_2\|y\|_2,      as required.   	0.9589784741401672
718457	This is an example of the  Cauchy-Schwarz inequality :    \begin{align*} \|u\|_1 &= \sum_{i = 1}^n |u_i|\cdot 1 \\ &\le \left(\sum_{i = 1}^n |u_i|\right)^{1/2} \left(\sum_{i = 1}^n 1 \right)^{1/2} \\ &= \|u\|_2 \sqrt{n} \end{align*}    To improve the result to  \sqrt{q} , use a mix of  1 's and  0 's rather than a constant sequence  1  in the second sum. I'll leave it to you to work out the details.  	0.9589348435401917
304661	Note that  \rho(A)\le\|A\|  for every matrix norm, and  \|A\|_b:=\|S^{-1}AS\|_a  is a matrix norm whenever  \|\cdot\|_a  is a matrix norm and  S  is nonsignular. By combining known matrix norms with similarity transform, one can always obtain a sufficient condition for  \rho(A)<1  in the form of  \|S^{-1}AS\|<1 .    For instance, suppose  A  satisfies the condition  \color{red}{\|\pmatrix{P&Q}\|_\infty<1} , where  \|X\|_\infty=\max_i\sum_j|x_{ij}|  is the maximum row sum norm. Then we get   \|A\|:=\left\|S^{-1}AS\right\|_\infty<1\ \text{ with }\ S=\pmatrix{I\\ &(1+\epsilon)I}.     Alternatively, if  \color{red}{Q \text{ is positive definite and } \operatorname{trace}(P^2)+2\operatorname{trace}(Q)<1} , then  A  is similar to  B=\pmatrix{P&Q^{1/2}\\ Q^{1/2}&0}  and  \|B\|_F^2<1 . Hence  \rho(A)=\rho(B)\le\|B\|_F<1 .  	0.9588611721992493
1069232	Hint for question 1:  B_1  is open in the  d_{\infty} - metric?     It has been shown  here  (the question is yours) that  \forall x\in \mathbb{R}^n  the following inequalities hold  \|x\|_{\infty}\leq \|x\|_1\leq n\|x\|_{\infty}.  Let  x_0\in B_1  and let  x  be such that  \|x-x_0\|_{\infty}< r  for some  r>0 . Then  \|x\|_1\leq \|x-x_0\|_{1}+\|x_0\|_{1}\leq n\|x-x_0\|_{\infty}+\|x_0\|_{1}<  nr+\|x_0\|_{1}.  Is there any positive value for  r  such that  nr+\|x_0\|_{1}<1 , i.e.  x\in B_1 ?     In that case the set  B_1  is open with respect to the metric  d_{\infty} .    P.S. For  B_2  the following inequalities should be useful  \|x\|_{\infty}\leq \|x\|_2\leq \sqrt{n}\|x\|_{\infty}.  	0.9588593244552612
1231410	The result is false. Consider for example  F=C([0,1])  with norms  \|\ \|_\infty  and  \|\ \|_1  and  T  the identity operator. It is easy to find a sequence  x_n(t)  such that  \|x_n\|_1\to0  and  \|x_n\|_\infty  does not converge.    Now let's look at the original the problem. We have  Mx=M^{1/2}M^{1/2}x\in\text{Im}(M^{1/2}) . \begin{align} \|Mx_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}&=\|M^{1/2}M^{1/2}x_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}\\ &=\|PM^{1/2}x_n-PM^{1/2}x\|\\ &\le\|M^{1/2}x_n-M^{1/2}x\|, \end{align} and the last expression converges to  0 .  	0.9588227272033691
851001	This boils down to the question, whether for symmetric  A    \|Ax\|_2^2\le \|A\|_\infty \cdot x^TAx   holds for all  x .    This is not true without positive definiteness of  A  (take  A=-I ).    It is true for positive definite  A :   \|Ax\|_2^2 =\|A^{1/2}A^{1/2}x\|_2^2 \le \|A^{1/2}\|_2^2 \cdot \|A^{1/2}x\|_2^2 \le  \|A^{1/2}\|_2^2\cdot x^TAx.   Since  A^{1/2}  is symmetric positive definite   \|A^{1/2}\|_2^2 = \lambda_\max(A^{1/2})^2 = \lambda_\max(A) \le \|A\|_\infty,   which is the claim. Note that we can use any matrix norm on  \mathbb R^{n,n}  instead of  \|\cdot\|_\infty .  	0.9585137963294983
199079	As many others have pointed out, the matrix norm of  I  is not necessarily equal to  1 . In fact, if  \|\cdot\|  is a matrix norm  c\|\cdot\|  is also a matrix norm for any  c\ge1 .    Yet, if your matrix norm is submultiplicative, we have  $0<\|I\|=\|I^2\|\le\|I\|^2$  and hence  $\|I\|\ge1$ , but strict inequality may still occur. E.g. the Frobenius norm  $\|\cdot\|_F$  is submultiplicative, but  $\|I_n\|_F=\sqrt{n}>1$  when  $n>1$ .    If your textbook is not erred, it is perhaps talking about an  induced  matrix norm, i.e. one defined by  $\|A\|=\sup_{x\neq0}\frac{\color{red}{\|}Ax\color{red}{\|}}{\color{red}{\|}x\color{red}{\|}}$  for some vector norm  $\color{red}{\|\cdot\|}$  defined on  K^n . In this case, it is straight from the definition that  \|I\|=1 .  	0.9584485292434692
1095961	An infinite matrix  A  satisfies a bound   \|Ax\|_p \le C\|x\|_p \tag1  if and only if it satisfies the bilinear bound  |y^*Ax| \le C\|x\|_p\|y\|_q \tag2  (with same  C  in both equations). Here    1/p+1/q=1  and  {}^*  stands for conjugate transpose. Rewriting  (2)  as  |x^*A^*y| \le C\|x\|_p\|y\|_q \tag3  we see that it is also equivalent to   \|A^*y\|_q \le C \|y\|_q\tag4  Simply put, the adjoint of a bounded operator is a bounded operator on the  dual  space. As David C. Ullrich said, there is no reason for it to be bounded on the original space.    For a concrete example with  A^*  not bounded on the same  \ell^p  space, let  p=4/3  and    A = \begin{pmatrix} 1 & 1/\sqrt{2} & 1/\sqrt{3} & 1/\sqrt{4}  & 1/\sqrt{5} & \cdots \\ 0 & 0 & 0 & 0 & 0 & \cdots \\ 0 & 0 & 0 & 0 & 0 & \cdots  \end{pmatrix}   where all rows except the first are zero. This is  a bounded rank-1 operator on  \ell^{4/3} , because the first row entries are a vector from the dual space  \ell^{4} . However, trying to apply the transpose  A^*  on  \ell^{4/3} , we see that it maps the first basis vector to  (1,  1/\sqrt{2},   1/\sqrt{3},  1/\sqrt{4}, 1/\sqrt{5},\dots)  which is not an element of  \ell^{4/3} .  	0.9584190249443054
698265	Denote  B=AA_1^{-1}=\left[\matrix{I\\A_2A_1^{-1}}\right] . A proof strategy (among several others) could be      Show that  B^*B-I  is positive-semidefinite. It would mean that the eigenvalues  \lambda_i(B^*B)\ge 1  and, hence, the singular values  \sigma_i(B)\ge 1 .   Show (e.g. via SVD) that  \|B^{+}\|_2=\frac{1}{\sigma_\min(B)}\le 1 .   Show that  B^{+}=A_1A^{+} .   Estimate   \|A^{+}\|_2=\|A_1^{-1}A_1A^{+}\|_2\le\|A_1^{-1}\|_2\|A_1A^{+}\|_2\le \|A_1^{-1}\|_2.     	0.9583341479301453
176898	Similar solution to @uncookedfalcon's answer, but with different notation would be as follows:    \begin{align} |aX+bY| &\leq |aX| + |bY|  & \text{triangle inequality} \\ |aX+bY| &\leq |a|\cdot|X| + |b|\cdot|Y|  & \text{norm homogeneity} \\ \mathbb{E}\Big[|aX+bY|\Big] &\leq \mathbb{E}\Big[|a|\cdot|X| + |b|\cdot|Y|\Big]  & \text{expected value monotonicity} \\ \mathbb{E}\Big[|aX+bY|\Big] &\leq |a|\cdot\mathbb{E}\Big[|X|\Big] + |b|\cdot\mathbb{E}\Big[|Y|\Big]  & \text{expected value linearity} \\ \end{align}    In fact this exactly the same with  \mathbb{E}[X] = \int_\Omega X\ \mathrm{d}P , but is nicer if you don't want to mention the integral symbol while  \mathbb{E}[X] = \sum_{k \in \mathrm{Cod}(k)}k\cdot P(X = k)  or  \mathbb{E}[X] = \sum_{\omega \in \Omega}X(\omega) \cdot P(\omega) . I know all this is just a difference in notation (i.e. the sum is still the integral for appropriate measure), but sometimes it matters.    I hope this helps ;-)  	0.9582816958427429
183624	As copper.hat already pointed out, the inequality  \|f\|_p \leq \mu(E)^{1-\frac{1}{p}} \cdot \|f\|_{\infty}  does not hold for all  p \geq 1 . Instead, it should read  \|f\|_p \leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty}    This follows from  \|f\|_p \leq \mu(E)^{\frac{1}{p}-\frac{1}{r}} \cdot \|f\|_r \qquad (r>p) by letting  r \to \infty , using that  \lim_{r \to \infty} \|f\|_r = \|f\|_{\infty}  (see for a  proof  here).       Edit: Actually, there is a rather quick (and direct) proof of the inequality:  \begin{align} \|f\|_p^p &= \int_E \underbrace{|f|^p}_{\stackrel{p \geq 1}{\leq} \|f\|_{\infty}^p} \, d\mu \leq \mu(E) \cdot \|f\|_{\infty}^p \\ \Rightarrow \|f\|_p &\leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty} \end{align}  	0.9581931829452515
1115939	If  natural norm  means a matrix norm induced by a vector norm, then it is generally not true.     For any induced matrix norm and a diagonal  D , we have   \|D\|=\max_{x\neq 0}\frac{\|Dx\|}{\|x\|}\geq\max_{1\leq i\leq n}\frac{\|De_i\|}{\|e_i\|}=\max_{1\leq i\leq n}|D_{ii}|   obtained by picking  x  to be columns of the identity matrix. So the maximum of  |D_{ii}|  is at least a lower bound on  \|D\| .    There are cases, where the inequality becomes an equality. This is true for any matrix  p -norm with  1\leq p\leq \infty .    In order to construct a counterexample, consider a vector norm   \|x\|_M:=\|Mx\|_\infty   with a nonsingular matrix  M . The induced matrix norm is related to the  \infty -norm by   \|D\|_M=\|MDM^{-1}\|_\infty.   For example, in  \mathbb{R}^2  and with   D=\begin{bmatrix}1&0\\0&2\end{bmatrix}, \quad M=\begin{bmatrix}2&1\\1&2\end{bmatrix},   we get   \|D\|_M=3> 2=\max_{1\leq i\leq2}|D_{ii}|.   	0.9581659436225891
366865	Note first that your definition of the operator norm is not correct. It should say that  \|T\|_{op}  is the least  c  such that  \|T\vec v\|\leq c\,\|\vec v\|  for all  \vec v .    Note also that the operator norm depends on which norms you give to  \mathbb R^n  and  \mathbb R^m . The canonical choice is to take the Euclidean norm in both, but that choice is by no means the only one.     As for those  A  such that  \|A\|_{op}\leq1 , you cannot expect any algebraic property to characterize them. Indeed, given any matrix  B , the matrix  A=B/\|B\|_{op}  satisfies  \|A\|_{op}=1 .     Of course there are some necessary conditions. If  \|A\|_{op}\leq1 , then for all  k,j   |A_{kj}|=|\langle Ae_j,e_k\rangle|\leq\|A\|_{op}\,\|e_j\|\,\|e_k\|\leq1.  More than that, if  e  is the vector all entries equal to 1, then   1\geq\|A\|_{op}\geq\,\frac1n\,|\langle Ae,e\rangle|=\frac1n\,\left|\sum_{k=1}^m\sum_{j=1}^nA_{kj}\right|.   Or, for each  j ,   1\geq\|A\|_{op}^2\geq\|Ae_j\|^2=\sum_{k=1}^m|A_{kj}|^2   	0.9580414295196533
1032772	Yes, if  \hat{u} \in L^2(\mathbb{R}) \cap L^\infty(\mathbb{R}) , one can modify  u  to satisfy the equality. I will provide justification below, but would like to note first that my answer is based off the  2 \pi  definition of the Fourier transform. That is, I define the Fourier transform to be:  \hat{u}(\omega) = \int_{\mathbb{R}} f(t) e^{-2 \pi i t \omega} \, dt    If  u^2 \in L^1(\mathbb{R}) , then  u \in L^2(\mathbb{R})  and so  \hat{u} \in L^2(\mathbb{R})  by the  L^2(\mathbb{R})  Fourier transform given by Plancherel's theorem. Now, suppose that  \hat{u}  is also in  L^\infty(\mathbb{R}) . Define a new function  u_a(t) := \sqrt{a} u(at)  where  a > 0 . Note that  \|u_a^2\|_1 = \|u^2\|_1  and that  \widehat{u_a}(\omega) = (1/\sqrt{a}) \hat{u}(\omega/a) . Thus, we have  \|\widehat{u_a}\|_\infty^2 = \frac{1}{a} \|\hat{u}\|_\infty^2 . We can then force  \|\widehat{u_a}\|_\infty^2 = \|u_a^2\|_1 = \|u^2\|_1  by letting  a = \|\hat{u}\|_\infty^2/\|u^2\|_1 , and  u_a  will satisfy the desired equality.  	0.9580102562904358
901268	"First use the embedding of  H^1  into some  L^p ,  p>4 . Then interpolate  L^p -norms (H\""older inequality), then apply the result you mentioned:   \|v\|_{L^p} \le c \|v\|_{H^1},     \|v\|_{L^4} \le \|v\|_{L^2}^\theta \|v\|_{L^p}^{1-\theta} \le  c \|v\|_{L^2}^\theta \|v\|_{H^1}^{1-\theta}   Since you are dealing with 2-dimensional domain, this is true with  \theta=1/2  (see Temam's book on Navier-Stokes). Set  v:=\nabla u . Then we get   \|\nabla u\|_{L^4} \le c \|u\|_{H^1}^\theta \|u\|_{H^2}^{1-\theta} \le c \|u\|_{L^2}^{\theta'} \|u\|_{H^2}^{1-\theta'}   where in the last step I used the interpolation inequality for Sobolev spaces you mentioned in the comment. As written in the question,  1-\theta'=3/4 .  "	0.9579982161521912
958762	The necessary and sufficient condition is that the  spectral radius  of  A  is less than  1 . Equivalently, there exists a positive integer  n  and a submultiplicative matrix norm  \|\cdot \|  such that  \|A^n\|<1 .     If you are looking for an easy-to-check sufficient condition, it's reasonable to consider the  induced norms  for either  \ell^1 - or  \ell^\infty - vector norms. These are    \|A\|_1 = \max_{j}\sum_{i}|a_{ij}|\quad\text{ and }\quad \|A\|_\infty = \max_{i}\sum_{j}|a_{ij}|   If either of these norms is less than  1 , then the inequality  \|Ax\|_p\le \|A\|_p\|x\|_p  (with  p  being  1  or  \infty ) yields convergence to zero.  	0.9579310417175293
698211	Hint:    \frac{1}{\|A^{-1}\|\|A\|}=\left|\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\right|\leq 1 \ [\text{applicable only for the normal matrices}]      \frac{1}{\|A^{-1}\|\|A\|}=\frac{\sigma_{min}(A)}{\sigma_{max}(A)}\leq 1 \ [\text{true only when, } \|\cdot\| \text{ denotes } \|\cdot\|_2]  For any matrix  A, \ \kappa(A) \geq 1 .    [Please check the comments below for another approach to solve this problem.]  	0.9578703045845032
302124	One way is to apply  Hölder's inequality  to  \sqrt{fg} , since we then find that    \begin{align*} \int_0^1 \sqrt{fg} dx &= \|\sqrt{fg}\|_1 \\ &\le \|\sqrt{f}\|_2 \|\sqrt g\|_2 \\ &= \left(\int_0^1 f(x) dx\right)^{1/2}\left(\int_0^1 g(x) dx\right)^{1/2} \end{align*}    Now since we have  \sqrt{fg} dx \ge 1  almost everywhere, monotonicity of the integral leads to  1 \le \int_0^1 \sqrt{fg} ; the final result follows by squaring.  	0.9578378796577454
439850	Define  f:[0,2\pi]\to\mathbb{C}  as:  f(\theta)=\sum_{n=1}^{+\infty}x_n e^{ni\theta}.  Then  f  is a  L_2 -function with zero mean over  (0,2\pi) , and we have:  \|f\|_1 \leq \sqrt{2\pi}\,\|f\|_2 \tag{1}  due to the Cauchy-Schwarz inequality and   \|f\|_2 \leq \|f'\|_2\tag{2}  due to  Wirtinger's inequality , or just Parseval's identity.  (1)  and  (2)  give:  \frac{1}{2\pi}\int_{0}^{2\pi}\left|\sum_{n=1}^{+\infty}x_n e^{ni\theta}\right|\,d\theta\leq\frac{1}{\sqrt{2\pi}}.\tag{3}  Probably this can be improved a little, since equality cannot hold both in  (1)  and in  (2) .  	0.9577962756156921
641652	k(A,2)=\|A\|_2\|A^{-1}\|_2= \max | \lambda_A|.\max|\lambda_{A^{-1}}|\leq\|A\|\|A^{-1}\|=k(A,\|.\|)  for each operator norm  \|.\| .    That  \max|\lambda_A|\leq \|A\|  follows from:    Let  x  be an eigenvector of  A  for the eigenvalue  \lambda , where  \max\limits_{i}|\lambda_i|=|\lambda| . Then for arbitrary matrix norm  \|.\| , subordinate to the vector norm  \|.\| , we have  \|A\|=\max\limits_{y\neq 0} \frac{\|Ay\|}{\|y\|}\ge \frac{\|Ax\|}{\|x\|}=\frac{\|\lambda x\|}{\|x\|}=|\lambda|  	0.9577630162239075
549755	First, if  P  is a  nontrivial  projection ( P\neq 0 ), any submultiplicative norm of  P  is bounded from  below  by one. This follows simply from the fact that  P  is idempotent and  \|P\|=\|P^2\|\leq\|P\|^2  which gives  \|P\|\geq 1 . Consequently, if  \|\cdot\|  is the matrix norm induced by the vector norm induced by the scalar product w.r.t. which  P  is orthogonal, then either  \|P\|=0  or  \|P\|=1  with the first option possible if and only if  P=0 .    The only way how to make a norm of a nontrivial projection smaller than one is to use a matrix norm which is not submultiplicative. This excludes, e.g.,  p -norms, Frobenius norm, and any matrix norm induced by a vector norm. On the other hand, some matrix norms are not submultiplicative; e.g., the  \max -norm  \|P\|_\max:=\max\limits_{i,j}|p_{ij}| .    E.g., with (stealing the  P  from Omnomnomnom)   P=\frac{1}{5}\pmatrix{1&2\\2&4},     \|P\|_2=1, \quad \|P\|_1=\|P\|_\infty=\frac{6}{5}>1, \quad \|P\|_\max=\frac{4}{5}<1.   	0.9577411413192749
983643	True for the edited version.    One can assume that  \mu  is a unitary measure since one can divide by  \mu(\Omega)  and obtain a new measure where all norms will be multiplied by  \dfrac{1}{\mu(\Omega)}.  So, I shall assume  \mu  is unitary.    In this case, a direct application of Holder inequality allows concluding at once  \|f\|_1 \leq \|f\|_2 \|1\|_2 = \|f\|_2;  by translation invariance of the norm, convergence in  \mathscr{L}^2  implies convergence in  \mathscr{L}^1.  (Plus, if you divide a function  f  in  \mathscr{L}^2  as  |f| = |f|\mathbf{1}_{\{|f|\leq 1\}}+|f|\mathbf{1}_{\{|f|>1\}} \leq |f|^2+1,  once gets that  \mathscr{L}^2 \subset \mathscr{L}^1 ).  	0.9577218890190125
764213	I'm afraid your intition is leading you in a wrong direction. Consider  \mathbb R^2  equipped with the  1- norm  \left\|\begin{pmatrix}x_1\\x_2\end{pmatrix}\right\|_1=|x_1|+|x_2| .     Let  n\in \mathbb N  and  a=\begin{pmatrix}n\\0\end{pmatrix} ,  b=\begin{pmatrix}0\\n\end{pmatrix} ,  c=\begin{pmatrix}n\\n\end{pmatrix} . Then  \|a\|_1=\|b\|_1=\frac{1}{2}\|c\|_1 = n,  but  \|a-b\|_1 = 2n,  i.e. there is no upper bound.    I'm not sure for which norms you can generalize this kind of argument.  	0.9576398730278015
827463	The standard  Holder's inequality  (for points in  \mathbb{R}^n ) can be written as     \|fg\|_p \leq \|f\|_q \|g\|_r     where  p^{-1} = q^{-1} + r^{-1} ; note that this requires  q, r \geq p .     The inequality you wrote above follows by taking  r^{-1} = p^{-1} - q^{-1}  and  g = \mathbf{1} .        The version above can be proved by taking  \tilde{f} = |f|^p  and  \tilde{g} = |g|^p  and applying the standard version quoted in Wikipedia.        In arbitrary measure spaces, what you have is the estimate     \|x\|_p \leq |\mathrm{supp}(x)|^{\frac1p -\frac1q} \|x\|_q     for every  q \geq p , provided the support of the function  x  has finite measure. This is sometimes useful in probability theory.   	0.957619309425354
150534	The inequality is true. It is obvious when  A  is singular. When  A  is invertible, for any unit vector  x , we have  \|x^TA\|\ge\sigma_\min(A) . Therefore \begin{align} \|AB\| &= \max_{\|x\|=1} \|x^TAB\|\\ &= \max_{\|x\|=1} \|x^TA\|\left\|\frac{x^TA}{\|x^TA\|}B\right\|\tag{1}\\ &\ge \max_{\|x\|=1} \sigma_\min(A)\left\|\frac{x^TA}{\|x^TA\|}B\right\|\\ &= \max_{\|y\|=1} \sigma_\min(A)\left\|y^TB\right\|\tag{2}\\ &= \sigma_\min(A)\|B\|, \end{align} where we have used the assumption that  A  is invertible in  (1)  (so that we can divide by  \|x^TA\|\neq0 ) and  (2)  (so that every  x  corresponds to a unique  y  and vice versa).  	0.9574840068817139
967419	I think it is best to reserve  \|A\|_2  for the usual  2 -norm  \displaystyle \|A\|_2 = \sup_{\|x\|_2 = 1} \|Ax\|_2 .    You can use the singular value decomposition: there exist unitary matrices  U  and  V  and a matrix  S  with singular values of  A  along the diagonal and zeros elsewhere satisfying  A = USV.    Suppose that  x  is a vector with  \|x\|_2 = 1 . Then you have  \|Ax\|_2 = \|USVx\|_2 = \|SVx\|_2 \le \|S\|_2 \|Vx\|_2 = \|S\|_2.  Because of its particular form, the  2 -norm of  S  is its largest entry. Since its nonzero entries are the singular values of  A , this means  \|S\|_2 = \sigma_1(A)  so that  \|A\|_2 \le \sigma_1(A) .    The fact that  \|A\|_\infty \le \sqrt n \|A\|_2  is easy to prove, giving you   \frac 1 {\sqrt n} \|A\|_\infty \le \sigma_1(A).  	0.9574557542800903
221753	Since  \|f(x)\| = \|Ax\| \leq \|A\| \|x\|,  by the definition of the operator norm:   \|A\| = \sup_{x\in \mathbb{R}^3}\frac{\|Ax\|}{\|x\|}.   In case of Euclidean norm and  A: \mathbb{R}^3\to  \mathbb{R}^3  is a linear transformation:    \|A\|  = \sqrt{\lambda_{\mathrm{max}}(A^T A)},   where  \lambda_{\mathrm{max}}(A^T A)  is the largest eigenvalue of  A^TA , and you wanna check this is less than  1 .    In your case:    A^T A = \begin{pmatrix}5/16 & 0 & 0 \\ 0 & 1/9 & 0\\ 0 & 0 & 5/16 \end{pmatrix},   which gives the  \lambda_{\mathrm{max}} = 5/16 , and  \|A\| = \sqrt{5}/4 < 1 , thus    \|f(x)\| = \|Ax\| \leq \|A\| \|x\| < \frac{\sqrt{5}}{4}\|x\|   and  f  is a contraction.  	0.9573177695274353
43157	Well, actually it is well known that       \|A\| = \|A\|_{1\rightarrow\infty} = \max_{x\ne 0}\frac{\|Ax\|_{\infty}}{\|x\|_1}\,.   Thus, in particular, it is consistent with the corresponding norms:       \|Ax\|_{\infty} \le \|A\| \|x\|_1\,.   	0.9571772217750549
646888	Let  \left \| \cdot  \right \| _{a}  and  \left \| \cdot  \right \| _{b}  be norms on  V .     The key idea is that if  \dim V<\infty   then the unit ball in the topology induced by both norms is compact.     This means in particular that there is an  M\geq 0  such that  \left \| x \right \|_{a}\leq M  for all  x\in V  such that  \left \| x \right \|_{b}\leq1 .     But then  \left \| \frac{x}{\left \| x \right \|_{b}} \right \|_{a}\leq M  for all  x\in V , or what is the same thing  \left \| x \right \|_{a}\leq M\left \| x \right \|_{b} . Reversing the roles of the norms, we get  \left \| x \right \|_{b}\leq M'\left \| x \right \|_{1}  and the result follows.     Note: one does need to show that  \left \| \cdot \right \|_{a}:V\to \mathbb R  is continuous on  V  in the topology induced by  \left \| \cdot \right \|_{b}  but this is fairly routine.   	0.9571467041969299
121759	In the space of positive semi-definite matrices, trace is a proper inner-product (it is easy to show that), i.e. it obeys the Cauchy-Schwarz inequality:  \langle x,y \rangle \leq \sqrt{ \langle x,x \rangle \langle y,y\rangle} . So    \mbox{tr}\{AB\}\leq \sqrt{\mbox{tr}\{A^2\} \mbox{tr}\{B^2\}}    Now, since  A  is positive semidefinite,  \mbox{tr}\{A^2\} \leq \mbox{tr}\{A\}^2 , i.e., the eigenvalues of  A^2  are squared eigenvalues of  A , and since they are positive    \mbox{tr} \{A^2\} = \sum_{i=1}^{N}\lambda_{i}^{2}\leq \left( \sum_{i=1}^{N}\lambda_{i} \right)^{2} = \mbox{tr}\{A\}^2 \leq 1      A similar argument for B proves  \mbox{tr}\{B^2\}\leq 1  . So  \mbox{tr}\{AB\}\leq 1 . Hope this answers your question.   	0.9571452736854553
571530	The operator norm is defined (equivalently) as any of :   \|A\| = \max_{\|x\|=1}\|Ax\|   \|A\| = \max_{\|x\|\leq1}\|Ax\|   \|A\| = \max_{\|x\|\neq0}\frac{\|Ax\|}{\|x\|}    Above, you test against  k = (0,1,1)  (I'm not sure what you mean by  e_1k_2+e_2k_3)  - you're multiplying vectors here, which makes little sense in context, and none at all without specifying if you are using the cross or dot product), which has (Euclidian) norm  \sqrt{2} , so if you wish to use it to test your operator norm, you need to use the third formula above, and  \frac{\|Ak\|_2}{\|k\|_2} = \frac{\|(1,1,0)\|_2}{\|(0,1,1)\|_2} = \frac{\sqrt{2}}{\sqrt{2}} = 1 , so you have a witness that  \|A\| \geq 1 . In order to prove that  \|A\| = 1 , you now need to show that  \|Ax\|_2 \leq \|x\|_2  for all  x \in\mathbb{R}^3  (or possibly faster, using the first formula,  \|Ax\|_2 \leq 1  for all  x \in \mathbb{R}^3  with  \|x\|_2 = 1 .    (NB: Throughout (except in the definition of the operator norm, which applies on any normed linear space) I've made the use of the Euclidian norm explicit by denoting it  \|\cdot\|_2  - this is not actually important).  	0.9571279883384705
1047283	Since  \|A_1^{-1}\|_2=\|(A_1^\ast)^{-1}\|_2 , it suffices to prove that for every nonzero vector  x\in\mathbb C^m , there exists some nonzero vector  u\in\mathbb C^n  such that   \|A^+x\|_2/\|x\|_2\le \|(A_1^\ast)^{-1}u\|_2/\|u\|_2.\tag{1}    (1)  is evident when  A^+x=0 . So, suppose  A^+x\ne0 . Pick  u=A^+x . Then it suffices to show that   \|u\|_2^2\le \|x\|_2\|(A_1^\ast)^{-1}u\|_2.\tag{2}   Since  AA^+  is an orthogonal projection,  \|x\|_2\ge\|AA^+x\|_2=\|Au\|_2\ge\|A_1u\|_2 . Hence it suffices to prove that   \|u\|_2^2\le \|A_1u\|_2\|(A_1^\ast)^{-1}u\|_2,\tag{3}   but this is just Cauchy-Schwarz inequality.  	0.9571136236190796
727404	The desired inequality is false, even with constants, since it does not have the right sort of homogeneity. Take a function  f  for which the inequality does hold, and define    g_{\lambda}(x) = \lambda^{d} f(\lambda x)    This is the  L^1  dilation of  f , and  \|g_{\lambda} \|_1 = \|f\|_1  for all  \lambda > 0 . On the other hand,    \begin{align*} \big| |x|^{2d} g_{\lambda}(x) \big| = \lambda^{-d} \big| |\lambda x|^{2d} f(\lambda x)\big| = \lambda^{-d} |y|^{2d} |f(y)| \end{align*} It follows that   \big\| |.|^{2d} g_{\lambda} \big\| = \lambda^{-d} \big\||.|^{2d} f\big\|  But then we have  \|g_{\lambda}\|_1 = \|f\|_1 \le \big\||.|^{2d} f\big\|_{\infty} = \lambda^d \big\||.|^{2d} g_{\lambda}\big\|_{\infty}  Now send  \lambda  to zero to find a counterexample.  	0.9570361971855164
336962	The reverse inequality follows from the fact that  \|B\|_2 = \sigma .    Let  u  and  v  be unit vectors. Then \begin{align*} | u^* B v | &\leq \|u \|_2 \|B v\|_2 \qquad \text{(by Cauchy-Schwarz inequality)}\\ &\leq \|u\|_2 \|B\|_2 \|v\|_2 \\ &= \sigma. \end{align*}    It follows that  \begin{equation} \max_{\|u\|_2=1,\|v\|_2=1} | u^* B v| \leq \sigma. \end{equation}  	0.9569337964057922
759127	I think it is worth mentioning a proof using the Fourier analytic definition of  H^s , if only for its succinctness.    We have   \| u \|_{L^\infty} \leq \| \hat{u} \|_1 \leq \| \langle \xi \rangle^{-s} \|_2 \| \langle \xi \rangle^s \hat{u} \|_2 \leq C \| u \|_{H^s}.  Here  \langle \xi \rangle = \sqrt{1 + \lvert \xi \rvert^2} . Interpolation with  L^2  implies   \| u \|_p \leq C(s) \| u \|_{H^s} \quad \forall 2 \leq p \leq \infty.   	0.9568707346916199
212358	We can (for example) consider two cases:    Case 1 -  q> p    We have that  \|u\|_p^p=\int_I |u|^p . Because  q>p , we have that  \frac{q}{p}>1 , hence we can apply Holder inequality, to conclude that  \|u\|_p^p\leq C \|u\|_q^{p} , where  C>0  is a positive constant (that can change in every line). From it we have that  \|u\|_{1,p}=\|u\|_p+\|u'\|_p\leq C\|u\|_q+\|u'\|_p\leq C\||u\||    Case 2-  q\leq p    In this case  W^{1,p}(I)  is embedded in  W^{1,q}(I) , hence  \|u\|_p=\Big(\int_I|u|^p\Big)^{1/p}\leq C\|u\|_\infty\leq C\|u\|_{1,q} . From here you can conclude.  	0.956832766532898
581003	You need to cut the study in two :    On  A=[-1,1] , you have  \|f\|_{1,A}^2 \leq 2\|f\|_{2,A}^2 \leq 2\|f\|_2^2  (Jensen inequality)    On  A^c=[-1,1]^c , you have     \|f\|_{1,A^c} = \left\| \frac{g}{x} \right\|_{1,A^c} \leq \|g\|_{2,A^c}\|\frac{1}{x}\|_{2,A^c} \leq \sqrt{2} \|g\|_2    Hence     \|f\|_1 \leq \sqrt{2}(\|f\|_2+\|g\|_2)    	0.95682692527771
1223990	If you read  Boyd  in chapter six there is regularization and least squares problems. Regularization follows the following problem like this.     \textrm{ minimize w.r.t }R_{+}^{2} (\| Ax -b\|,\|x \|)     this is called the bi-criterion problem which is a convex optimization problem.    Regularization has a general pattern which looks like this    \textrm{ minimize}  \| Ax -b\| +  \gamma \|x \|     Where we have a parameter   \gamma \in (0,\infty)    which is our regularization parameter. In the case of  \ell_{2}  regularization we have      \textrm{ minimize}  \| Ax -b\|_{2} +  \delta \|x \|_{2}     where our 2-norm  here  \|x \|_{2} = \left( \sum_{i=1}^{m} |x_{i} |^{2} \right)^{\frac{1}{2}}    The superscript simply means     \| x \|_{2}^{2} =  \sum_{i=1}^{m} |x_{i} |^{2}   	0.9568090438842773
815349	Let  A  denote the transformation defined by  Ae_{j}=v_{j}  for  j=1,\ldots,n . First, I claim that  \|A\|=\|A\|_{op}\leq \sqrt{n} . Indeed, for  x=\sum_{j}\lambda_{j}e_{j} , we have    \|Ax\|\leq\sum_{j}|\lambda_{j}|\|Ae_{j}\|=\sum_{j}|\lambda_{j}|\leq\sqrt{n}(\sum_{j}|\lambda_{j}|^{2})^{1/2}    where we use the hypothesis that the  v_{j}  are unit vectors and convexity.    By the SVD, w.l.o.g. we may assume that the matrix of  A  relative to the standard basis is diagonal with entries  \sigma_{1},\ldots,\sigma_{n} . Since  \|n^{-1/2}A\|\leq 1 , we have that  |n^{-1/2}\sigma_{j}|\leq 1  for  j=1,\ldots,n . Whence,    \frac{\theta}{n^{n/2}}=\frac{|\sigma_{1}\cdots\sigma_{n}|}{n^{n/2}}\leq\frac{|\sigma_{j}|}{n^{1/2}}  for any  j=1,\ldots,n . Therefore  |\sigma_{j}|\geq n^{\frac{n-1}{2}}\theta , from which it follows that  \|T\|=\|A^{-1}\|\lesssim_{n}\theta^{-1} .  	0.9567233920097351
1168432	About Hölder and your upper bound, OK:   \|Tx\|_1 = \sum_{k=1}^\infty\left|\frac{x_k}{2^k}\right|\le \left(\sum_{k=1}^\infty 2^{-2k}\right)^{1/2} \left(\sum_{k=1}^\infty x_k^2\right)^{1/2} = \frac1{\sqrt 3}\|x\|_2.   About finding  x  with  \|Tx\|_1\approx\frac1{\sqrt 3}\|x\|_2  (equality  can  be impossible), see the conditions for equality in the Hölder inequality.  	0.956555962562561
727344	A possible solution steps:      Prove that    \int_0^1\frac{1}{|x-t|^{1/2}}\,dt=2\sqrt{x}+2\sqrt{1-x}\le 2\sqrt{2}.    Prove that  \|g\|_\infty\le 2\sqrt{2}\|f\|_\infty  (simple estimation by 1).   Prove that  \|g\|_1\le 2\sqrt{2}\|f\|_1  (using e.g.  Tonelli's theorem  and 1).   Conclude that  \|g\|_2\le 2\sqrt{2}\|f\|_2  by the  Riesz-Thorin theorem .    	0.956528902053833
550172	No. Chose the norm  \|\cdot\|_\text{me}  defined by  \|x\|_\text{me} :=\frac12 \|x\|_\infty . You can easily check that this is in fact a norm, but  \|2\cdot e_1\|_\text{me} = 1 .    There is however the following theorem (a special case of the fact that all norms on  \mathbb R^n  are equivalent):        For any norm  \|\cdot\|  on  \mathbb R^n  there are constants  c  and  C  depending on the norm such that    \|x\| < 1 \Rightarrow \|x\|_\infty < C\\ \|x\|_\infty < c \Rightarrow \|x\| < 1    Or equivalently    \frac1C\|x\|_\infty \le \|x\| \le \frac1c\|x\|_\infty    	0.9564502239227295
160133	Presumably  A  is a real  n\times n  matrix with all its eigenvalues being real. The inequality is false in general (see chaohuang's answer for a counterexample), but it is true in each of the following circumstances (exercises):      A  is real symmetric (in this case,  A  is guaranteed to have only real eigenvalues),   A  is a  doubly stochastic matrix  (hint:  Perron-Frobenius theorem ),   the norm of every column of  A  does not exceed  \frac1{\sqrt{n}}  (hint: for any unit vector  x , we have  \|Ax\|\le\sum_i|x_j|\|a_{\ast j}\|\le\|x\|\ \left\|\left(\|a_{\ast 1}\|,\ldots,\|a_{\ast n}\|\right)\right\|  by Cauchy-Schwarz inequalty),   the norm of every row of  A  does not exceed  \frac1{\sqrt{n}} ,   \|A\|_1\|A\|_\infty\le1 , where  \|A\|_1  and  \|A\|_\infty  are respectively the maximum absolute column sum norm and the maximum absolute row sum norm of  A  (hint:  \rho(A^TA)\le\|A^TA\|_1\le\|A^T\|_1\|A\|_1 ).    	0.9563927054405212
163785	It is not true in general that  \rho(AB)\leq \rho(A)\rho(B) . Consider:   A=\left( \matrix{1&0\\ 1& 1}\right)\quad B=\left( \matrix{1&1\\ 0& 1}\right)   Then  \rho(A)=\rho(B)=1 . But   AB=\left( \matrix{1&1\\ 1& 2}\right)   has  \rho(AB)=(3+\sqrt{5})/2 .    If  A  and  B  commute, we have   \|(AB)^n\|=\|A^nB^n\|\leq \|A^n\|\|B^n\|   hence   \|(AB)^n\|^{1/n}\leq \|A^n\|^{1/n}\|B^n\|^{1/n}.     Letting  n  tend to  +\infty , we find the desired inequality thanks to the Spectral Radius Formula (or Gelfand's formula):  http://en.wikipedia.org/wiki/Spectral_radius   \rho(C)=\lim_{n\rightarrow +\infty}\|C^n\|^{1/n}.  	0.9563659429550171
842078	Let  \mathrm A \in [0,1)^{n \times n}  be a symmetric, positive definite, nonnegative matrix  whose diagonal entries are equal to  1 . Using the  Gershgorin circle theorem , the minimum eigenvalue of  \mathrm A  is bounded by    \lambda_{\min} (\mathrm A) \geq 1 - \max_{1 \leq i \leq n} \, \sum_{j \neq i} a_{ij} = 1 - \| \mathrm A - \mathrm I_n\|_{\infty}    Unfortunately, unless  a_{ij} \ll 1 , this bound is likely too loose. I suspect that this bound is only potentially useful for small perturbations of the identity matrix.    If the bound is not too loose, then, using the inequality  \| \cdot \|_{\infty} \leq \sqrt n \, \| \cdot \|_{F} , we obtain    \lambda_{\min} (\mathrm A) \geq 1 - \| \mathrm A - \mathrm I_n \|_{\infty} \geq 1 - \sqrt n \, \| \mathrm A - \mathrm I_n \|_{F}  	0.9561598896980286
784172	No, it is not true. It can be confusing because they are usually indicated with the same symbol.    One is the Frobenius norm   \|A\|_F = \left(\sum_{i,j} |a_{ij}|^2 \right)^{1/2} = \sqrt{\mathrm{Tr}(A^\dagger A)} = \left(\sum_i \sigma_i^2\right)^{1/2},   where  \sigma_i^2  are the eigenvalues of the positive matrix  A^\dagger A .    The Frobenius norm is the Euclidean norm of the vector of the singular values  \sigma_i  of  A .    The other is the induced norm   \|A\|_2 = \mathrm{sup}_{x\neq 0} \frac{|A x|_2}{|x|_2} = \mathrm{max}_i \sigma_i,   the largest singular value of  A .     \|A\|_F = |\boldsymbol{\sigma}|_2 \leq |\boldsymbol{\sigma}|_\infty = \|A\|_2  	0.9561312198638916
993075	Let's denote  H=\partial^2 f  and  v=x-a . Your question why (I assume you are talking about the spectral norm)  \|H\|\le\lambda_{\max}  is true can be answered as, in fact,   \|H\|=\lambda_{\max}   for  Hermitian matrices . It can be proved via unitary diagonalisation of Hermitian matrix   H=UDU^*   where  D=\text{diag}\{\lambda_1,\lambda_2,\ldots,\lambda_n\}  is the diagonal matrix of eigenvalues and  U  is unitary, because  \|H\|=\|D\|=\lambda_{\max}  (the spectral norm is unitary invariant and the norm of the diagonal matrix is an easy exercise).    It makes the answer to your second question trivial as  \|H\|=\lambda_{\max}\ge\lambda_{\min} . What is more interesting is to take a bit better estimations for   \langle Hv,v\rangle.    Using the property of  Rayleigh quotient  it is easy to conclude that   \lambda_{\min}\|v\|^2\le\langle Hv,v\rangle\le\lambda_{\max}\|v\|^2,   which fits your needs better.  	0.956122100353241
904864	Hint:  Show that  \lambda_{min}(A) \|x\|_2 \leq \|x\|_A \leq \lambda_{max}(A) \|x\|_2 .  It is helpful to start with the case in which  A  is diagonal.       For the new version of the question: define  \|x\|_* = \|Ax\| .  Notably, we have    \|x\|_*^2 = \langle Ax, Ax \rangle= \langle x,A^2 x \rangle\\ \|x\|_A^2 = \langle A^{1/2}x, A^{1/2}x \rangle= \langle x,A x \rangle   With that in mind: if we make the substitution  y = A^{1/2}x , then it suffices to show that for all  y , we have   \lambda_{min}(A^{1/2})\|x\|_A = \lambda_{min}(A^{1/2}) \|y\|_2 \leq\\ \|y\|_{A^{1/2}} = \|x\|_* = \|Ax\| \leq\\ \lambda_{max}(A^{1/2}) \|y\|_2 = \lambda_{max}(A^{1/2})\|x\|_A   	0.9560655951499939
612600	Hint:  note the following:      \textrm{Cond}_2(A) = \|A\| \|A^{-1}\|   Because  A  is positive semidefinite,  \|A\| = \max_{\|x\| = 1} x^TAx   Because  A  is positive semidefinite,  \|A^{-1}\| = \max_{\|x\| = 1} \frac{1}{x^TAx}      For an lower bound of each of these maxima, plug in the  j th standard basis vector for  x .    Let  L_j  denote the  j th row of  L .  We have   e_j^T(LDL^T)e_j = d_{jj}\cdot \|L_j\|^2 \geq d_{jj}   	0.9560604095458984
871677	I take it that you want to confirm that the operator norm is indeed a norm, and in particular that the triangle inequality is satisfied. Following the approach of Kreyzwig, let's show that  \|T\|=\underbrace{\sup \|Tx\|}_{\|x\|=1, x \neq 0}.    Well, this is not so bad, let  \|x\|=a , and then let  y=\frac{1}{a}x  for  x \neq 0 . Then  \|y\|=1 , and by linearity, we obtain:    \|T\|=\sup\frac{1}{a}\|Tx\|=\sup\|T(\frac{1}{a}x)\|=\sup\|Ty\|  where  \|y\|=1 . From this point of view, the result should seem a little more obvious, since we are taking a supremum over the values of functions applied to vectors of unit length. We get that    \sup_{\|x\|=1}\|(T_1+T_2)x\|=\sup_{\|x\|=1}\|T_1x+T_2x\| \leq \sup_{\|x\|=1}\|T_1x\|+\sup_{\|x\|=1} \|+T_2x\|.    The last inequality follows from basic properties of the supremum.  	0.9560231566429138
538214	Hint:  it suffices to show that the lowest eigenvalue of  A - B  is positive.  Note that for a symmetric matrix  M , the lowest eigenvalue can be expressed as   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx   and that   \|M\| = \max_{\|x\| = 1} \left|x^TMx \right|        I've combined several ideas to get that last equality.  From the usual results regarding the  Rayleigh quotient , we have   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx \\ \lambda_{max}(M) = \max_{\|x\| = 1} x^TMx   For arbitrary matrices, we have   \|A\| = \max_{\|x\| = 1}\|Ax\|= \sqrt{\max_{\|x\| = 1} x^TA^TAx}   So, for symmetric  M , we have   \|M\| = \sqrt{\max_{\|x\| = 1} x^TM^2x} = \sqrt{\lambda_{max}(M^2)}   Note, however, that the eigenvalues of  M^2  are simply  \lambda^2  for all eigenvalues  \lambda  of  M .  Thus, we have   \|M\| = \sqrt{\lambda_{max}(M^2)} = \max\{|\lambda|: \lambda \text{ is an eigenvalue of } M\} = \max_{\|x\| = 1} \left|x^TMx \right|   	0.9558638334274292
576080	First, note that   \left\|\,f\vphantom{\hat{f}}\,\right\|_{A(\mathbb{T})}=\left\|\,\hat{f}\,\right\|_{\ell^1}   Suppose  g(x)=1 . Then  \hat{g}(n)=[n=0] , and therefore,  \|g\|_{A(\mathbb{T})}=1 . Then, your question asks if   \left\|\,f\,\right\|_{A(\mathbb{T})} \le C\left\|\,f\,\right\|_{L^2}   which is impossible. For counterexample, we can look at   \widehat{f_n}(k)=\left\{\begin{array}{cl} \frac1{|k|}&\text{if }1\le|k|\le n\\ 0&\text{otherwise} \end{array}\right.   where  \|f_n\|_{A(\mathbb{T})}=2H_n , yet  \|f_n\|_{L^2}\le\frac{\pi^2}3 , where  H_n  is a  Harmonic number .  	0.9558252096176147
377420	For the unit circle  U=\{x\in\mathbb{R}^2\,\colon\, |x|<1\} , a routine example is something like that   f(x)= \begin{cases} \ln{\ln{\frac{1}{|x|}}},\quad |x| 0,\tag{1}   and notice that  (1)  yields   \lim_{|x|\to 0}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)=0\quad \forall\,\mu>0.\tag{2}   But  (2)  implies that   C_{\mu}\overset{\rm def}=\sup_{x\in U}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)<\infty\quad\forall\, \mu>0,   whence follows inequality    \int_{U}|f(x)|^2dx\leqslant C^2_{\mu}\int_{U}\frac{dx}{|x|^{2\mu}}= \frac{\pi}{1-\mu} C^2_{\mu}   with some  \mu\in (0,1) , while   \int_{U}|\nabla f(x)|^2 dx=\int\limits_{|x|	0.955813467502594
515698	Note that trivially  \|g\|_\infty \le \|g\|_\infty + \underbrace{\|g'\|_\infty}_{\ge 0}  You must now see if there is some constant  C  such that  \|g\|_\infty + \|g'\|_\infty \le C\|g\|_\infty    Note that on  C[0,1] ,  g'  need not even exist, let alone be continuous. This leads to a very simple counter-example:  g(x) = \sqrt x  with  \|g'\|_\infty = \|\frac1{2\sqrt \cdot}\|_\infty = \infty       EDITED QUESTION  For  C^1[0,1]  (i.e.  g'  is continuous as well) we can see that The spline functions defined by  g_n(x) = \cases{0 & $x<\frac12 - \frac1n$ \\ \ast & $x\in \frac12 + [-\frac1n, \frac1n]$\\ 1 & $x > \frac12 + \frac1n$}  Where  \ast  is the  C^1  (degree  2 ) spline interpolation are all  C^1  and  \|g_n\|_\infty = 1  but  \|g_n'\|_\infty \ge \frac n2 \qquad \text{by MVT}  So there cannot be a bounding constant on  C^1  either.  	0.9557976722717285
1205098	Following Jalex Stark's hint, let \begin{align*} v \doteq (1/1,1/2,1/3,\dots), \end{align*} so that  v \notin \ell^1  and  v \in \ell^2 . Now define the bounded operator  A : \ell^2 \to \ell^2  as, \begin{align*} Au \doteq \langle v,u \rangle v. \end{align*} Indeed,  A  is bounded, since by Cauchy Schwartz, \begin{align*} \|A\| = \sup_{u \neq 0}\frac{\|Au\|}{\|u\|} = \sup_{u \neq 0}\frac{|\langle v,u\rangle|\|v\|}{\|u\|} \leq \sup_{u \neq 0} \|v\|^2 = \|v\|^2. \end{align*} Now take  u = (1/1^2,1/2^2,1/3^2,\dots) \in \ell^1 , and we will have \begin{align*} Au = \langle v,u \rangle v = \left(\sum_{n=1}^{\infty}1/n^3 \right)v \notin \ell^1. \end{align*}  	0.9557787179946899
298231	Let  \| v \|_{p}  denotes the  p -norm of  v \in \Bbb{R}^{n} . In particular,     \| v \|_{2} = [ v_{1}^{2} + \cdots + v_{n}^{2} ]^{1/2} \quad \text{and} \quad \| v \|_{\infty} = \max \{ |v_{1}|, \cdots, |v_{n}| \}.     Then the following map     F : D^{n} \to [-1, 1]^{n} : v \mapsto \frac{\|v\|_{2}}{\|v\|_{\infty}} v     gives the homeomorphism with the inverse     G : [-1, 1]^{n} \to D^{n} : w \mapsto \frac{\|w\|_{\infty}}{\|w\|_{2}} w.     (Of course, we set  F(0) = 0 = G(0) .)    The idea is simple:     D^{n} = \{ \| v \|_{2} \leq 1 \} \quad \text{and}  \quad [-1, 1]^{n} = \{ \| v\|_{\infty} \leq 1 \}.     So the map  F  rescales the vector so that  \| F(v) \|_{\infty} = \| v \|_{2} . Checking that both  F  and  G  are indeed continuous is not theoretically hard, though it may be somewhat cumbersome.    The following graph may help you what is actually going on in  n = 3 . The sphere (above) is mapped into the cube (below) by the mapping  F . (The seams between the faces of the cube are software artifacts.)     	0.9557642936706543
742389	Let  \|A\|  denote the spectral norm of  A  (which gives the largest eigenvalue for positive definite matrices).  We have   \|B^{1/2}AB^{1/2}\| \leq \|B^{1/2}\| \cdot \|A\| \cdot \|B^{1/2}\| = \|A\|\|B\|   that is, it is at most the product of the largest eigenvalues of  A  and  B .  	0.9557396173477173
1031466	Setting  p,q,r = \infty ,  n=1 ,  j=2 ,  m=3  in the  Gagliardo-Nirenberg interpolation inequality  yields  \sup |f''| \le C \sup |f'''|^{2/3} \sup |f|^{1/3}.    You can establish this pretty easily starting from the easier case  \|f'\| \le c \|f''\|^{1/2} \|f\|^{1/2},    where I'm using  \| \cdot \|  for the sup norm. This is a quantification of the following fact: if the velocity is large at some time, then the only way to make the displacement stay small is to impose a large acceleration. This holds without any condition on the domain (other than one-dimensionality).    Applying this twice (to the functions  f'  and  f ) yields  \|f''\| \le c \|f'''\|^{1/2} \|f'\|^{1/2}\le c \|f'''\|^{1/2} (c \|f''\|^{1/2} \|f\|^{1/2})^{1/2}= c^{3/2} \|f''\|^{1/4} \|f'''\|^{1/2}\|f\|^{1/4}.    Now just divide both sides by  \| f''\|^{1/4}  to get  \|f''\|^{3/4} \le c^{3/2} \|f'''\|^{2/4} \|f\|^{1/4},    which is the G-N inequality I stated with  C = c^2 .  	0.9557133913040161
142948	Consider the equation   \sum_{i=1}^n\frac12x_i\frac{\mathrm{d}}{\mathrm{d}x_i}|f|^2=\mathrm{Re}\left(\nabla f\cdot\overline{xf}\right)\tag{1}   Integrating  (1)  over  \mathbb{R}^n  and then integrating by parts on the left side:   \begin{align} \frac n2\|f\|_2^2 &=\mathrm{Re}\left(\int_{\mathbb{R}^n}\nabla f\cdot\overline{xf}\,\mathrm{d}x\right)\\ &\le\left|\int_{\mathbb{R}^n}\nabla f\cdot\overline{xf}\,\mathrm{d}x\right|\\[6pt] &\le\|\nabla f\|_2\|xf\|_2\\[9pt] &=2\pi\|\xi\hat{f}\|_2\|xf\|_2\tag{2} \end{align}   Thus,   \|\xi\hat{f}\|_2\|xf\|_2\ge\frac{n}{4\pi}\|\hat{f}\|_2\|f\|_2\tag{3}   The last inequality says that the  L^2  support radius for  f  and  \hat{f}  cannot have a product less than  \frac{n}{4\pi} .  This inequality is sharp as can be seen using the function  f(x) = e^{-\pi x\cdot x} , whose Fourier Transform is itself, and whose  L^2  support radius is  \sqrt{\frac{n}{4\pi}} .  	0.9556559920310974
142406	Use the following two facts/properties:      The Frobenius norm and the 2-norm coincide for vectors:  \|u\|_2 = \|u\|_{F} .   The Frobenius norm is submultiplicative:  \|AB\|_{F} \leq \|A\|_{F}\|B\|_{F}  for any compatible matrices  A , B  (in particular when,  B  is a vector).      The proof then goes like this:   \begin{align*} \|A\|_{2} &=  \sup_{\|u\|_{2}=1}\|Au\|_{2} \quad \text{(Definition of 2-norm)}\\&= \sup_{\|u\|_{2}=1}\|Au\|_{F} \quad \text{(Property 1)} \\&\leq \sup_{\|u\|_{2}=1}\|A\|_{F}\|u\|_{F} \quad \text{(Property 2)}\\&= \sup_{\|u\|_{2}=1}\|A\|_{F}\|u\|_{2} \quad \text{(Property 1)}\\&= \|A\|_{F}\sup_{\|u\|_{2}=1}\|u\|_{2} \\&= \|A\|_{F}. \end{align*}   	0.9556549787521362
929907	Consider a QR factorization of  A  in the form   A=\begin{bmatrix} A_1\\A_2 \end{bmatrix} = \begin{bmatrix} Q_1\\Q_2 \end{bmatrix} R=:QR   where  Q  has orthogonal columns and is partitioned in the same way as  A . The matrix  A  has full column rank so  R  is nonsingular.    We have  A^+=R^{-1}Q^*  and hence  \|A^+\|_2=\|R^{-1}\|_2 . We have to show that  \|R^{-1}\|_2\leq\|A_1^{-1}\|_2 . From  A_1=Q_1R , we get  R^{-1}=A_1^{-1}Q_1  and    \|R^{-1}\|_2\leq\|Q_1\|_2\|A_1^{-1}\|_2.   But  Q_1  is a submatrix of  Q  so  \|Q_1\|_2\leq\|Q\|_2=1 .  	0.955588161945343
1205085	Teeing off of Aweygan's great comment, if you assume that you want the  \ell^2  norm instead and that  A  has finite norm, then    \begin{align}  \|A\|_{\text{alt}} & = \sup \frac{\|A u\|_2}{\|u\|_1}\\ & \le \sup \frac{\|A\|_2\|u\|_2}{\|u\|_1} \\ & \le \sup \frac{\|A\|_2\|u\|_1}{\|u\|_1} \end{align}  	0.9555462002754211
1169035	This comes from the  Dual Function of the TV Norm .      You can read about its derivation by the  Dual Formulation of the ROF (Rudin Osher Fatemi) Model  (Another reference would be  Dual Methods for the Minimization of the Total Variation by Rémy Abergel  [See appendix]).    If you have a minimization problem given by:     \arg \min_{x} f \left( x \right) + \lambda \left\| D x \right\|     You can replace it by:     \arg \min_{x} \max_{z \: : \: {\left\| z \right\|}_{*} \leq 1} f \left( x \right) + \lambda {z}^{T} D x     Where   \left\| \cdot \right\|_{*}   is the Dual Norm of   \left\| \cdot \right\|  .    For instance, for   {L}_{p}   norms it means   {\left\| \cdot \right\|} = {\left\| \cdot \right\|}_{p}   and   {\left\| \cdot \right\|}_{*} = {\left\| \cdot \right\|}_{q}   where   \frac{1}{p} + \frac{1}{q} = 1  .    Remark  It would be great if someone could add a simple and intuitive derivation why the above is the Dual of the TV Norm.    References:      Duality Based Algorithms for Total Variation Regularized Image Restoration .   Dual Methods for the Minimization of the Total Variation .    	0.9554879069328308
1160491	Not sure if this is useful, but if you rewrite  S  as  I-(R-I)^T(R-I) , then \begin{align} \|S\|_F\le\|I\|_F+\|R-I\|_F^2 &= \|I\|_F+\|I\|_F^2+\|R\|_F^2-2\operatorname{tr}(R)\tag{1} \end{align} and tie occurs when  R=I . You may further relax the  -2\operatorname{tr}(R)  in  (1)  to  2\|I\|_F\|R\|_F  using Cauchy-Schwarz inequality, but then the resulting inequality is not tight because tie occurs in the Cauchy-Schwarz inequality here at  R=-I  rather than at  R=I .  	0.9554258584976196
1235849	I do not know, if there is any way to use directional derivatives, but the following should work: We have  g = f - \def\id{\mathrm{id}}\id  and hence for each  x \in \mathbb R^n  that    g'(x) = f'(x) - \id \iff f'(x) = g'(x) + \id   Hence   \def\<#1>{\left<#1\right>} \< f'(x)h,h> = \  + \|h\|^2   By Cauchy-Schwarz   \  \ge -\|g'(x)h\| \|h\| \ge -k\|h\|^2   and hence    \  = \  +\|h\|^2 \ge (1-k)\|h\|^2   For (ii), use (i). Note that for  x \in \mathbb R^n  we have  \begin{align*}   \  &= \  + \int_0^1 \frac{d}{dt} \ \, dt\\        &= \  + \int_0^1 \  \, dt\\        &\ge \  + \int_0^1 (1-k)\|x\|^2\, dt\\        &\ge -\|f(0)\|\|x\| + (1-k)\|x\|^2 \end{align*} By Cauchy-Schwarz \begin{align*}   \|f(x)\| &\ge \frac{\ }{\|x\|}\\      &\ge -\|f(0)\| + (1-k)\|x\|  \end{align*} and the result follows.  	0.9552144408226013
128059	By the Schur complement  condition  for positive definiteness,  B  is positive definite iff  A  is positive definite and  1 - u^T A^{-1} u > 0 .  Now, \begin{align*} |u^T A^{-1} u | &\leq \| u \| \| A^{-1} u \| \quad \text{(by Cauchy Schwarts)} \\ &\leq \|u\| \|L^{-T}\| \|L^{-1}\| \|u\| \\ &< 1. \end{align*} (In the last step, I use  \|u\| < 1  and  \| L^{-1} || = \|L^{-T} \| \leq 1 .)    This shows that  1 - u^T A^{-1} u > 0 , so  B > 0 .  	0.9551623463630676
965133	For simplicity I'll assume that  \alpha = 1 . You want to evaluate   x^\star = \arg \min_x \quad I(x) + \frac12 \|x\|^2 + \frac12 \|x - z \|^2   where  I  is the indicator function of the nonnegative orthant. We'll combine the two quadratic terms into a single quadratic term by completing the square.  Notice that \begin{align} &\frac12 \|x \|^2 +\frac{1}{2} \|x - z \|^2 \\ &= \frac12 \|x\|^2 + \frac12 \|x\|^2 - \langle x,z \rangle + \frac12\|z\|^2 \\ &= \|x\|^2 - \langle x,z \rangle + \frac12 \| z\|^2 \\ &= \underbrace{\|x\|^2 - 2 \langle x,z/2\rangle + \|z/2\|^2}_{\text{perfect square}} - \|z/2\|^2 + \frac12 \|z\|^2\\ &= \left\|x - \frac{z}{2} \right\|^2 + \text{terms that do not depend on  x }. \end{align} Therefore,   x^\star = \arg \min_x \quad I(x) + \|x - \frac{z}{2} \|^2.   Computing  x^\star  has now been reduced to evaluating the prox-operator of  I .  	0.9551308155059814
