pid	doc_raw	doc	score
2790711	"<p>This is the Holder's inequality for <span class=""math-container"" id=""25750767"">q=1</span> and <span class=""math-container"" id=""25750768"">p=\infty</span> i.e. </p>  <p>\begin{align*} \|v u\|_2 &amp;=\sqrt{\|v^2 u^2 \|_1}\\ &amp;\leq    \sqrt{\| v^2\|_\infty}\sqrt{ \| u^2\|_1}\\ &amp;=\|v\|_\infty \|u\|_2 \end{align*} And for the proof someone can see Martin's answer.</p>  <p>In certain cases the previous version of Holder's can be, easily, derived from the more popular one, i.e. when <span class=""math-container"" id=""25750769"">1&lt;p,q&lt;\infty</span>.  Indeeed,  <span class=""math-container"" id=""25750770""> \int v^2 u^2 dx \leq \sqrt[p]{\int v^{2p}}  \sqrt[q]{\int u^{2q}}   </span></p>  <p>For <span class=""math-container"" id=""25750771"">\frac{1}{q}+\frac{1}{p}=1</span>. When <span class=""math-container"" id=""25750772""> q\longrightarrow 1</span> then <span class=""math-container"" id=""25750773"">p\longrightarrow \infty</span>.</p>  <p>Someone can argue, in certain cases, that <span class=""math-container"" id=""25750774"">\sqrt[p]{\int v^{2p}}\longrightarrow \| u\|^2 _\infty</span>. </p>  <p>Again, under certain conditions, it can be shown that <span class=""math-container"" id=""25750775"">\int u^{2q} \rightarrow \int u^2  </span>.</p> "	This is the Holder's inequality for  q=1  and  p=\infty  i.e.     \begin{align*} \|v u\|_2 &=\sqrt{\|v^2 u^2 \|_1}\\ &\leq    \sqrt{\| v^2\|_\infty}\sqrt{ \| u^2\|_1}\\ &=\|v\|_\infty \|u\|_2 \end{align*} And for the proof someone can see Martin's answer.    In certain cases the previous version of Holder's can be, easily, derived from the more popular one, i.e. when  1	0.9680376052856445
1594927	"<p>Let <span class=""math-container"" id=""14738762"">A</span> be some <span class=""math-container"" id=""14738763"">m \times n</span> real matrix. Let <span class=""math-container"" id=""14738764"">\|A\|_1</span> be the <em>maximim absolute column sum norm</em>, <span class=""math-container"" id=""14738765"">\|A \|_\infty</span> be the <em>maximim absolute row sum norm</em> and <span class=""math-container"" id=""14738766"">\|A \|_2 = \sigma_{\max}</span> be the <em>2-norm</em> or <em>spectral norm</em> (<span class=""math-container"" id=""14738767"">\sigma_{\max}</span> is the maximum singular value). We have the following fundamental inequalities: <span class=""math-container"" id=""14738768"">(i) \quad \frac{1}{\sqrt{n}} \|A \|_\infty \leq \|A \|_2 \leq \sqrt{m} \|A \|_\infty</span> <span class=""math-container"" id=""14738769"">(ii) \quad \frac{1}{\sqrt{m}} \|A \|_1 \leq \|A \|_2 \leq \sqrt{n} \|A \|_1</span> <span class=""math-container"" id=""14738770""> \|A \|_2 \leq \sqrt{\|A \|_1 \|A \|_\infty} \quad (\text{some kind of Hölder's inequality})</span>.</p>  <p>So we see that if you impose additionally that also <span class=""math-container"" id=""14738771"">\|A \|_\infty \leq 1</span> (i.e. row sum is also small) then your assertion holds.</p>  <p>However it's easy to construct (quadratic) matrices with small column sum but big row sum (and vice versa). And inequality <span class=""math-container"" id=""14738772"">(i)</span> (or <span class=""math-container"" id=""14738773"">(ii)</span> whatever you are starting with) forces the largest singular value to be big. More precisely the following matrix <span class=""math-container"" id=""14738774""> \begin{pmatrix} 0.99 &amp; 0.99 &amp; 0.99\\ 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0\\ \end{pmatrix} </span> fulfilles your condition but has singular value <span class=""math-container"" id=""14738775"">1.71473</span>.</p> "	Let  A  be some  m \times n  real matrix. Let  \|A\|_1  be the  maximim absolute column sum norm ,  \|A \|_\infty  be the  maximim absolute row sum norm  and  \|A \|_2 = \sigma_{\max}  be the  2-norm  or  spectral norm  ( \sigma_{\max}  is the maximum singular value). We have the following fundamental inequalities:  (i) \quad \frac{1}{\sqrt{n}} \|A \|_\infty \leq \|A \|_2 \leq \sqrt{m} \|A \|_\infty   (ii) \quad \frac{1}{\sqrt{m}} \|A \|_1 \leq \|A \|_2 \leq \sqrt{n} \|A \|_1    \|A \|_2 \leq \sqrt{\|A \|_1 \|A \|_\infty} \quad (\text{some kind of Hölder's inequality}) .    So we see that if you impose additionally that also  \|A \|_\infty \leq 1  (i.e. row sum is also small) then your assertion holds.    However it's easy to construct (quadratic) matrices with small column sum but big row sum (and vice versa). And inequality  (i)  (or  (ii)  whatever you are starting with) forces the largest singular value to be big. More precisely the following matrix   \begin{pmatrix} 0.99 & 0.99 & 0.99\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix}   fulfilles your condition but has singular value  1.71473 .  	0.9676823616027832
504772	"<p>Not true for the spectral norm you use. Simple example: <span class=""math-container"" id=""5285641""> A = \begin{bmatrix}1 &amp; 1 \\ -1 &amp; 1\end{bmatrix}, \quad \tilde{A} = \begin{bmatrix}1 &amp; 1 \\ 0 &amp; 1\end{bmatrix}, </span> <span class=""math-container"" id=""5285642""> \|A\|_2 = \sqrt{2}\approx 1.4142, \qquad  \|\tilde{A}\|_2 = \sqrt{\frac{\sqrt{5}+3}{2}}\approx 1.6180. </span> On the other hand, zeroing out entries of <span class=""math-container"" id=""5285643"">A</span> to get <span class=""math-container"" id=""5285644"">\tilde{A}</span> can give:</p>  <ul> <li><p>strict inequality for the Frobenius norm (2-norm of the vector of the singular values): <span class=""math-container"" id=""5285645"">\|\tilde{A}\|_F &lt; \|A\|_F</span>,</p></li> <li><p>non-strict inequalities for some other norms, e.g.: <span class=""math-container"" id=""5285646"">\|\tilde{A}\|_{\star} \leq \|A\|_{\star}</span>, where <span class=""math-container"" id=""5285647"">\star = 1,\infty</span> (usual matrix <span class=""math-container"" id=""5285648"">p</span>-norms) or <span class=""math-container"" id=""5285649"">\star=M</span>, where <span class=""math-container"" id=""5285650"">\|A\|_M=\max_{i,j}|a_{ij}|</span>.</p></li> </ul> "	Not true for the spectral norm you use. Simple example:   A = \begin{bmatrix}1 & 1 \\ -1 & 1\end{bmatrix}, \quad \tilde{A} = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix},     \|A\|_2 = \sqrt{2}\approx 1.4142, \qquad  \|\tilde{A}\|_2 = \sqrt{\frac{\sqrt{5}+3}{2}}\approx 1.6180.   On the other hand, zeroing out entries of  A  to get  \tilde{A}  can give:      strict inequality for the Frobenius norm (2-norm of the vector of the singular values):  \|\tilde{A}\|_F < \|A\|_F ,   non-strict inequalities for some other norms, e.g.:  \|\tilde{A}\|_{\star} \leq \|A\|_{\star} , where  \star = 1,\infty  (usual matrix  p -norms) or  \star=M , where  \|A\|_M=\max_{i,j}|a_{ij}| .    	0.967046856880188
2408986	"<p><span class=""math-container"" id=""22303135"">A\in\mathbb{R}^{n\times n}</span>, symmetric and <span class=""math-container"" id=""22303136"">e:=[1, 1, \ldots, 1]^{\top}</span>. Now,</p>  <p>\begin{equation} e^{\top}Ae=\sum_{i,j}A_{i,j}=:c \end{equation}</p>  <p>and,</p>  <p>\begin{align} Aee^{\top}A=\alpha\alpha^{\top} \end{align}</p>  <p>where <span class=""math-container"" id=""22303137"">\alpha:=Ae</span> denotes the vector of row (or column) sums of <span class=""math-container"" id=""22303138"">A</span>.</p>  <p>So,</p>  <p>\begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}=\|A-c\alpha\alpha^{\top}\|_{F} \end{equation}</p>  <p>One way to proceed can be to apply triangle inequality. For that, note that for a rank-1 symmetric matrix <span class=""math-container"" id=""22303139"">xx^{\top}</span>, <span class=""math-container"" id=""22303140"">\|xx^{\top}\|_{F}=\|x\|_{2}^{2}</span>. This is because for any matrix, the Frobenius norm is the <span class=""math-container"" id=""22303141"">2</span>-norm of the vector of singular values, and <span class=""math-container"" id=""22303142"">xx^{\top}</span> has just one non-zero eigenvalue, namely, <span class=""math-container"" id=""22303143"">\|x\|_{2}^{2}</span>. This gives</p>  <p>\begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}\le \|A\|_{F}+\frac{1}{c}\|\alpha\|_{2}^{2}. \end{equation}</p>  <p>Is this the kind of characterization you are looking for?</p> "	A\in\mathbb{R}^{n\times n} , symmetric and  e:=[1, 1, \ldots, 1]^{\top} . Now,    \begin{equation} e^{\top}Ae=\sum_{i,j}A_{i,j}=:c \end{equation}    and,    \begin{align} Aee^{\top}A=\alpha\alpha^{\top} \end{align}    where  \alpha:=Ae  denotes the vector of row (or column) sums of  A .    So,    \begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}=\|A-c\alpha\alpha^{\top}\|_{F} \end{equation}    One way to proceed can be to apply triangle inequality. For that, note that for a rank-1 symmetric matrix  xx^{\top} ,  \|xx^{\top}\|_{F}=\|x\|_{2}^{2} . This is because for any matrix, the Frobenius norm is the  2 -norm of the vector of singular values, and  xx^{\top}  has just one non-zero eigenvalue, namely,  \|x\|_{2}^{2} . This gives    \begin{equation} \|A-\frac{1}{e^{\top}Ae}Aee^{\top}A\|_{F}\le \|A\|_{F}+\frac{1}{c}\|\alpha\|_{2}^{2}. \end{equation}    Is this the kind of characterization you are looking for?  	0.96689772605896
1853806	"<p>Yes; apply Holder's inequality (you can also use Jensen if you like): \begin{align*} \|f\|_{L^p}=\|f\cdot1\|_{L^p}\leq\|f\|_{L^q}\|1\|_{1/(1/p-1/q)}=\|f\|_{L^q}\mu(X)^{1/(1/p-1/q)}\leq\|f\|_{L^q} \end{align*} since we assumed <span class=""math-container"" id=""17160207"">\mu(X)&lt;1</span>, and <span class=""math-container"" id=""17160208"">1/(1/p-1/q)\geq0</span> from <span class=""math-container"" id=""17160209"">p\leq q</span>.</p>  <p><strong>Edit:</strong> Please note I used a slightly generalized version of the Holder inequality for the case where a product is being estimated in <span class=""math-container"" id=""17160210"">L^p</span> for <span class=""math-container"" id=""17160211"">p</span> not necessarily <span class=""math-container"" id=""17160212"">1</span>. This is described <a href=""https://math.stackexchange.com/questions/159887/generalized-h%C3%B6lder-inequality"">here</a>.</p> "	Yes; apply Holder's inequality (you can also use Jensen if you like): \begin{align*} \|f\|_{L^p}=\|f\cdot1\|_{L^p}\leq\|f\|_{L^q}\|1\|_{1/(1/p-1/q)}=\|f\|_{L^q}\mu(X)^{1/(1/p-1/q)}\leq\|f\|_{L^q} \end{align*} since we assumed  \mu(X)<1 , and  1/(1/p-1/q)\geq0  from  p\leq q .    Edit:  Please note I used a slightly generalized version of the Holder inequality for the case where a product is being estimated in  L^p  for  p  not necessarily  1 . This is described  here .  	0.9667444825172424
2425957	"<p>I'm posting this as an answer, since it doesn't fit in a comment, and seems to be pretty darn close to what is required.</p>  <p>I get the following bound: \begin{align} \|f-g\|_p^p  &amp;= \| |f-g|^p \|_1 \\  &amp;\le \| |f-g| |f-g|^{p-1} \| \\  &amp;= \| f-g \|_2 \| |f-g|^{p-1} \|_2 \tag{by Hölder} \\  &amp;= \|f-g\|_2 \left( \int |f-g|^{2p-2} \right)^{\frac{1}{2}} \\  &amp;\le \|f-g\|_2 \left( \int (|f|-|g|)^{2p-2} \right)^{\frac{1}{2}} \tag{triangle inequality} \\  &amp;\le \|f-g\|_2 \left( \int (2C)^{2p-2} \right)^{\frac{1}{2}} \\  &amp;= (2C)^{p-1} \sqrt{\mu(X)} \|f-g\|_2 \end{align} This differs from the desired bound in three ways:  (1) the exponent on <span class=""math-container"" id=""22467836"">C</span> is <span class=""math-container"" id=""22467837"">p-1</span>, not <span class=""math-container"" id=""22467838"">p-2</span> (this could be a clerical error on my part), (2) the extra 2 in the constant, and (3) it appears to me that the inequality also depends on the measure of the space.</p> "	I'm posting this as an answer, since it doesn't fit in a comment, and seems to be pretty darn close to what is required.    I get the following bound: \begin{align} \|f-g\|_p^p  &= \| |f-g|^p \|_1 \\  &\le \| |f-g| |f-g|^{p-1} \| \\  &= \| f-g \|_2 \| |f-g|^{p-1} \|_2 \tag{by Hölder} \\  &= \|f-g\|_2 \left( \int |f-g|^{2p-2} \right)^{\frac{1}{2}} \\  &\le \|f-g\|_2 \left( \int (|f|-|g|)^{2p-2} \right)^{\frac{1}{2}} \tag{triangle inequality} \\  &\le \|f-g\|_2 \left( \int (2C)^{2p-2} \right)^{\frac{1}{2}} \\  &= (2C)^{p-1} \sqrt{\mu(X)} \|f-g\|_2 \end{align} This differs from the desired bound in three ways:  (1) the exponent on  C  is  p-1 , not  p-2  (this could be a clerical error on my part), (2) the extra 2 in the constant, and (3) it appears to me that the inequality also depends on the measure of the space.  	0.9645787477493286
191021	"<p>I give it a try with your first question: If <span class=""math-container"" id=""2216498"">A\in\mathbb{R}^{m\times n}</span> then <span class=""math-container"" id=""2216499"">\frac{1}{\sqrt{n}}\|A\|_\infty\leq \|A\|_2\leq \sqrt{m}\|A\|_\infty</span> <span class=""math-container"" id=""2216500"">\frac{1}{\sqrt{m}}\|A\|_1\leq \|A\|_2\leq \sqrt{n}\|A\|_1</span></p>  <p>Also <span class=""math-container"" id=""2216501"">\|AB\|_a\leq \|A\|_a\|B\|_a</span></p>  <p>Note that <span class=""math-container"" id=""2216502"">\|B_i\|_2&lt;1</span> implies that absolute value of the every element of matrix B is less than 1, <span class=""math-container"" id=""2216503"">|B_{ij}|&lt;1</span>, then <span class=""math-container"" id=""2216504"">\|B\|_\infty&lt;n</span> (infinity norm of a matrix is the maximum absolute row sum of the matrix). And with the same argument <span class=""math-container"" id=""2216505"">\|B\|_1&lt;m</span>.</p>  <p>Then using these properties we have <span class=""math-container"" id=""2216506"">\|vA^TB\|_\infty\leq \sqrt{n}\|vA^TB\|_2\leq \sqrt{n} \|vA^T\|_2\|B\|_2\leq \sqrt{nm} \|vA^T\|_2\|B\|_\infty\leq n\sqrt{nm} \|vA^T\|_2</span> or <span class=""math-container"" id=""2216507"">\|vA^TB\|_\infty\leq \sqrt{n}\|vA^TB\|_2\leq \sqrt{n} \|vA^T\|_2\|B\|_2\leq n \|vA^T\|_2\|B\|_1\leq nm \|vA^T\|_2</span> (these are conservative bounds you can probably obtain a tighter bound)</p> "	I give it a try with your first question: If  A\in\mathbb{R}^{m\times n}  then  \frac{1}{\sqrt{n}}\|A\|_\infty\leq \|A\|_2\leq \sqrt{m}\|A\|_\infty   \frac{1}{\sqrt{m}}\|A\|_1\leq \|A\|_2\leq \sqrt{n}\|A\|_1    Also  \|AB\|_a\leq \|A\|_a\|B\|_a    Note that  \|B_i\|_2<1  implies that absolute value of the every element of matrix B is less than 1,  |B_{ij}|<1 , then  \|B\|_\infty	0.964419960975647
993200	"<p>If there is an <span class=""math-container"" id=""9450795"">\alpha</span> such that for any nonzero <span class=""math-container"" id=""9450796"">x</span>, <span class=""math-container"" id=""9450797"">\|Ax\|_2\leq\alpha\|x\|_2</span>, then by the definition of the operator 2-norm, <span class=""math-container"" id=""9450798""> \|A\|_2=\max_{x\neq 0}\frac{\|Ax\|_2}{\|x\|_2}\leq\alpha. </span> In your case, <span class=""math-container"" id=""9450799"">\alpha=\|u\|_2\|v\|_2</span>. Note that actually for <span class=""math-container"" id=""9450800"">A=uv^*</span>, we have the equality: <span class=""math-container"" id=""9450801"">\|A\|_2=\|u\|_2\|v\|_2</span>. It is easy to see this by realizing for what vectors the Cauchy-Schwarz inequality gives the equality: <span class=""math-container"" id=""9450802""> \|Ax\|_2=\|uv^*x\|_2=\underbrace{\|u\|_2\color{red}{|v^*x|}\leq\|u\|_2\color{red}{\|v\|_2\|x\|_2}}_{\text{Cauchy-Schwarz}}. </span> The equality is attained for <span class=""math-container"" id=""9450803"">x</span> proportional to <span class=""math-container"" id=""9450804"">v</span>.</p> "	If there is an  \alpha  such that for any nonzero  x ,  \|Ax\|_2\leq\alpha\|x\|_2 , then by the definition of the operator 2-norm,   \|A\|_2=\max_{x\neq 0}\frac{\|Ax\|_2}{\|x\|_2}\leq\alpha.   In your case,  \alpha=\|u\|_2\|v\|_2 . Note that actually for  A=uv^* , we have the equality:  \|A\|_2=\|u\|_2\|v\|_2 . It is easy to see this by realizing for what vectors the Cauchy-Schwarz inequality gives the equality:   \|Ax\|_2=\|uv^*x\|_2=\underbrace{\|u\|_2\color{red}{|v^*x|}\leq\|u\|_2\color{red}{\|v\|_2\|x\|_2}}_{\text{Cauchy-Schwarz}}.   The equality is attained for  x  proportional to  v .  	0.9643555879592896
1317223	"<p><strong>Partial results:</strong></p>  <p>We define <span class=""math-container"" id=""12241459""> \DeclareMathOperator{\tr}{trace} \|A\|_1 = 1'|A|1 = \sum_{i,j}|a_{ij}|\\ \|A\|_2 = \sqrt{\tr(A'A)} = \sqrt{\sum_{i,j}|a_{ij}|^2} </span> <a href=""http://en.wikipedia.org/wiki/Matrix_norm#.22Entrywise.22_norms"" rel=""nofollow"">These are</a> the entrywise <span class=""math-container"" id=""12241460"">1</span>-norm and entrywise <span class=""math-container"" id=""12241461"">2</span>-norm (AKA Frobenius norm) respectively.  Both are submultiplicative matrix norms.</p>  <p>We note that for any matrix <span class=""math-container"" id=""12241462"">A</span>, <span class=""math-container"" id=""12241463""> \|A\|_2 \leq \|A\|_1 \leq n \|A\|_2 </span> By the information given, we note that <span class=""math-container"" id=""12241464"">\|A\|_2^2 = \tr(A'A) = o(n)</span>. In particular, this means that <span class=""math-container"" id=""12241465"">\|A\|_2 = o(\sqrt{n})</span>.  Now, because the norms are submultiplicative, we also have <span class=""math-container"" id=""12241466""> \|A^3\|_2 \leq \|A\|_2^3 = o(n^{3/2}) </span> From there, we have <span class=""math-container"" id=""12241467""> 1'A^31 = \|A^3\|_1 \leq n \|A^3\|_2 = o(n^{5/2})=o(n^{2.5}) </span> Which may be the best we can do.</p>  <hr>  <p>The information is <span class=""math-container"" id=""12241468"">1'A1 = O(n)</span> doesn't let us do better directly.  In particular, we have <span class=""math-container"" id=""12241469""> \|A^3\|_1 \leq \|A\|_1^3 = O(n^3) </span></p> "	Partial results:    We define   \DeclareMathOperator{\tr}{trace} \|A\|_1 = 1'|A|1 = \sum_{i,j}|a_{ij}|\\ \|A\|_2 = \sqrt{\tr(A'A)} = \sqrt{\sum_{i,j}|a_{ij}|^2}    These are  the entrywise  1 -norm and entrywise  2 -norm (AKA Frobenius norm) respectively.  Both are submultiplicative matrix norms.    We note that for any matrix  A ,   \|A\|_2 \leq \|A\|_1 \leq n \|A\|_2   By the information given, we note that  \|A\|_2^2 = \tr(A'A) = o(n) . In particular, this means that  \|A\|_2 = o(\sqrt{n}) .  Now, because the norms are submultiplicative, we also have   \|A^3\|_2 \leq \|A\|_2^3 = o(n^{3/2})   From there, we have   1'A^31 = \|A^3\|_1 \leq n \|A^3\|_2 = o(n^{5/2})=o(n^{2.5})   Which may be the best we can do.       The information is  1'A1 = O(n)  doesn't let us do better directly.  In particular, we have   \|A^3\|_1 \leq \|A\|_1^3 = O(n^3)   	0.9642890095710754
930678	"<p>This follows from <a href=""http://en.wikipedia.org/wiki/H%C3%B6lder&#39;s_inequality"" rel=""nofollow"">Hölder's inequality</a>. Letting <span class=""math-container"" id=""8973277"">r</span> denote the Hölder conjugate of <span class=""math-container"" id=""8973278"">\frac{q}{p}</span>, we have <span class=""math-container"" id=""8973279"">\|f\|_p^p = \|f^p\|_1 \leq \|f^p\|_{q/p}\|1\|_r = \|f^p\|_{q/p} = \|f\|_q^p</span> so <span class=""math-container"" id=""8973280"">\|f\|_p \leq \|f\|_q</span>. More generally, if <span class=""math-container"" id=""8973281"">(X, \mathcal{M}, \mu)</span> is a finite measure space, the same proof shows that <span class=""math-container"" id=""8973282"">\|f\|_p \leq \|f\|_q\mu(X)^{\frac{1}{p}-\frac{1}{q}}</span>.</p> "	This follows from  Hölder's inequality . Letting  r  denote the Hölder conjugate of  \frac{q}{p} , we have  \|f\|_p^p = \|f^p\|_1 \leq \|f^p\|_{q/p}\|1\|_r = \|f^p\|_{q/p} = \|f\|_q^p  so  \|f\|_p \leq \|f\|_q . More generally, if  (X, \mathcal{M}, \mu)  is a finite measure space, the same proof shows that  \|f\|_p \leq \|f\|_q\mu(X)^{\frac{1}{p}-\frac{1}{q}} .  	0.9640804529190063
135979	"<p>To prove the stated inequality (for normalized measure spaces): Let <span class=""math-container"" id=""1566316"">X</span> be a finite measure space and <span class=""math-container"" id=""1566317"">q \le p</span>. We have using Hölder for <span class=""math-container"" id=""1566318"">f \in L^p</span> \begin{align*}    \|f\|_q &amp;= \bigl\||f|^q\bigr\|_1^{1/q}\\\            &amp;= \bigl\|1 \cdot |f|^q\bigr\|_1^{1/q}\\\            &amp;= \|1\|_{p/(p-q)} \bigl\||f|^q\bigr\|_{p/q}^{1/q}\\\            &amp;= \mu(X)^{(p-q)/p} \|f\|_p \end{align*} If <span class=""math-container"" id=""1566319"">\mu</span> is normalized, i. e. <span class=""math-container"" id=""1566320"">\mu(X) = 1</span> (for example in <span class=""math-container"" id=""1566321"">[0,1]</span> or <span class=""math-container"" id=""1566322"">\mathbb T</span> with normalized arclength), then <span class=""math-container"" id=""1566323"">\|f\|_q \le \|f\|_p</span> for every <span class=""math-container"" id=""1566324"">q \le p</span>. especially <span class=""math-container"" id=""1566325"">\|f\|_{p-1} \le \|f\|_p</span>.</p> "	To prove the stated inequality (for normalized measure spaces): Let  X  be a finite measure space and  q \le p . We have using Hölder for  f \in L^p  \begin{align*}    \|f\|_q &= \bigl\||f|^q\bigr\|_1^{1/q}\\\            &= \bigl\|1 \cdot |f|^q\bigr\|_1^{1/q}\\\            &= \|1\|_{p/(p-q)} \bigl\||f|^q\bigr\|_{p/q}^{1/q}\\\            &= \mu(X)^{(p-q)/p} \|f\|_p \end{align*} If  \mu  is normalized, i. e.  \mu(X) = 1  (for example in  [0,1]  or  \mathbb T  with normalized arclength), then  \|f\|_q \le \|f\|_p  for every  q \le p . especially  \|f\|_{p-1} \le \|f\|_p .  	0.964065670967102
2719876	"<p>Observe we have that \begin{align} \|Ax\|_2^2= x^TA^TAx \end{align} where <span class=""math-container"" id=""25104084"">A^TA</span> is diagonalizable since it's symmetric. Then we see that \begin{align} x^TA^TAx = y^TDy \end{align} where <span class=""math-container"" id=""25104085"">D</span> consists of the square of the singular values. Hence, we get that \begin{align} \|Ax\|_2^2 = \lambda_1^2y_1^2+\lambda_2^2y_2^2. \end{align} In the case when <span class=""math-container"" id=""25104086"">\|x\|_2=1</span> then we also have that <span class=""math-container"" id=""25104087"">\|y\|_2=1</span>. Hence it follows \begin{align} \|Ax\|_2^2 \leq \max_{1\le i \le 2}\lambda_i^2 \ \ \implies \ \ \|Ax\|_2 \leq \max_{1 \le i \le 2}\lambda_i. \end{align}</p>  <p>Note that I say <strong>singular value</strong>, not <strong>eigenvalue</strong>.</p>  <p><strong>Additional</strong>: Consider \begin{align} A= \begin{pmatrix} 0 &amp; 1\\ 0 &amp; 0 \end{pmatrix} \end{align} then we see that  \begin{align} \sup_{\|x\|_2=1}\|Ax\|_2 = 1 \end{align} but the eigenvalues of <span class=""math-container"" id=""25104088"">A</span> are <span class=""math-container"" id=""25104089"">0</span>. Moreover, observe that \begin{align} A^TA= \begin{pmatrix} 0 &amp; 0\\ 0 &amp; 1 \end{pmatrix} \end{align} which means the singular values of <span class=""math-container"" id=""25104090"">A</span> are equal to <span class=""math-container"" id=""25104091"">1</span> and <span class=""math-container"" id=""25104092"">0</span>.</p> "	Observe we have that \begin{align} \|Ax\|_2^2= x^TA^TAx \end{align} where  A^TA  is diagonalizable since it's symmetric. Then we see that \begin{align} x^TA^TAx = y^TDy \end{align} where  D  consists of the square of the singular values. Hence, we get that \begin{align} \|Ax\|_2^2 = \lambda_1^2y_1^2+\lambda_2^2y_2^2. \end{align} In the case when  \|x\|_2=1  then we also have that  \|y\|_2=1 . Hence it follows \begin{align} \|Ax\|_2^2 \leq \max_{1\le i \le 2}\lambda_i^2 \ \ \implies \ \ \|Ax\|_2 \leq \max_{1 \le i \le 2}\lambda_i. \end{align}    Note that I say  singular value , not  eigenvalue .    Additional : Consider \begin{align} A= \begin{pmatrix} 0 & 1\\ 0 & 0 \end{pmatrix} \end{align} then we see that  \begin{align} \sup_{\|x\|_2=1}\|Ax\|_2 = 1 \end{align} but the eigenvalues of  A  are  0 . Moreover, observe that \begin{align} A^TA= \begin{pmatrix} 0 & 0\\ 0 & 1 \end{pmatrix} \end{align} which means the singular values of  A  are equal to  1  and  0 .  	0.9624617099761963
1965830	"<p>Here is a proof for the Euclidean norm. This approach is buried in Schäffer. J., ""<a href=""http://repository.cmu.edu/math/163/"" rel=""nofollow"">Norms and determinants of linear mappings</a>"", Technical report, CMU, Department of Mathematical Sciences, 1970. He does not use the SVD, but the idea is essentially the same.</p>  <p>Let <span class=""math-container"" id=""17537433"">A=U \Sigma V^*</span> be a singular value decomposition of <span class=""math-container"" id=""17537434"">A</span>, with <span class=""math-container"" id=""17537435"">\Sigma=\operatorname{diag} (\sigma_1,...,\sigma_n)</span>, and <span class=""math-container"" id=""17537436"">\sigma_1\ge ... \ge\sigma_n</span>.</p>  <p>Then <span class=""math-container"" id=""17537437"">\|A\| = \sigma_1, \|A^{-1}\| = {1 \over \sigma_n}</span>, and <span class=""math-container"" id=""17537438"">|\det A| = \sigma_1 \cdots \sigma_n</span>.</p>  <p>Hence <span class=""math-container"" id=""17537439"">|\det A| \le \sigma_1 \cdots \sigma_{n-1} {1 \over \|A^{-1} \|} \le \|A\|^{n-1} {1 \over \|A^{-1} \|}</span>, from which we obtain <span class=""math-container"" id=""17537440"">|\det A| \|A^{-1} \| \le \|A\|^{n-1}</span>.</p>  <p>The result is not true for general operator norms, for example, with <span class=""math-container"" id=""17537441"">A=\begin{bmatrix} {1 \over 2} &amp; 1 \\ 0 &amp; 2 \end{bmatrix}</span>, we have <span class=""math-container"" id=""17537442"">\det A = 1</span>, <span class=""math-container"" id=""17537443"">\|A^{-1} \|_\infty = 3, \|A\|_\infty  = 2</span> and so <span class=""math-container"" id=""17537444"">3=\|A^{-1} \|_\infty \not&lt; \|A\|_\infty^{2-1} = 2</span>.</p>  <p>As an aside, it is worth noting the related <a href=""https://en.wikipedia.org/wiki/Hadamard%27s_inequality"" rel=""nofollow"">Hadamard's inequality</a>, <span class=""math-container"" id=""17537445"">|\det A| \le \|Ae_1\| \cdots \|A e_n\|</span> (Euclidean norm).</p> "	"Here is a proof for the Euclidean norm. This approach is buried in Schäffer. J., "" Norms and determinants of linear mappings "", Technical report, CMU, Department of Mathematical Sciences, 1970. He does not use the SVD, but the idea is essentially the same.    Let  A=U \Sigma V^*  be a singular value decomposition of  A , with  \Sigma=\operatorname{diag} (\sigma_1,...,\sigma_n) , and  \sigma_1\ge ... \ge\sigma_n .    Then  \|A\| = \sigma_1, \|A^{-1}\| = {1 \over \sigma_n} , and  |\det A| = \sigma_1 \cdots \sigma_n .    Hence  |\det A| \le \sigma_1 \cdots \sigma_{n-1} {1 \over \|A^{-1} \|} \le \|A\|^{n-1} {1 \over \|A^{-1} \|} , from which we obtain  |\det A| \|A^{-1} \| \le \|A\|^{n-1} .    The result is not true for general operator norms, for example, with  A=\begin{bmatrix} {1 \over 2} & 1 \\ 0 & 2 \end{bmatrix} , we have  \det A = 1 ,  \|A^{-1} \|_\infty = 3, \|A\|_\infty  = 2  and so  3=\|A^{-1} \|_\infty \not< \|A\|_\infty^{2-1} = 2 .    As an aside, it is worth noting the related  Hadamard's inequality ,  |\det A| \le \|Ae_1\| \cdots \|A e_n\|  (Euclidean norm).  "	0.962037205696106
955263	"<p>It is maybe a too big cannon to use (or prove) the continuity of eigenvalues. It is well known that the (matrix) norms are continuous. This follows from the triangle inequality: <span class=""math-container"" id=""9160035""> \|A\|\leq\|B\|+\|A-B\|, \quad \|B\|\leq\|A\|+\|A-B\|\quad\Rightarrow \quad |\|A\|-\|B\||\leq\|A-B\|. </span> Therefore, if <span class=""math-container"" id=""9160036"">A</span> and <span class=""math-container"" id=""9160037"">B</span> are close then <span class=""math-container"" id=""9160038"">\|A\|</span> and <span class=""math-container"" id=""9160039"">\|B\|</span> are close as well.</p>  <p>Then use the fact that <span class=""math-container"" id=""9160040"">\|A\|=\rho(A)</span> when <span class=""math-container"" id=""9160041"">\|\cdot\|:=\|\cdot\|_2</span> is the spectral norm and <span class=""math-container"" id=""9160042"">A</span> is SPD.</p>  <p>You can relate the componentwise closeness to the normwise closeness by realizing that <span class=""math-container"" id=""9160043""> \|A\|_2=\max_{\|x\|_2=1}\|Ax\|_2\leq n\max_{1\leq i,j\leq n}|a_{ij}|=:n\|A\|_{\max} </span> (or by simply showing that <span class=""math-container"" id=""9160044"">\|A\|_{\max}</span> is a norm and use the fact that norms on a finite-dimensional space are equivalent). Therefore, if <span class=""math-container"" id=""9160045"">|a_{ij}-b_{ij}|&lt;\epsilon</span> for all <span class=""math-container"" id=""9160046"">i,j=1,\ldots,n</span> and some <span class=""math-container"" id=""9160047"">\epsilon&gt;0</span>, we have <span class=""math-container"" id=""9160048"">|\|A\|_2-\|B\|_2|&lt;\delta</span> with <span class=""math-container"" id=""9160049"">\delta:=\epsilon/n</span>.</p> "	It is maybe a too big cannon to use (or prove) the continuity of eigenvalues. It is well known that the (matrix) norms are continuous. This follows from the triangle inequality:   \|A\|\leq\|B\|+\|A-B\|, \quad \|B\|\leq\|A\|+\|A-B\|\quad\Rightarrow \quad |\|A\|-\|B\||\leq\|A-B\|.   Therefore, if  A  and  B  are close then  \|A\|  and  \|B\|  are close as well.    Then use the fact that  \|A\|=\rho(A)  when  \|\cdot\|:=\|\cdot\|_2  is the spectral norm and  A  is SPD.    You can relate the componentwise closeness to the normwise closeness by realizing that   \|A\|_2=\max_{\|x\|_2=1}\|Ax\|_2\leq n\max_{1\leq i,j\leq n}|a_{ij}|=:n\|A\|_{\max}   (or by simply showing that  \|A\|_{\max}  is a norm and use the fact that norms on a finite-dimensional space are equivalent). Therefore, if  |a_{ij}-b_{ij}|<\epsilon  for all  i,j=1,\ldots,n  and some  \epsilon>0 , we have  |\|A\|_2-\|B\|_2|<\delta  with  \delta:=\epsilon/n .  	0.9620344042778015
1961789	"<p>This is nothing but the dual norm of the the norm <span class=""math-container"" id=""18176329"">x \mapsto \|x\|_A:= \langle x, Ax\rangle^{1/2} = \|A^{1/2}x\|_2</span> induced by the postive-definite matrix (p.d) <span class=""math-container"" id=""18176330"">A</span>, evaluated at the point <span class=""math-container"" id=""18176331"">a := a_1-a_2</span>. The value can be computed analytically as <span class=""math-container"" id=""18176332"">\|A^{-1/2}a\|_2</span> and is attained by setting <span class=""math-container"" id=""18176333"">x = \frac{1}{\|A^{-1/2}a\|_2}A^{-1/2}a</span> in the quotient <span class=""math-container"" id=""18176334"">\langle x, a\rangle /\|x\|_A</span>. Indeed,</p>  <p><span class=""math-container"" id=""18176335""> \begin{aligned} \max_{x \ne 0} \frac{\langle x, a\rangle}{(x^TAx)^{1/2}} &amp;= \max_{y \ne 0}\frac{\langle A^{-1/2}y,a\rangle}{\|y\|_2} = \max_{\|y\|_2 \le 1} \langle y, A^{-1/2}a\rangle = \|A^{-1/2}a\|_2. \end{aligned} </span> where we've used the change of variable <span class=""math-container"" id=""18176336"">y = A^{1/2}x</span>.</p>  <p>In particular, we've shown that the dual of the norm induced by a p.s.d matrix is the norm induced by it's inverse. That is, that induced norms <span class=""math-container"" id=""18176337"">\|.\|_A</span> and <span class=""math-container"" id=""18176338"">\|.\|_{A^{-1}}</span> are duals to each other.</p>  <p><strong>N.B.</strong> Your problem doesn't make sense if <span class=""math-container"" id=""18176339"">A</span> is not strictly positive-definite, because for positive semi-definite matrices, the denominator <span class=""math-container"" id=""18176340"">x^TAx</span> can be equal zero for non-zero <span class=""math-container"" id=""18176341"">x</span>.</p> "	This is nothing but the dual norm of the the norm  x \mapsto \|x\|_A:= \langle x, Ax\rangle^{1/2} = \|A^{1/2}x\|_2  induced by the postive-definite matrix (p.d)  A , evaluated at the point  a := a_1-a_2 . The value can be computed analytically as  \|A^{-1/2}a\|_2  and is attained by setting  x = \frac{1}{\|A^{-1/2}a\|_2}A^{-1/2}a  in the quotient  \langle x, a\rangle /\|x\|_A . Indeed,     \begin{aligned} \max_{x \ne 0} \frac{\langle x, a\rangle}{(x^TAx)^{1/2}} &= \max_{y \ne 0}\frac{\langle A^{-1/2}y,a\rangle}{\|y\|_2} = \max_{\|y\|_2 \le 1} \langle y, A^{-1/2}a\rangle = \|A^{-1/2}a\|_2. \end{aligned}   where we've used the change of variable  y = A^{1/2}x .    In particular, we've shown that the dual of the norm induced by a p.s.d matrix is the norm induced by it's inverse. That is, that induced norms  \|.\|_A  and  \|.\|_{A^{-1}}  are duals to each other.    N.B.  Your problem doesn't make sense if  A  is not strictly positive-definite, because for positive semi-definite matrices, the denominator  x^TAx  can be equal zero for non-zero  x .  	0.961755096912384
911322	"<p>Any induced norm satisfies <span class=""math-container"" id=""8769921"">\|A\|\ge \rho(A)</span>, where <span class=""math-container"" id=""8769922"">\rho(A)</span> is the spectral radius of <span class=""math-container"" id=""8769923"">A</span>, <em>i.e.</em> the largest absolute value of its eigenvalues. This fact is stated without proof <a href=""http://en.wikipedia.org/w/index.php?title=Matrix_norm&amp;oldid=622243996#Induced_norm"" rel=""nofollow"">in Wikipedia</a>; here is a proof.</p>  <blockquote>   <p>Any induced norm is of the form   <span class=""math-container"" id=""8769924"">\|A\|=\max_x\frac{\|Ax\|}{\|x\|}.</span> Let <span class=""math-container"" id=""8769925"">x^*</span> be an eigenvector corresponding to the eigenvalue <span class=""math-container"" id=""8769926"">\lambda^*</span> with the largest absolute value. Then <span class=""math-container"" id=""8769927"">\|A\|\ge\frac{\|Ax^*\|}{\|x^*\|}=\frac{\|\lambda^*x^*\|}{\|x^*\|}=|\lambda^*|\frac{\|x^*\|}{\|x^*\|}=|\lambda^*|.</span></p> </blockquote>  <p>Let <span class=""math-container"" id=""8769928"">A=\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}</span>. Then your norm is <span class=""math-container"" id=""8769929"">\|A\|=\dfrac1{\sqrt2}\|A\|_F=\dfrac1{\sqrt2}</span> while <span class=""math-container"" id=""8769930"">\rho(A)=1</span>, contradicting the above inequality.</p> "	Any induced norm satisfies  \|A\|\ge \rho(A) , where  \rho(A)  is the spectral radius of  A ,  i.e.  the largest absolute value of its eigenvalues. This fact is stated without proof  in Wikipedia ; here is a proof.        Any induced norm is of the form    \|A\|=\max_x\frac{\|Ax\|}{\|x\|}.  Let  x^*  be an eigenvector corresponding to the eigenvalue  \lambda^*  with the largest absolute value. Then  \|A\|\ge\frac{\|Ax^*\|}{\|x^*\|}=\frac{\|\lambda^*x^*\|}{\|x^*\|}=|\lambda^*|\frac{\|x^*\|}{\|x^*\|}=|\lambda^*|.      Let  A=\begin{bmatrix}1&0\\0&0\end{bmatrix} . Then your norm is  \|A\|=\dfrac1{\sqrt2}\|A\|_F=\dfrac1{\sqrt2}  while  \rho(A)=1 , contradicting the above inequality.  	0.9617016315460205
2407247	"<p>For the identity map <span class=""math-container"" id=""22220623"">I</span> </p>  <p><span class=""math-container"" id=""22220624"">\|I\|_{op}=\sup \{|L(x)| : |x|=1 \}=\sup \{|x| : |x|=1 \}=1.</span></p>  <p><span class=""math-container"" id=""22220625""> \|I\|_{tr} = \sum_{i=1}^{n}\sum_{j=1}^{n}(a_{i,j})^2= \sum_{i=1}^{n}\sum_{j=1}^{n}(\delta_{i,j})^2=n</span></p>  <p>(I recall that <span class=""math-container"" id=""22220626"">\delta_{i,j}=1</span> if <span class=""math-container"" id=""22220627"">i=j</span> and <span class=""math-container"" id=""22220628"">0</span>, otherwise).</p>  <p>I guess that there should be taken a root from <span class=""math-container"" id=""22220629"">\|L\|_{tr}</span> in order to make it norm, that is to assure that <span class=""math-container"" id=""22220630"">\|\lambda L\|_{tr}=|\lambda|\|L\|_{tr}</span> for each real <span class=""math-container"" id=""22220631"">\lambda</span>, because in the present state we have <span class=""math-container"" id=""22220632"">\|\lambda L\|_{tr}= |\lambda|^2\|L\|_{tr}</span>. In particular, <span class=""math-container"" id=""22220633"">\|\lambda I\|_{op}=|\lambda|</span>, whereas <span class=""math-container"" id=""22220634"">\|\lambda I\|_{tr}=|\lambda|^2</span>, so <span class=""math-container"" id=""22220635"">0&lt;|\lambda|&lt;1</span> the left part inequality fails too. But for corrected norm <span class=""math-container"" id=""22220636"">\|L\|_{tr}=\sqrt{\sum_{i=1}^{n}\sum_{j=1}^{m}(a_{i,j})^2}</span> the inequality <span class=""math-container"" id=""22220637""> \|L\|_{op} \leq \|L\|_{tr}</span>  indeed follows from Cauchy-Schwartz inequality.</p> "	For the identity map  I      \|I\|_{op}=\sup \{|L(x)| : |x|=1 \}=\sup \{|x| : |x|=1 \}=1.     \|I\|_{tr} = \sum_{i=1}^{n}\sum_{j=1}^{n}(a_{i,j})^2= \sum_{i=1}^{n}\sum_{j=1}^{n}(\delta_{i,j})^2=n    (I recall that  \delta_{i,j}=1  if  i=j  and  0 , otherwise).    I guess that there should be taken a root from  \|L\|_{tr}  in order to make it norm, that is to assure that  \|\lambda L\|_{tr}=|\lambda|\|L\|_{tr}  for each real  \lambda , because in the present state we have  \|\lambda L\|_{tr}= |\lambda|^2\|L\|_{tr} . In particular,  \|\lambda I\|_{op}=|\lambda| , whereas  \|\lambda I\|_{tr}=|\lambda|^2 , so  0<|\lambda|<1  the left part inequality fails too. But for corrected norm  \|L\|_{tr}=\sqrt{\sum_{i=1}^{n}\sum_{j=1}^{m}(a_{i,j})^2}  the inequality   \|L\|_{op} \leq \|L\|_{tr}   indeed follows from Cauchy-Schwartz inequality.  	0.9616745114326477
3007382	"<p>Observe here you have <span class=""math-container"" id=""27854095"">\begin{align} \|A(x-y)\|_2 \leq \|A\|_\text{op}\|x-y\|_2 \end{align}</span> where <span class=""math-container"" id=""27854096"">\begin{align} \|A\|_\text{op} = \sup_{\|x\|_2=1}\|Ax\|_2 = \sigma_\text{max}(A) \leq \left(\sum^n_{i=1}\sum^n_{j=1}|a_{ij}|^2\right)^{1/2} =\sqrt{\frac{39}{50}}&lt;1 \end{align}</span> where <span class=""math-container"" id=""27854097"">\sigma_\text{max}(A)</span> denotes the largest singular value of <span class=""math-container"" id=""27854098"">A</span>. </p> "	Observe here you have  \begin{align} \|A(x-y)\|_2 \leq \|A\|_\text{op}\|x-y\|_2 \end{align}  where  \begin{align} \|A\|_\text{op} = \sup_{\|x\|_2=1}\|Ax\|_2 = \sigma_\text{max}(A) \leq \left(\sum^n_{i=1}\sum^n_{j=1}|a_{ij}|^2\right)^{1/2} =\sqrt{\frac{39}{50}}<1 \end{align}  where  \sigma_\text{max}(A)  denotes the largest singular value of  A .   	0.9615275859832764
1960574	"<p>If <span class=""math-container"" id=""18170812"">A</span> is positive definite then the eigenvalues of <span class=""math-container"" id=""18170813"">A</span> are all positive. In particular, the smallest eigenvalue, say <span class=""math-container"" id=""18170814"">\lambda_s</span>, will be nonzero. Next observe \begin{align} \frac{1}{2}x^TAx-b^Tx=\frac{1}{2}x^TQ^TDQx-b^Tx=\geq\frac{\lambda_s}{2}\| Qx\|^2-\|b\|\|x\|=\frac{\lambda_s}{2}\|x\|^2-\|b\|\|x\|. \end{align} Thus as <span class=""math-container"" id=""18170815"">\|x\|\rightarrow \infty</span> we see that <span class=""math-container"" id=""18170816"">f</span> will also go to infinity.</p>  <p>For the converse direction, assume <span class=""math-container"" id=""18170817"">A</span> is not positive definite then there is an eigen-direction corresponding to a non-positive eigenvalue, say <span class=""math-container"" id=""18170818"">x</span>. Then we see that if <span class=""math-container"" id=""18170819"">\lambda&lt;0</span> then \begin{align}  f(\alpha x)= \frac{\lambda\alpha^2}{2}\|x\|^2-\alpha b^Tx\leq \frac{\lambda\alpha^2}{2}\|x\|^2+|\alpha|\|b\|\| x\| \end{align} which goes to negative infinity as <span class=""math-container"" id=""18170820"">|\alpha|\rightarrow \infty</span>. For the case <span class=""math-container"" id=""18170821"">\lambda=0</span>, we will leave it as an exercise for the reader.</p> "	If  A  is positive definite then the eigenvalues of  A  are all positive. In particular, the smallest eigenvalue, say  \lambda_s , will be nonzero. Next observe \begin{align} \frac{1}{2}x^TAx-b^Tx=\frac{1}{2}x^TQ^TDQx-b^Tx=\geq\frac{\lambda_s}{2}\| Qx\|^2-\|b\|\|x\|=\frac{\lambda_s}{2}\|x\|^2-\|b\|\|x\|. \end{align} Thus as  \|x\|\rightarrow \infty  we see that  f  will also go to infinity.    For the converse direction, assume  A  is not positive definite then there is an eigen-direction corresponding to a non-positive eigenvalue, say  x . Then we see that if  \lambda<0  then \begin{align}  f(\alpha x)= \frac{\lambda\alpha^2}{2}\|x\|^2-\alpha b^Tx\leq \frac{\lambda\alpha^2}{2}\|x\|^2+|\alpha|\|b\|\| x\| \end{align} which goes to negative infinity as  |\alpha|\rightarrow \infty . For the case  \lambda=0 , we will leave it as an exercise for the reader.  	0.9615047574043274
2486906	"<p>I mistakenly thought <span class=""math-container"" id=""23009987"">\|\cdot\|</span> was the <span class=""math-container"" id=""23009988"">\ell^2</span>-norm. See below the line for the general situation.</p>  <p>Hint: <span class=""math-container"" id=""23009989"">\|Ax\|^2=\left\|\begin{bmatrix}\lambda_1 x_1 \\ \vdots \\ \lambda_n x_n\end{bmatrix}\right\|^2 = \lambda_1^2 x_1^2 + \cdots + \lambda_n^2 x_n^2 \le (\max_i \lambda_i^2) (x_1^2 + \cdots + x_n^2) = (\max_i \lambda_i^2) \|x\|^2.</span> By looking at the definition of <span class=""math-container"" id=""23009990"">\|A\|</span>, can you now compute <span class=""math-container"" id=""23009991"">\|A\|</span>? Computing <span class=""math-container"" id=""23009992"">\|A^{-1}\|</span> is similar, since it is also a diagonal matrix.</p>  <hr>  <p>General situation:</p>  <ol> <li>For any <a href=""https://en.wikipedia.org/wiki/Matrix_norm#Definition"" rel=""nofollow noreferrer"">[submultiplicative] matrix norm</a> <span class=""math-container"" id=""23009993"">\|\cdot\|</span>, we have <span class=""math-container"" id=""23009994"">\|A\| \ge \max_i |\lambda_i|</span>. (See below.)</li> <li>Since subordinate norms are submultiplicative matrix norms, this inequality holds in the setting of your question.</li> <li>Moreover, by considering <span class=""math-container"" id=""23009995"">x</span> being the standard basis vectors, we see that we actually have the equality <span class=""math-container"" id=""23009996"">\|A\| = \max_i |\lambda_i|</span>. Can you conclude from here?</li> </ol>  <p>Proof of Claim 1: Let <span class=""math-container"" id=""23009997"">\|\cdot \|</span> be a [submultiplicative] matrix norm. Let <span class=""math-container"" id=""23009998"">x</span> be a <span class=""math-container"" id=""23009999"">\lambda_i</span>-eigenvector, and let <span class=""math-container"" id=""23010000"">X</span> be the <span class=""math-container"" id=""23010001"">n \times n</span> matrix whose columns are all <span class=""math-container"" id=""23010002"">x</span>. Then <span class=""math-container"" id=""23010003"">|\lambda_i| \|X\| = \|\lambda_i X\| = \|A X\| \le \|A\| \|X\|.</span></p> "	I mistakenly thought  \|\cdot\|  was the  \ell^2 -norm. See below the line for the general situation.    Hint:  \|Ax\|^2=\left\|\begin{bmatrix}\lambda_1 x_1 \\ \vdots \\ \lambda_n x_n\end{bmatrix}\right\|^2 = \lambda_1^2 x_1^2 + \cdots + \lambda_n^2 x_n^2 \le (\max_i \lambda_i^2) (x_1^2 + \cdots + x_n^2) = (\max_i \lambda_i^2) \|x\|^2.  By looking at the definition of  \|A\| , can you now compute  \|A\| ? Computing  \|A^{-1}\|  is similar, since it is also a diagonal matrix.       General situation:      For any  [submultiplicative] matrix norm   \|\cdot\| , we have  \|A\| \ge \max_i |\lambda_i| . (See below.)   Since subordinate norms are submultiplicative matrix norms, this inequality holds in the setting of your question.   Moreover, by considering  x  being the standard basis vectors, we see that we actually have the equality  \|A\| = \max_i |\lambda_i| . Can you conclude from here?      Proof of Claim 1: Let  \|\cdot \|  be a [submultiplicative] matrix norm. Let  x  be a  \lambda_i -eigenvector, and let  X  be the  n \times n  matrix whose columns are all  x . Then  |\lambda_i| \|X\| = \|\lambda_i X\| = \|A X\| \le \|A\| \|X\|.  	0.9613463282585144
117974	"<p>You could of course generalize your current measure</p>  <p>\begin{align}  S(X) = \frac{\frac{k^{(1/m)}}{k^{(1/n)}} -\frac{\|X\|_m}{\|X\|_n} } {\frac{k^{(1/m)}}{k^{(1/n)}}-1} \end{align}</p>  <p>while preserving your properties you specified.</p>  <p>An interesting special case could be <span class=""math-container"" id=""1367416"">m = 1, n \to \infty</span>, in which case the expression simplifies to</p>  <p>\begin{equation}  S(X) = \frac{k-\frac{\|X\|_1}{\|X\|_c}}{k-1} \end{equation}</p>  <p>where <span class=""math-container"" id=""1367417"">c = \infty</span>, (for some reason, mathjax refused to render when I inserted <span class=""math-container"" id=""1367418"">\infty</span> directly in the fraction)</p> "	You could of course generalize your current measure    \begin{align}  S(X) = \frac{\frac{k^{(1/m)}}{k^{(1/n)}} -\frac{\|X\|_m}{\|X\|_n} } {\frac{k^{(1/m)}}{k^{(1/n)}}-1} \end{align}    while preserving your properties you specified.    An interesting special case could be  m = 1, n \to \infty , in which case the expression simplifies to    \begin{equation}  S(X) = \frac{k-\frac{\|X\|_1}{\|X\|_c}}{k-1} \end{equation}    where  c = \infty , (for some reason, mathjax refused to render when I inserted  \infty  directly in the fraction)  	0.9608859419822693
1255541	"<p>What you are doing is actually using Schwarz inequality to prove Schawarz inequality.</p>  <p>In the last step you need to use the Holder's Inequality as follows \begin{align} |\langle x,y\rangle|&amp; = \biggl{|}\sum_\limits{n=1}^{\infty}\langle x,e_1\rangle \overline{\langle y,e_i\rangle}\biggr{|}\\ &amp;\leq \sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|| \overline{\langle y,e_i\rangle}|\\ &amp;\leq \sqrt{\sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|^2} \sqrt{\sum_\limits{n=1}^{\infty}|\langle y,e_1\rangle|^2} &amp;&amp; \text{ (Holder's Inequality)}\\ &amp;\leq \|x\|\|y\| &amp;&amp; \text{(Bessel's Inequality)}. \end{align}</p>  <p><span class=""math-container"" id=""11713450"">\textbf{EDIT:}</span> Sorry for this not helping answer. An easy way out is as follows:</p>  <p>Let <span class=""math-container"" id=""11713451"">x\in X</span> and let <span class=""math-container"" id=""11713452"">y\neq 0</span> be given. Then just take <span class=""math-container"" id=""11713453"">e=\frac{y}{\|y\|}</span>. This <span class=""math-container"" id=""11713454"">\{e\}</span> only makes an orthonormal system. So, by Bessel's Inequality it will follow that <span class=""math-container"" id=""11713455"">|\langle x,e\rangle|^2 \leq \|x\|^2</span>. Now just multiply both sides by <span class=""math-container"" id=""11713456"">\|y\|</span> to get <span class=""math-container"" id=""11713457"">|\langle x,y\rangle|^2\leq \|x\|\|y\|</span>.</p> "	What you are doing is actually using Schwarz inequality to prove Schawarz inequality.    In the last step you need to use the Holder's Inequality as follows \begin{align} |\langle x,y\rangle|& = \biggl{|}\sum_\limits{n=1}^{\infty}\langle x,e_1\rangle \overline{\langle y,e_i\rangle}\biggr{|}\\ &\leq \sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|| \overline{\langle y,e_i\rangle}|\\ &\leq \sqrt{\sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|^2} \sqrt{\sum_\limits{n=1}^{\infty}|\langle y,e_1\rangle|^2} && \text{ (Holder's Inequality)}\\ &\leq \|x\|\|y\| && \text{(Bessel's Inequality)}. \end{align}    \textbf{EDIT:}  Sorry for this not helping answer. An easy way out is as follows:    Let  x\in X  and let  y\neq 0  be given. Then just take  e=\frac{y}{\|y\|} . This  \{e\}  only makes an orthonormal system. So, by Bessel's Inequality it will follow that  |\langle x,e\rangle|^2 \leq \|x\|^2 . Now just multiply both sides by  \|y\|  to get  |\langle x,y\rangle|^2\leq \|x\|\|y\| .  	0.9608449935913086
2150745	"<p>HINT: we must show that if <span class=""math-container"" id=""19876678"">q\neq p</span> then these norms are not equivalent, that is doesnt exists <span class=""math-container"" id=""19876679"">K\in(0,\infty)</span> such that</p>  <p><span class=""math-container"" id=""19876680"">K\|f\|_q\le\|f\|_p,\quad\forall f\in C(I,\Bbb K)</span></p>  <p>or equivalently</p>  <p><span class=""math-container"" id=""19876681"">\frac{\|f\|_q}{\|f\|_p}\le K^{-1}</span></p>  <p>for <span class=""math-container"" id=""19876682"">f\neq 0</span>. Choosing <span class=""math-container"" id=""19876683"">f(x):=a^x</span>, <span class=""math-container"" id=""19876684"">q=2</span> and <span class=""math-container"" id=""19876685"">p=1</span> and <span class=""math-container"" id=""19876686"">I:=[0,1]</span> we have that it must be true that</p>  <p><span class=""math-container"" id=""19876687"">\frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}\le K^{-1},\quad\forall a&gt;1</span></p>  <p>but the above is equivalent to say that</p>  <p><span class=""math-container"" id=""19876688"">\frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}=\frac{\sqrt{\frac{a^2-1}{2\ln a}}}{\frac{a-1}{\ln a}}=\sqrt{\frac{(a+1)\ln a}{(a-1)2}}\le K^{-1},\quad\forall a&gt;1</span></p>  <p>what cannot be possible, hence <span class=""math-container"" id=""19876689"">\|{\cdot}\|_2</span> and <span class=""math-container"" id=""19876690"">\|{\cdot}\|_1</span> are not equivalent. We can generalize this result easily for any <span class=""math-container"" id=""19876691"">q&gt;p</span> and any interval <span class=""math-container"" id=""19876692"">[\alpha,\beta]</span>.</p> "	HINT: we must show that if  q\neq p  then these norms are not equivalent, that is doesnt exists  K\in(0,\infty)  such that    K\|f\|_q\le\|f\|_p,\quad\forall f\in C(I,\Bbb K)    or equivalently    \frac{\|f\|_q}{\|f\|_p}\le K^{-1}    for  f\neq 0 . Choosing  f(x):=a^x ,  q=2  and  p=1  and  I:=[0,1]  we have that it must be true that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}\le K^{-1},\quad\forall a>1    but the above is equivalent to say that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}=\frac{\sqrt{\frac{a^2-1}{2\ln a}}}{\frac{a-1}{\ln a}}=\sqrt{\frac{(a+1)\ln a}{(a-1)2}}\le K^{-1},\quad\forall a>1    what cannot be possible, hence  \|{\cdot}\|_2  and  \|{\cdot}\|_1  are not equivalent. We can generalize this result easily for any  q>p  and any interval  [\alpha,\beta] .  	0.9605357050895691
2447640	"<p>It is valid but as noted the Gershgorin theorem is quite an overkill. A more elementary approach follows.</p>  <p>Let <span class=""math-container"" id=""22650638"">A=D-B</span>, where <span class=""math-container"" id=""22650639"">D</span> is the diagonal part of <span class=""math-container"" id=""22650640"">A</span>. Let <span class=""math-container"" id=""22650641"">A</span> be row diagonally dominant, that is, <span class=""math-container"" id=""22650642"">\tag{1} |a_{ii}|&gt;\sum_{j\neq i}|a_{ij}|\;\iff\; 1&gt;\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|, \quad i=1,\ldots,n. </span> The latter inequality is equivalent to <span class=""math-container"" id=""22650643"">\tag{2}1&gt;\|D^{-1}B\|_\infty.</span></p>  <p>If <span class=""math-container"" id=""22650644"">A</span> was singular, <span class=""math-container"" id=""22650645"">Ax=0</span> and hence <span class=""math-container"" id=""22650646"">x=D^{-1}Bx</span> for some nonzero <span class=""math-container"" id=""22650647"">x</span>. We would have <span class=""math-container"" id=""22650648""> \|x\|_\infty=\|D^{-1}Bx\|_\infty\leq\|D^{-1}B\|_\infty\|x\|_\infty </span> and, dividing by <span class=""math-container"" id=""22650649"">\|x\|_\infty</span>, <span class=""math-container"" id=""22650650""> 1\leq\|D^{-1}B\|_\infty, </span> which contradicts (2).</p> "	It is valid but as noted the Gershgorin theorem is quite an overkill. A more elementary approach follows.    Let  A=D-B , where  D  is the diagonal part of  A . Let  A  be row diagonally dominant, that is,  \tag{1} |a_{ii}|>\sum_{j\neq i}|a_{ij}|\;\iff\; 1>\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|, \quad i=1,\ldots,n.   The latter inequality is equivalent to  \tag{2}1>\|D^{-1}B\|_\infty.    If  A  was singular,  Ax=0  and hence  x=D^{-1}Bx  for some nonzero  x . We would have   \|x\|_\infty=\|D^{-1}Bx\|_\infty\leq\|D^{-1}B\|_\infty\|x\|_\infty   and, dividing by  \|x\|_\infty ,   1\leq\|D^{-1}B\|_\infty,   which contradicts (2).  	0.9605176448822021
3022186	"<p>As @Alonso Delfín mentioned, the desired conclusion is wrong. Actually this question is related to the so-called dual norm. By Hölder inequality, for <span class=""math-container"" id=""27990713"">a,x\in\mathbb{R}^n</span>, there holds <span class=""math-container"" id=""27990714""> \sum |a_ix_i|\leq\|a\|_{\ell^p}\|x\|_{\ell^q}, </span> where <span class=""math-container"" id=""27990715"">p^{-1}+q^{-1}=1</span> for <span class=""math-container"" id=""27990716"">1\leq p,q\leq\infty</span> and for each <span class=""math-container"" id=""27990717"">p,q</span> and <span class=""math-container"" id=""27990718"">a</span> there exists <span class=""math-container"" id=""27990719"">x</span> establishing the equality. Therefore we have <span class=""math-container"" id=""27990720""> \|\langle a,\cdot\rangle\|_{\ell^p}=\|a\|_{\ell^q}, </span> and that's why it is called the dual norm. As your case, it actually should be <span class=""math-container"" id=""27990721""> \|\langle a,\cdot\rangle\|_{\ell^\infty}=\|a\|_{\ell^1}. </span></p> "	As @Alonso Delfín mentioned, the desired conclusion is wrong. Actually this question is related to the so-called dual norm. By Hölder inequality, for  a,x\in\mathbb{R}^n , there holds   \sum |a_ix_i|\leq\|a\|_{\ell^p}\|x\|_{\ell^q},   where  p^{-1}+q^{-1}=1  for  1\leq p,q\leq\infty  and for each  p,q  and  a  there exists  x  establishing the equality. Therefore we have   \|\langle a,\cdot\rangle\|_{\ell^p}=\|a\|_{\ell^q},   and that's why it is called the dual norm. As your case, it actually should be   \|\langle a,\cdot\rangle\|_{\ell^\infty}=\|a\|_{\ell^1}.   	0.9604347348213196
2923042	"<p>Though not an answer to the question, I would like to mention that the inequality <span class=""math-container"" id=""27052857"">\|B\|_F \leq \|B\|_{\mathrm{op}}</span> is not correct, it should be <span class=""math-container"" id=""27052858"">\|B\|_{\mathrm{op}}\leq\|B\|_{F}</span>. A quick way to see this, is to use the equations <span class=""math-container"" id=""27052859""> \|B\|^2_{\mathrm{op}}  = \lambda_{\max}(B^TB)= \sigma^2_{\max}(B) </span> and <span class=""math-container"" id=""27052860""> \|B\|_F^2 = \operatorname{trace}(B^T B) = \sum_{i=1}^{\min\{m,n\}} \sigma^2_i (B),</span> where <span class=""math-container"" id=""27052861"">\lambda_i</span> and <span class=""math-container"" id=""27052862"">\sigma_i</span> stand for an eigenvalue and singular value respectively.</p>  <p>Reference: <a href=""https://en.wikipedia.org/wiki/Matrix_norm"" rel=""nofollow noreferrer"">""Matrix norm"" on Wikipedia</a>.</p> "	"Though not an answer to the question, I would like to mention that the inequality  \|B\|_F \leq \|B\|_{\mathrm{op}}  is not correct, it should be  \|B\|_{\mathrm{op}}\leq\|B\|_{F} . A quick way to see this, is to use the equations   \|B\|^2_{\mathrm{op}}  = \lambda_{\max}(B^TB)= \sigma^2_{\max}(B)   and   \|B\|_F^2 = \operatorname{trace}(B^T B) = \sum_{i=1}^{\min\{m,n\}} \sigma^2_i (B),  where  \lambda_i  and  \sigma_i  stand for an eigenvalue and singular value respectively.    Reference:  ""Matrix norm"" on Wikipedia .  "	0.9604169726371765
1186757	"<p>I'm assuming you are talking about functions on <span class=""math-container"" id=""11126095"">\mathbb{R}^n</span>.</p>  <p>So you are asking whether <span class=""math-container"" id=""11126096"">\|f*g\|_\infty\le \|f\|_1\|g\|_1</span></p>  <p>You can already see by scaling that this can't be true. Namely, replace <span class=""math-container"" id=""11126097"">f</span> by <span class=""math-container"" id=""11126098"">f_\lambda(x)=f(\lambda x)</span> and <span class=""math-container"" id=""11126099"">g</span> by <span class=""math-container"" id=""11126100"">g_\lambda</span> (<span class=""math-container"" id=""11126101"">\lambda&gt;0</span>). Also let's say that <span class=""math-container"" id=""11126102"">f,g</span> are such that <span class=""math-container"" id=""11126103"">\frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1</span> (just a normalization). We have</p>  <p><span class=""math-container"" id=""11126104"">\lambda^{-n} \|f*g\|_\infty=\|f_\lambda*g_\lambda\|_\infty \le \|f_\lambda\|_1 \|g_\lambda\|_1=\lambda^{-2n} \|f\|_1 \|g\|_1</span></p>  <p>Then, <span class=""math-container"" id=""11126105"">\lambda^n \le \frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1</span></p>  <p>For <span class=""math-container"" id=""11126106"">\lambda&gt;1</span> this is a contradiction!</p>  <p><strong>But</strong> <span class=""math-container"" id=""11126107"">\|f*g\|_\infty\le \|f\|_1 \|g\|_\infty</span> is true (simply pull out the <span class=""math-container"" id=""11126108"">L^\infty</span> norm).</p>  <p>In general, there is <a href=""http://en.wikipedia.org/wiki/Young%27s_inequality#Young.27s_inequality_for_convolutions"" rel=""nofollow"">Young's convolution inequality</a>:</p>  <p><span class=""math-container"" id=""11126109"">\|f*g\|_r\le \|f\|_p \|g\|_q</span></p>  <p>if <span class=""math-container"" id=""11126110"">1+\frac1r=\frac1p+\frac1q</span>, <span class=""math-container"" id=""11126111"">1\le p,q,r\le \infty</span>.</p> "	I'm assuming you are talking about functions on  \mathbb{R}^n .    So you are asking whether  \|f*g\|_\infty\le \|f\|_1\|g\|_1    You can already see by scaling that this can't be true. Namely, replace  f  by  f_\lambda(x)=f(\lambda x)  and  g  by  g_\lambda  ( \lambda>0 ). Also let's say that  f,g  are such that  \frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1  (just a normalization). We have    \lambda^{-n} \|f*g\|_\infty=\|f_\lambda*g_\lambda\|_\infty \le \|f_\lambda\|_1 \|g_\lambda\|_1=\lambda^{-2n} \|f\|_1 \|g\|_1    Then,  \lambda^n \le \frac{\|f\|_1  \|g\|_1}{\|f*g\|_\infty}=1    For  \lambda>1  this is a contradiction!    But   \|f*g\|_\infty\le \|f\|_1 \|g\|_\infty  is true (simply pull out the  L^\infty  norm).    In general, there is  Young's convolution inequality :    \|f*g\|_r\le \|f\|_p \|g\|_q    if  1+\frac1r=\frac1p+\frac1q ,  1\le p,q,r\le \infty .  	0.9601579904556274
1973095	"<p>You can also prove it by relating spectral radius with matrix norm. For a linear operator <span class=""math-container"" id=""18281027"">A</span>, <span class=""math-container"" id=""18281028"">\operatorname{spr}(A) \le \|A\|</span> where <span class=""math-container"" id=""18281029"">\begin{align} \|A\| &amp;= \underset{\|x\|=1}{\sup} \|Ax\| \\ \operatorname{spr}(A) &amp;= \max\left\lbrace |\lambda|: \lambda \text{ is an eigenvalue of } A \right\rbrace \end{align}</span>  The spectral radius <span class=""math-container"" id=""18281030"">\operatorname{spr}(A)</span> and <span class=""math-container"" id=""18281031"">\|A\|</span> are equal iff the operator <span class=""math-container"" id=""18281032"">A</span> is <a href=""https://en.wikipedia.org/wiki/Normal_matrix"" rel=""nofollow noreferrer"">normal</a>.</p>  <p>So <span class=""math-container"" id=""18281033"">\|A\| &lt; 1 \Rightarrow \operatorname{spr}(A) &lt; 1 \Rightarrow </span> all of the eigenvalues of <span class=""math-container"" id=""18281034"">A</span> have absolute value smaller than 1.</p>  <p>See Exercise I.2.6 of <a href=""http://www.springer.com/us/book/9780387948461"" rel=""nofollow noreferrer"">Rajendra Bhatia's Matrix Analysis</a> where he calls it the <a href=""https://en.wikipedia.org/wiki/Neumann_series"" rel=""nofollow noreferrer"">Neumann Series</a>. <a href=""https://i.stack.imgur.com/E7QQ6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E7QQ6.png"" alt=""enter image description here""></a></p> "	You can also prove it by relating spectral radius with matrix norm. For a linear operator  A ,  \operatorname{spr}(A) \le \|A\|  where  \begin{align} \|A\| &= \underset{\|x\|=1}{\sup} \|Ax\| \\ \operatorname{spr}(A) &= \max\left\lbrace |\lambda|: \lambda \text{ is an eigenvalue of } A \right\rbrace \end{align}   The spectral radius  \operatorname{spr}(A)  and  \|A\|  are equal iff the operator  A  is  normal .    So  \|A\| < 1 \Rightarrow \operatorname{spr}(A) < 1 \Rightarrow   all of the eigenvalues of  A  have absolute value smaller than 1.    See Exercise I.2.6 of  Rajendra Bhatia's Matrix Analysis  where he calls it the  Neumann Series .   	0.9599092602729797
507461	"<p>Notice that from <a href=""https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality"" rel=""nofollow"">Hölder's inequality</a>, valid for <span class=""math-container"" id=""5302982"">p \geq 1</span>, <span class=""math-container"" id=""5302983"">   \|f\|_1 = \|f \cdot 1\|_1 \leq \|f\|_p \|1\|_q = \|f\|_p, </span> where <span class=""math-container"" id=""5302984"">\frac{1}{p} + \frac{1}{q} = 1</span>, since <span class=""math-container"" id=""5302985"">\|1\|_q = 1</span>.</p>  <p>But this implies that the identity <span class=""math-container"" id=""5302986"">   \begin{array}{rrl}     \mathrm{id}: &amp;(C([0,1], \|\cdot\|_p) &amp;\rightarrow &amp;(C([0,1], \|\cdot\|_1)     \\                  &amp;x &amp;\mapsto &amp;x   \end{array} </span> is continuous. And since <span class=""math-container"" id=""5302987"">A = \mathrm{id}^{-1}(B(0,1))</span>, and <span class=""math-container"" id=""5302988"">B(0,1)</span> is open in the <span class=""math-container"" id=""5302989"">1</span>-norm, it follows that <span class=""math-container"" id=""5302990"">A</span> is open.</p>  <hr>  <p><b>Edit:</b> added observation from Pedro Tamaroff that Hölder is valid for <span class=""math-container"" id=""5302991"">p \geq 1</span>.</p> "	Notice that from  Hölder's inequality , valid for  p \geq 1 ,     \|f\|_1 = \|f \cdot 1\|_1 \leq \|f\|_p \|1\|_q = \|f\|_p,   where  \frac{1}{p} + \frac{1}{q} = 1 , since  \|1\|_q = 1 .    But this implies that the identity     \begin{array}{rrl}     \mathrm{id}: &(C([0,1], \|\cdot\|_p) &\rightarrow &(C([0,1], \|\cdot\|_1)     \\                  &x &\mapsto &x   \end{array}   is continuous. And since  A = \mathrm{id}^{-1}(B(0,1)) , and  B(0,1)  is open in the  1 -norm, it follows that  A  is open.       Edit:  added observation from Pedro Tamaroff that Hölder is valid for  p \geq 1 .  	0.9598352909088135
179439	"<p><em>This got too long for a comment:</em></p>  <p>Using the definition of the <a href=""http://en.wikipedia.org/wiki/Vector_norm#p-norm"" rel=""nofollow"">vecto <span class=""math-container"" id=""2084465"">p</span> norm</a> <span class=""math-container"" id=""2084466"">     \|\mathbf{x}\|_p := \bigg( \sum_{i=1}^n |x_i|^p \bigg)^{1/p}, </span> you can combine 3.) and 4.) like the following:</p>  <p>Let  <span class=""math-container"" id=""2084467""> \left \| A \right \| _p = \max \limits _{x \ne 0} \frac{\left \| A x\right \| _p}{\left \| x\right \| _p}.  </span> So you get back 3.) with <span class=""math-container"" id=""2084468"">p=2</span>. In the case of <span class=""math-container"" id=""2084469"">p=1</span> and <span class=""math-container"" id=""2084470"">p=\infty</span>, the norms can be computed as:</p>  <ul> <li><span class=""math-container"" id=""2084471""> \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} |, </span>  which is simply the maximum absolute column sum of the matrix.</li> <li><span class=""math-container"" id=""2084472""> \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} |, </span>  which is simply the maximum absolute row sum of the matrix</li> </ul>  <p>Here I'm not sure, which of both you mean, but none of them is a vector as already pointed out in the comments (taken from <a href=""http://en.wikipedia.org/wiki/Matrix_norm#Induced_norm"" rel=""nofollow"">Matrix norm/Induced_norm</a>, which also provides some examples, that might help with the interpretation).</p> "	This got too long for a comment:    Using the definition of the  vecto  p  norm        \|\mathbf{x}\|_p := \bigg( \sum_{i=1}^n |x_i|^p \bigg)^{1/p},   you can combine 3.) and 4.) like the following:    Let    \left \| A \right \| _p = \max \limits _{x \ne 0} \frac{\left \| A x\right \| _p}{\left \| x\right \| _p}.    So you get back 3.) with  p=2 . In the case of  p=1  and  p=\infty , the norms can be computed as:       \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} |,    which is simply the maximum absolute column sum of the matrix.    \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} |,    which is simply the maximum absolute row sum of the matrix      Here I'm not sure, which of both you mean, but none of them is a vector as already pointed out in the comments (taken from  Matrix norm/Induced_norm , which also provides some examples, that might help with the interpretation).  	0.9598090648651123
3054620	"<p>To show that <span class=""math-container"" id=""18238682"">f \in L^p(\mathbb{R}^n)</span> is in fact a tempered distribution, we need to verify whether <span class=""math-container"" id=""18238683"">f</span> acting on Schwatz class test functions produces a finite number, i.e. for <span class=""math-container"" id=""18238684"">\phi \in \mathcal{S}(\mathbb{R}^n)</span> <span class=""math-container"" id=""18238685"">\begin{equation}   |\langle f,\phi \rangle | &lt; +\infty \end{equation}</span> To see this, observe that by Schwartz inequality,</p>  <p><span class=""math-container"" id=""18238686"">\begin{eqnarray}  |\langle f,\phi \rangle |&amp;=&amp; \bigg|\int f \phi \bigg|      \nonumber \\                          &amp;\leq&amp;  \int \big|f \phi \big|  \nonumber \\                          &amp;\le&amp; \bigg(\int \big|f \big|^p\bigg)^{1/p} \bigg(\int                              \big| \phi \big|^q\bigg)^{1/q} \text{ where } \frac{1}{p} + \frac{1}{q}=1\\                          &amp;=&amp; C_f || \phi||_{L^q} \text{ where } C_f=||f||_{L^p}\\                          &amp;&lt;&amp; +\infty \text{ as } \phi \in L^q \end{eqnarray}</span></p> "	To show that  f \in L^p(\mathbb{R}^n)  is in fact a tempered distribution, we need to verify whether  f  acting on Schwatz class test functions produces a finite number, i.e. for  \phi \in \mathcal{S}(\mathbb{R}^n)   \begin{equation}   |\langle f,\phi \rangle | < +\infty \end{equation}  To see this, observe that by Schwartz inequality,    \begin{eqnarray}  |\langle f,\phi \rangle |&=& \bigg|\int f \phi \bigg|      \nonumber \\                          &\leq&  \int \big|f \phi \big|  \nonumber \\                          &\le& \bigg(\int \big|f \big|^p\bigg)^{1/p} \bigg(\int                              \big| \phi \big|^q\bigg)^{1/q} \text{ where } \frac{1}{p} + \frac{1}{q}=1\\                          &=& C_f || \phi||_{L^q} \text{ where } C_f=||f||_{L^p}\\                          &<& +\infty \text{ as } \phi \in L^q \end{eqnarray}  	0.9597285985946655
2700684	"<p>We start with <span class=""math-container"" id=""24930981"">\|ax-by\|^2</span> since it is a non-negative number, with the hope to derive the Cauchy-Schwarz inequality from a trivial inequality. For instance, by just picking <span class=""math-container"" id=""24930982"">a</span> and <span class=""math-container"" id=""24930983"">b</span> as <span class=""math-container"" id=""24930984"">1</span> we have <span class=""math-container"" id=""24930985"">0 \leq \|x-y\|^2 = \|x\|^2+\|y\|^2 -2\langle x,y\rangle \tag{1}</span> from which it follows that <span class=""math-container"" id=""24930986""> \langle x,y\rangle \leq \frac{\|x\|^2+\|y\|^2}{2}\qquad\text{(trivial inequality)}.\tag{2}</span> Now we may exploit a symmetry: if we pick some <span class=""math-container"" id=""24930987"">\lambda&gt;0</span> and replace <span class=""math-container"" id=""24930988"">x</span> by <span class=""math-container"" id=""24930989"">\lambda x</span> and <span class=""math-container"" id=""24930990"">y</span> by <span class=""math-container"" id=""24930991"">\frac{1}{\lambda}y</span> the LHS of <span class=""math-container"" id=""24930992"">(2)</span> is unchanged. In particular <span class=""math-container"" id=""24930993""> \forall \lambda&gt;0,\qquad\langle x,y\rangle \leq \frac{\lambda^2\|x\|^2+\frac{1}{\lambda^2}\|y\|^2}{2}\qquad\text{(less trivial inequality)}.\tag{3}</span> Given some <span class=""math-container"" id=""24930994"">\|x\|,\|y\|&gt;0</span>, we may pick <span class=""math-container"" id=""24930995"">\lambda&gt;0</span> such that the RHS of <span class=""math-container"" id=""24930996"">(3)</span> is minimal.<br> By the AM-GM inequality the RHS of <span class=""math-container"" id=""24930997"">(3)</span> is <span class=""math-container"" id=""24930998"">\geq\|x\|\|y\|</span>, but equality is attained if <span class=""math-container"" id=""24930999"">\lambda=\sqrt{\frac{\|y\|}{\|x\|}}</span>. <span class=""math-container"" id=""24931000""> \langle x,y\rangle \leq \|x\|\|y\| \tag{4} </span> Is the Cauchy-Schwarz inequality. By performing a backtracking  in the outlined proof we have that equality holds iff <span class=""math-container"" id=""24931001"">x,y</span> are linearly dependent. We also have that <span class=""math-container"" id=""24931002"">(4)</span> holds for <em>any</em> positive definite inner product <span class=""math-container"" id=""24931003"">\langle \cdot,\cdot\rangle</span>, where <span class=""math-container"" id=""24931004"">\|x\|</span> is defined through <span class=""math-container"" id=""24931005"">\|x\|^2=\langle x,x\rangle</span>.</p> "	We start with  \|ax-by\|^2  since it is a non-negative number, with the hope to derive the Cauchy-Schwarz inequality from a trivial inequality. For instance, by just picking  a  and  b  as  1  we have  0 \leq \|x-y\|^2 = \|x\|^2+\|y\|^2 -2\langle x,y\rangle \tag{1}  from which it follows that   \langle x,y\rangle \leq \frac{\|x\|^2+\|y\|^2}{2}\qquad\text{(trivial inequality)}.\tag{2}  Now we may exploit a symmetry: if we pick some  \lambda>0  and replace  x  by  \lambda x  and  y  by  \frac{1}{\lambda}y  the LHS of  (2)  is unchanged. In particular   \forall \lambda>0,\qquad\langle x,y\rangle \leq \frac{\lambda^2\|x\|^2+\frac{1}{\lambda^2}\|y\|^2}{2}\qquad\text{(less trivial inequality)}.\tag{3}  Given some  \|x\|,\|y\|>0 , we may pick  \lambda>0  such that the RHS of  (3)  is minimal.  By the AM-GM inequality the RHS of  (3)  is  \geq\|x\|\|y\| , but equality is attained if  \lambda=\sqrt{\frac{\|y\|}{\|x\|}} .   \langle x,y\rangle \leq \|x\|\|y\| \tag{4}   Is the Cauchy-Schwarz inequality. By performing a backtracking  in the outlined proof we have that equality holds iff  x,y  are linearly dependent. We also have that  (4)  holds for  any  positive definite inner product  \langle \cdot,\cdot\rangle , where  \|x\|  is defined through  \|x\|^2=\langle x,x\rangle .  	0.9595434665679932
2839988	"<p>Suppose <span class=""math-container"" id=""26234541""> A, B \in \mathbb{R}^{n \times n}</span> then we know the singular value decomposition exists for both of these. So, the following is true.</p>  <p><span class=""math-container"" id=""26234542""> A = U \Sigma V^{T}  </span> </p>  <p>with <span class=""math-container"" id=""26234543""> U,V, \Sigma \in \mathbb{R}^{n \times n} </span> then we know <span class=""math-container"" id=""26234544"">\| A \|_{F} \leq   \|U \| \|\Sigma\| \|V^{T} \|  </span> but <span class=""math-container"" id=""26234545"">U, V </span> are orthogonal which means their norm is 1. So, we have  <span class=""math-container"" id=""26234546""> \|A \|_{F}  \leq \|\Sigma\|_{F} </span>  and <span class=""math-container"" id=""26234547""> \Sigma </span> is diagonal matrix. The norm of <span class=""math-container"" id=""26234548""> \Sigma </span> is <span class=""math-container"" id=""26234549""> \| S \|_{F} = max_{i} |\sigma_{i}| </span> that is it is the maximum singular value which is the top entry incidently.</p>  <p>So, you have some bounds. From here then </p>  <p><span class=""math-container"" id=""26234550""> \|A+B\| \leq \| A \| +\|B\|  </span> <span class=""math-container"" id=""26234551"">  \sigma_{A+B} \leq \sigma_{A} + \sigma_{B} </span></p> "	Suppose   A, B \in \mathbb{R}^{n \times n}  then we know the singular value decomposition exists for both of these. So, the following is true.     A = U \Sigma V^{T}        with   U,V, \Sigma \in \mathbb{R}^{n \times n}   then we know  \| A \|_{F} \leq   \|U \| \|\Sigma\| \|V^{T} \|    but  U, V   are orthogonal which means their norm is 1. So, we have    \|A \|_{F}  \leq \|\Sigma\|_{F}    and   \Sigma   is diagonal matrix. The norm of   \Sigma   is   \| S \|_{F} = max_{i} |\sigma_{i}|   that is it is the maximum singular value which is the top entry incidently.    So, you have some bounds. From here then      \|A+B\| \leq \| A \| +\|B\|       \sigma_{A+B} \leq \sigma_{A} + \sigma_{B}   	0.9594707489013672
2352027	"<p>As pointed out in another answer, the stated inequality is false. A simple example is given by the diagonal matrix <span class=""math-container"" id=""18431528"">A=\operatorname{diag}(1,-1)</span>, where <span class=""math-container"" id=""18431529"">\operatorname{tr}(A^2)=2&gt;\operatorname{tr}(A)^2=0</span>.</p>  <p>If <span class=""math-container"" id=""18431530"">A</span> is positive semidefinite, by diagonalising it, the inequality reduces to <span class=""math-container"" id=""18431531""> \left(\sum\lambda_i^m\right)^{1/m}\ge\left(\sum\lambda_i^{m+1}\right)^{1/{m+1}}, </span> where <span class=""math-container"" id=""18431532"">\lambda_1,\ldots,\lambda_n\ge0</span> are the eigenvalues of <span class=""math-container"" id=""18431533"">A</span>, which is indeed true.</p>  <p>Alternatively, if you use singular values instead, the following is true for every symmetric <span class=""math-container"" id=""18431534"">A</span>: <span class=""math-container"" id=""18431535""> \left(\sum\sigma_i^m\right)^{1/m}\ge\left(\sum\sigma_i^{m+1}\right)^{1/{m+1}}. </span> Both of these are special cases of the fact that <span class=""math-container"" id=""18431536"">\|\cdot\|_p\ge\|\cdot\|_q</span> when <span class=""math-container"" id=""18431537"">0&lt;p&lt;q</span>. Proofs of this fact are abundant on this site. See, the following threads, for instances:</p>  <ul> <li><a href=""https://math.stackexchange.com/questions/4094/how-do-you-show-that-l-p-subset-l-q-for-p-leq-q"">How do you show that <span class=""math-container"" id=""18431538"">l_p \subset l_q</span> for <span class=""math-container"" id=""18431539"">p \leq q</span>?</a></li> <li><a href=""https://math.stackexchange.com/questions/367899/q-norm-leq-p-norm""><span class=""math-container"" id=""18431540"">q</span>-norm <span class=""math-container"" id=""18431541"">\leq</span> <span class=""math-container"" id=""18431542"">p</span>-norm</a></li> </ul> "	As pointed out in another answer, the stated inequality is false. A simple example is given by the diagonal matrix  A=\operatorname{diag}(1,-1) , where  \operatorname{tr}(A^2)=2>\operatorname{tr}(A)^2=0 .    If  A  is positive semidefinite, by diagonalising it, the inequality reduces to   \left(\sum\lambda_i^m\right)^{1/m}\ge\left(\sum\lambda_i^{m+1}\right)^{1/{m+1}},   where  \lambda_1,\ldots,\lambda_n\ge0  are the eigenvalues of  A , which is indeed true.    Alternatively, if you use singular values instead, the following is true for every symmetric  A :   \left(\sum\sigma_i^m\right)^{1/m}\ge\left(\sum\sigma_i^{m+1}\right)^{1/{m+1}}.   Both of these are special cases of the fact that  \|\cdot\|_p\ge\|\cdot\|_q  when  0	0.959385335445404
2650536	"<p>Since <span class=""math-container"" id=""24493198"">A</span> is real and symmetric, there is an orthogonal <span class=""math-container"" id=""24493199"">U</span> such that <span class=""math-container"" id=""24493200"">U^T A U = \Lambda = \operatorname{diag}(\lambda_1,..., \lambda_n)</span>.</p>  <p>Since <span class=""math-container"" id=""24493201"">A &gt;0</span> then <span class=""math-container"" id=""24493202"">\lambda_n &gt; 0</span>. </p>  <p>Let <span class=""math-container"" id=""24493203"">\sqrt{\Lambda} = \operatorname{diag}(\sqrt{\lambda_1,}..., \sqrt{\lambda_n)}</span>, then we can write  <span class=""math-container"" id=""24493204"">\|x\|_A = \| \sqrt{\Lambda} U^Tx \|_2</span> and since <span class=""math-container"" id=""24493205""> \sqrt{\Lambda} U^T</span> is invertible, all the properties follow from those of the Euclidean norm.</p>  <p>Note that <span class=""math-container"" id=""24493206"">\lambda_n \|x\|_2^2 \le \langle x, Ax \rangle \le \lambda_1 \|x\|_2^2</span> (assuming the eigenvalues are ordered), so we see that <span class=""math-container"" id=""24493207"">\sqrt{\lambda_n} \|x\|_2 \le \|x\|_A  \le \sqrt{\lambda_1} \|x\|_2</span></p> "	Since  A  is real and symmetric, there is an orthogonal  U  such that  U^T A U = \Lambda = \operatorname{diag}(\lambda_1,..., \lambda_n) .    Since  A >0  then  \lambda_n > 0 .     Let  \sqrt{\Lambda} = \operatorname{diag}(\sqrt{\lambda_1,}..., \sqrt{\lambda_n)} , then we can write   \|x\|_A = \| \sqrt{\Lambda} U^Tx \|_2  and since   \sqrt{\Lambda} U^T  is invertible, all the properties follow from those of the Euclidean norm.    Note that  \lambda_n \|x\|_2^2 \le \langle x, Ax \rangle \le \lambda_1 \|x\|_2^2  (assuming the eigenvalues are ordered), so we see that  \sqrt{\lambda_n} \|x\|_2 \le \|x\|_A  \le \sqrt{\lambda_1} \|x\|_2  	0.9593387246131897
17657	"<p>For <span class=""math-container"" id=""170099"">p \geq 1</span> the generalized mean defines a norm, because it is the <span class=""math-container"" id=""170100"">\ell^{p}</span>-norm only up to a factor <span class=""math-container"" id=""170101"">\sqrt[p]{n}</span>.</p>  <p>However, if <span class=""math-container"" id=""170102"">p \lt 1</span>, the generalized mean (and also the <span class=""math-container"" id=""170103"">\ell^{p}</span>-expression) don't define a norm because the set <span class=""math-container"" id=""170104"">\|x\|_{p} \leq 1</span> is not convex: if <span class=""math-container"" id=""170105"">x_{i} \geq 0</span> and <span class=""math-container"" id=""170106"">y_{i} \geq 0</span> for all <span class=""math-container"" id=""170107"">i</span> then <span class=""math-container"" id=""170108"">\|x + y\|_{p} \geq \|x\|_{p} + \|y\|_{p}</span>!</p>  <p>If <span class=""math-container"" id=""170109"">p \leq q</span> then <span class=""math-container"" id=""170110"">\|x\|_{q} \leq \|x\|_{p}</span>. To see this, note that both sides of the inequality are invariant by multiplication with a positive real number, so we may take without loss of generality an <span class=""math-container"" id=""170111"">x</span> with <span class=""math-container"" id=""170112"">\|x\|_{p} = 1</span>. Then <span class=""math-container"" id=""170113"">\|x\|_{q}^{q} = \sum_{j = 1}^{n} |x_{j}|^{q} \leq \sum_{j = 1}^{n} |x_{j}|^{p} = 1</span> this is because for <span class=""math-container"" id=""170114"">t \leq 1</span> and <span class=""math-container"" id=""170115"">p \leq q</span> we have <span class=""math-container"" id=""170116"">t^{q} \leq  t^{p}</span>.</p>  <p>I don't understand what you ask about the sequence space.</p> "	For  p \geq 1  the generalized mean defines a norm, because it is the  \ell^{p} -norm only up to a factor  \sqrt[p]{n} .    However, if  p \lt 1 , the generalized mean (and also the  \ell^{p} -expression) don't define a norm because the set  \|x\|_{p} \leq 1  is not convex: if  x_{i} \geq 0  and  y_{i} \geq 0  for all  i  then  \|x + y\|_{p} \geq \|x\|_{p} + \|y\|_{p} !    If  p \leq q  then  \|x\|_{q} \leq \|x\|_{p} . To see this, note that both sides of the inequality are invariant by multiplication with a positive real number, so we may take without loss of generality an  x  with  \|x\|_{p} = 1 . Then  \|x\|_{q}^{q} = \sum_{j = 1}^{n} |x_{j}|^{q} \leq \sum_{j = 1}^{n} |x_{j}|^{p} = 1  this is because for  t \leq 1  and  p \leq q  we have  t^{q} \leq  t^{p} .    I don't understand what you ask about the sequence space.  	0.9593386650085449
2574150	"<p>Your estimate is not naive in general. With <span class=""math-container"" id=""23795472"">p=1</span>, <span class=""math-container"" id=""23795473"">q=2</span>, take  <span class=""math-container"" id=""23795474""> A=\begin{bmatrix}  1&amp;0&amp;\cdots&amp;0\\  1&amp;0&amp;\cdots&amp;0\\  \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ 1&amp;0&amp;\cdots&amp;0\\  \end{bmatrix}, </span> It is well-known that  <span class=""math-container"" id=""23795475"">\|A\|_1=\max\{\|A_j\|_1:\ j\},\ \ \ \|A\|_2=\|A^*A\|_2^{1/2}=\max\sigma(A^*A)^{1/2},</span> where <span class=""math-container"" id=""23795476"">A_j</span> denotes the <span class=""math-container"" id=""23795477"">j^{\rm th}</span> column of <span class=""math-container"" id=""23795478"">A</span>, and <span class=""math-container"" id=""23795479"">\sigma(B)</span> is the <em>spectrum</em> (i.e., the list of eigenvalues). </p>  <p>Thus <span class=""math-container"" id=""23795480"">\|A\|_1=n</span>, while <span class=""math-container"" id=""23795481"">\|A\|_2=\|A^*A\|_2^{1/2}=\left\|\begin{bmatrix} n&amp;0&amp;\cdots&amp;0\\0&amp;0&amp;\cdots&amp;0\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ 0&amp;0&amp;\cdots&amp;0\end{bmatrix}\right\|^{1/2}_2=n^{1/2}.  </span> So <span class=""math-container"" id=""23795482""> \|A\|_1=n^{1/1-1/2}\,\|A\|_2. </span></p> "	Your estimate is not naive in general. With  p=1 ,  q=2 , take    A=\begin{bmatrix}  1&0&\cdots&0\\  1&0&\cdots&0\\  \vdots&\vdots&\ddots&\vdots\\ 1&0&\cdots&0\\  \end{bmatrix},   It is well-known that   \|A\|_1=\max\{\|A_j\|_1:\ j\},\ \ \ \|A\|_2=\|A^*A\|_2^{1/2}=\max\sigma(A^*A)^{1/2},  where  A_j  denotes the  j^{\rm th}  column of  A , and  \sigma(B)  is the  spectrum  (i.e., the list of eigenvalues).     Thus  \|A\|_1=n , while  \|A\|_2=\|A^*A\|_2^{1/2}=\left\|\begin{bmatrix} n&0&\cdots&0\\0&0&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&0\end{bmatrix}\right\|^{1/2}_2=n^{1/2}.    So   \|A\|_1=n^{1/1-1/2}\,\|A\|_2.   	0.9591347575187683
1801517	"<p>I have question about the same proof. In the book it mentioned </p>  <p>""Set <span class=""math-container"" id=""11396062"">σ_{1}=\|A\|_{2}</span>.By a compactness argument, there must be vectors <span class=""math-container"" id=""11396063"">v_{1} \in C^n</span> and <span class=""math-container"" id=""11396064"">u_{1} \in C^{m}</span> with <span class=""math-container"" id=""11396065"">\|v_{1}\|_{2}=\|u_{1}\|_{2}=1</span> and <span class=""math-container"" id=""11396066"">Av_{1}=σ_{1}</span>.""  Since it was defined earlier that <span class=""math-container"" id=""11396067"">\|A\|_{2}=sup_{\|v\|_{2}=1} \|Av\|_{2}</span> and  <span class=""math-container"" id=""11396068"">σ_{1}\geqslantσ_{2}\geqslant…</span> , as semi-axis principle, I can follow why <span class=""math-container"" id=""11396069"">σ_{1}=\|A\|_{2}</span> but how it came to the conclusion that there is <span class=""math-container"" id=""11396070"">u_{1}\in C^{m}</span> with <span class=""math-container"" id=""11396071"">\|u_{1}\|_{2}=1</span> and <span class=""math-container"" id=""11396072"">Av_{1}=σ_{1}u_1</span>.</p> "	"I have question about the same proof. In the book it mentioned     ""Set  σ_{1}=\|A\|_{2} .By a compactness argument, there must be vectors  v_{1} \in C^n  and  u_{1} \in C^{m}  with  \|v_{1}\|_{2}=\|u_{1}\|_{2}=1  and  Av_{1}=σ_{1} .""  Since it was defined earlier that  \|A\|_{2}=sup_{\|v\|_{2}=1} \|Av\|_{2}  and   σ_{1}\geqslantσ_{2}\geqslant…  , as semi-axis principle, I can follow why  σ_{1}=\|A\|_{2}  but how it came to the conclusion that there is  u_{1}\in C^{m}  with  \|u_{1}\|_{2}=1  and  Av_{1}=σ_{1}u_1 .  "	0.9591052532196045
1223695	"<p>Do you remember how we can prove Cauchy-Schwarz through interpolation? </p>  <p>Since <span class=""math-container"" id=""11441590"">|xy|\leq\frac{x^2+y^2}{2}</span> we have: <span class=""math-container"" id=""11441591""> \| f\cdot g\|_1 \leq \frac{\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2} \tag{1} </span> but the LHS is just the same if we replace <span class=""math-container"" id=""11441592"">f</span> with <span class=""math-container"" id=""11441593"">\lambda f</span> and <span class=""math-container"" id=""11441594"">g</span> with <span class=""math-container"" id=""11441595"">\frac{1}{\lambda}g</span>, so: <span class=""math-container"" id=""11441596""> \| f\cdot g\|_1 \leq \frac{\lambda^2\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2\lambda^2} \tag{2} </span> and by choosing <span class=""math-container"" id=""11441597"">\lambda</span> in such a way the two terms in the RHS of <span class=""math-container"" id=""11441598"">(2)</span> are equal, i.e. <span class=""math-container"" id=""11441599"">\lambda=\sqrt{\frac{\|g\|_2}{\|f\|_2}}</span>, we get: <span class=""math-container"" id=""11441600""> \| f\cdot g\|_1 \leq \|f\|_2 \cdot \|g\|_2 \tag{3}</span> that is the usual Cauchy-Schwarz inequality.  If we start with the Young inequality: <span class=""math-container"" id=""11441601""> |xy|\leq \frac{|x|^p}{p}+\frac{|y|^q}{q}\tag{4} </span> and follow exactly the same interpolation steps, we end with: <span class=""math-container"" id=""11441602""> \| f\cdot g\|_1 \leq \|f\|_p\cdot \|g\|_q \tag{5} </span> that is the wanted Holder's inequality.</p> "	Do you remember how we can prove Cauchy-Schwarz through interpolation?     Since  |xy|\leq\frac{x^2+y^2}{2}  we have:   \| f\cdot g\|_1 \leq \frac{\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2} \tag{1}   but the LHS is just the same if we replace  f  with  \lambda f  and  g  with  \frac{1}{\lambda}g , so:   \| f\cdot g\|_1 \leq \frac{\lambda^2\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2\lambda^2} \tag{2}   and by choosing  \lambda  in such a way the two terms in the RHS of  (2)  are equal, i.e.  \lambda=\sqrt{\frac{\|g\|_2}{\|f\|_2}} , we get:   \| f\cdot g\|_1 \leq \|f\|_2 \cdot \|g\|_2 \tag{3}  that is the usual Cauchy-Schwarz inequality.  If we start with the Young inequality:   |xy|\leq \frac{|x|^p}{p}+\frac{|y|^q}{q}\tag{4}   and follow exactly the same interpolation steps, we end with:   \| f\cdot g\|_1 \leq \|f\|_p\cdot \|g\|_q \tag{5}   that is the wanted Holder's inequality.  	0.9590164422988892
725065	"<p>As with most things the proof isn't tricky when you know how!</p>  <p>Define two norms for <span class=""math-container"" id=""7226237"">x \in \mathbb{R^n}</span> as follows:</p>  <p><span class=""math-container"" id=""7226238""> \|x\|_1 := \sum_{i=1}^n |x_i|</span></p>  <p>and </p>  <p><span class=""math-container"" id=""7226239""> \|x\|_2 := (\sum_{i=1}^n |x_i|^2)^{1/2}.</span></p>  <p>For <span class=""math-container"" id=""7226240"">x,y \in \mathbb{R^n}</span> I will define <span class=""math-container"" id=""7226241"">xy:= x \cdot y</span> as a convenient short hand. </p>  <p>To prove your required inequality, it suffices (by the triangle inequality) to show that </p>  <p><span class=""math-container"" id=""7226242"">\forall x,y \in \mathbb{R^n} \ \ \|xy\|_1 \leq \|x\|_2\|y\|_2.</span></p>  <p>To this end, note that </p>  <p><span class=""math-container"" id=""7226243"">\forall a,b \in \mathbb{R}_{+} \quad 0 \leq \|(ax+by)^2\|_1 = a^2\|x^2\|_1 + 2ab\|xy\|_1 + b^2\|y^2\|_1.</span></p>  <p>Dividing through by <span class=""math-container"" id=""7226244"">b^2</span>, and setting <span class=""math-container"" id=""7226245"">\lambda:= a/b</span> results in this inequality:</p>  <p><span class=""math-container"" id=""7226246"">\forall \lambda&gt;0 \quad 0 \leq \lambda^2\|x^2\|_1 + 2\lambda\|xy\|_1 + b^2\|y^2\|_1.</span></p>  <p>We can conclude that there are no positive real roots of this quadratic in <span class=""math-container"" id=""7226247"">\lambda</span>. Therefore, we know that the determinant must be non-positive, i.e.</p>  <p><span class=""math-container"" id=""7226248""> (2\cdot \|xy\|_1)^2 \leq 4\|x^2\|_1 \|y^2\|_1.</span></p>  <p>Dividing both sides by <span class=""math-container"" id=""7226249"">4</span> and taking square roots gives:</p>  <p><span class=""math-container"" id=""7226250"">\|xy\|_1 \leq \|x^2\|_1^{1/2} \|y^2\|_1^{1/2} = \|x\|_2\|y\|_2,</span> </p>  <p>as required. </p> "	As with most things the proof isn't tricky when you know how!    Define two norms for  x \in \mathbb{R^n}  as follows:     \|x\|_1 := \sum_{i=1}^n |x_i|    and      \|x\|_2 := (\sum_{i=1}^n |x_i|^2)^{1/2}.    For  x,y \in \mathbb{R^n}  I will define  xy:= x \cdot y  as a convenient short hand.     To prove your required inequality, it suffices (by the triangle inequality) to show that     \forall x,y \in \mathbb{R^n} \ \ \|xy\|_1 \leq \|x\|_2\|y\|_2.    To this end, note that     \forall a,b \in \mathbb{R}_{+} \quad 0 \leq \|(ax+by)^2\|_1 = a^2\|x^2\|_1 + 2ab\|xy\|_1 + b^2\|y^2\|_1.    Dividing through by  b^2 , and setting  \lambda:= a/b  results in this inequality:    \forall \lambda>0 \quad 0 \leq \lambda^2\|x^2\|_1 + 2\lambda\|xy\|_1 + b^2\|y^2\|_1.    We can conclude that there are no positive real roots of this quadratic in  \lambda . Therefore, we know that the determinant must be non-positive, i.e.     (2\cdot \|xy\|_1)^2 \leq 4\|x^2\|_1 \|y^2\|_1.    Dividing both sides by  4  and taking square roots gives:    \|xy\|_1 \leq \|x^2\|_1^{1/2} \|y^2\|_1^{1/2} = \|x\|_2\|y\|_2,      as required.   	0.9589784741401672
1568216	"<p>This is an example of the <a href=""https://en.wikipedia.org/wiki/Cauchy-Schwarz_Inequality"" rel=""noreferrer"">Cauchy-Schwarz inequality</a>:</p>  <p>\begin{align*} \|u\|_1 &amp;= \sum_{i = 1}^n |u_i|\cdot 1 \\ &amp;\le \left(\sum_{i = 1}^n |u_i|\right)^{1/2} \left(\sum_{i = 1}^n 1 \right)^{1/2} \\ &amp;= \|u\|_2 \sqrt{n} \end{align*}</p>  <p>To improve the result to <span class=""math-container"" id=""14484140"">\sqrt{q}</span>, use a mix of <span class=""math-container"" id=""14484141"">1</span>'s and <span class=""math-container"" id=""14484142"">0</span>'s rather than a constant sequence <span class=""math-container"" id=""14484143"">1</span> in the second sum. I'll leave it to you to work out the details.</p> "	This is an example of the  Cauchy-Schwarz inequality :    \begin{align*} \|u\|_1 &= \sum_{i = 1}^n |u_i|\cdot 1 \\ &\le \left(\sum_{i = 1}^n |u_i|\right)^{1/2} \left(\sum_{i = 1}^n 1 \right)^{1/2} \\ &= \|u\|_2 \sqrt{n} \end{align*}    To improve the result to  \sqrt{q} , use a mix of  1 's and  0 's rather than a constant sequence  1  in the second sum. I'll leave it to you to work out the details.  	0.9589348435401917
586369	"<p>Note that <span class=""math-container"" id=""6005499"">\rho(A)\le\|A\|</span> for every matrix norm, and <span class=""math-container"" id=""6005500"">\|A\|_b:=\|S^{-1}AS\|_a</span> is a matrix norm whenever <span class=""math-container"" id=""6005501"">\|\cdot\|_a</span> is a matrix norm and <span class=""math-container"" id=""6005502"">S</span> is nonsignular. By combining known matrix norms with similarity transform, one can always obtain a sufficient condition for <span class=""math-container"" id=""6005503"">\rho(A)&lt;1</span> in the form of <span class=""math-container"" id=""6005504"">\|S^{-1}AS\|&lt;1</span>.</p>  <p>For instance, suppose <span class=""math-container"" id=""6005505"">A</span> satisfies the condition <span class=""math-container"" id=""6005506"">\color{red}{\|\pmatrix{P&amp;Q}\|_\infty&lt;1}</span>, where <span class=""math-container"" id=""6005507"">\|X\|_\infty=\max_i\sum_j|x_{ij}|</span> is the maximum row sum norm. Then we get <span class=""math-container"" id=""6005508""> \|A\|:=\left\|S^{-1}AS\right\|_\infty&lt;1\ \text{ with }\ S=\pmatrix{I\\ &amp;(1+\epsilon)I}. </span></p>  <p>Alternatively, if <span class=""math-container"" id=""6005509"">\color{red}{Q \text{ is positive definite and } \operatorname{trace}(P^2)+2\operatorname{trace}(Q)&lt;1}</span>, then <span class=""math-container"" id=""6005510"">A</span> is similar to <span class=""math-container"" id=""6005511"">B=\pmatrix{P&amp;Q^{1/2}\\ Q^{1/2}&amp;0}</span> and <span class=""math-container"" id=""6005512"">\|B\|_F^2&lt;1</span>. Hence <span class=""math-container"" id=""6005513"">\rho(A)=\rho(B)\le\|B\|_F&lt;1</span>.</p> "	Note that  \rho(A)\le\|A\|  for every matrix norm, and  \|A\|_b:=\|S^{-1}AS\|_a  is a matrix norm whenever  \|\cdot\|_a  is a matrix norm and  S  is nonsignular. By combining known matrix norms with similarity transform, one can always obtain a sufficient condition for  \rho(A)<1  in the form of  \|S^{-1}AS\|<1 .    For instance, suppose  A  satisfies the condition  \color{red}{\|\pmatrix{P&Q}\|_\infty<1} , where  \|X\|_\infty=\max_i\sum_j|x_{ij}|  is the maximum row sum norm. Then we get   \|A\|:=\left\|S^{-1}AS\right\|_\infty<1\ \text{ with }\ S=\pmatrix{I\\ &(1+\epsilon)I}.     Alternatively, if  \color{red}{Q \text{ is positive definite and } \operatorname{trace}(P^2)+2\operatorname{trace}(Q)<1} , then  A  is similar to  B=\pmatrix{P&Q^{1/2}\\ Q^{1/2}&0}  and  \|B\|_F^2<1 . Hence  \rho(A)=\rho(B)\le\|B\|_F<1 .  	0.9588611721992493
2447374	"<p>Hint for question 1: <span class=""math-container"" id=""22658994"">B_1</span> is open in the <span class=""math-container"" id=""22658995"">d_{\infty}</span>- metric? </p>  <p>It has been shown <a href=""https://math.stackexchange.com/questions/2443026/which-of-the-following-inequalities-are-satisfied-in-given-banach-spaces"">here</a> (the question is yours) that <span class=""math-container"" id=""22658996"">\forall x\in \mathbb{R}^n</span> the following inequalities hold <span class=""math-container"" id=""22658997"">\|x\|_{\infty}\leq \|x\|_1\leq n\|x\|_{\infty}.</span> Let <span class=""math-container"" id=""22658998"">x_0\in B_1</span> and let <span class=""math-container"" id=""22658999"">x</span> be such that <span class=""math-container"" id=""22659000"">\|x-x_0\|_{\infty}&lt; r</span> for some <span class=""math-container"" id=""22659001"">r&gt;0</span>. Then <span class=""math-container"" id=""22659002"">\|x\|_1\leq \|x-x_0\|_{1}+\|x_0\|_{1}\leq n\|x-x_0\|_{\infty}+\|x_0\|_{1}&lt;  nr+\|x_0\|_{1}.</span> Is there any positive value for <span class=""math-container"" id=""22659003"">r</span> such that <span class=""math-container"" id=""22659004"">nr+\|x_0\|_{1}&lt;1</span>, i.e. <span class=""math-container"" id=""22659005"">x\in B_1</span>? </p>  <p>In that case the set <span class=""math-container"" id=""22659006"">B_1</span> is open with respect to the metric <span class=""math-container"" id=""22659007"">d_{\infty}</span>.</p>  <p>P.S. For <span class=""math-container"" id=""22659008"">B_2</span> the following inequalities should be useful <span class=""math-container"" id=""22659009"">\|x\|_{\infty}\leq \|x\|_2\leq \sqrt{n}\|x\|_{\infty}.</span></p> "	Hint for question 1:  B_1  is open in the  d_{\infty} - metric?     It has been shown  here  (the question is yours) that  \forall x\in \mathbb{R}^n  the following inequalities hold  \|x\|_{\infty}\leq \|x\|_1\leq n\|x\|_{\infty}.  Let  x_0\in B_1  and let  x  be such that  \|x-x_0\|_{\infty}< r  for some  r>0 . Then  \|x\|_1\leq \|x-x_0\|_{1}+\|x_0\|_{1}\leq n\|x-x_0\|_{\infty}+\|x_0\|_{1}<  nr+\|x_0\|_{1}.  Is there any positive value for  r  such that  nr+\|x_0\|_{1}<1 , i.e.  x\in B_1 ?     In that case the set  B_1  is open with respect to the metric  d_{\infty} .    P.S. For  B_2  the following inequalities should be useful  \|x\|_{\infty}\leq \|x\|_2\leq \sqrt{n}\|x\|_{\infty}.  	0.9588593244552612
2880138	"<p>The result is false. Consider for example <span class=""math-container"" id=""26635214"">F=C([0,1])</span> with norms <span class=""math-container"" id=""26635215"">\|\ \|_\infty</span> and <span class=""math-container"" id=""26635216"">\|\ \|_1</span> and <span class=""math-container"" id=""26635217"">T</span> the identity operator. It is easy to find a sequence <span class=""math-container"" id=""26635218"">x_n(t)</span> such that <span class=""math-container"" id=""26635219"">\|x_n\|_1\to0</span> and <span class=""math-container"" id=""26635220"">\|x_n\|_\infty</span> does not converge.</p>  <p>Now let's look at the original the problem. We have <span class=""math-container"" id=""26635221"">Mx=M^{1/2}M^{1/2}x\in\text{Im}(M^{1/2})</span>. \begin{align} \|Mx_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}&amp;=\|M^{1/2}M^{1/2}x_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}\\ &amp;=\|PM^{1/2}x_n-PM^{1/2}x\|\\ &amp;\le\|M^{1/2}x_n-M^{1/2}x\|, \end{align} and the last expression converges to <span class=""math-container"" id=""26635222"">0</span>.</p> "	The result is false. Consider for example  F=C([0,1])  with norms  \|\ \|_\infty  and  \|\ \|_1  and  T  the identity operator. It is easy to find a sequence  x_n(t)  such that  \|x_n\|_1\to0  and  \|x_n\|_\infty  does not converge.    Now let's look at the original the problem. We have  Mx=M^{1/2}M^{1/2}x\in\text{Im}(M^{1/2}) . \begin{align} \|Mx_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}&=\|M^{1/2}M^{1/2}x_n-M^{1/2}x\|_{\text{Im}(M^{1/2})}\\ &=\|PM^{1/2}x_n-PM^{1/2}x\|\\ &\le\|M^{1/2}x_n-M^{1/2}x\|, \end{align} and the last expression converges to  0 .  	0.9588227272033691
1894857	"<p>This boils down to the question, whether for symmetric <span class=""math-container"" id=""17555058"">A</span> <span class=""math-container"" id=""17555059""> \|Ax\|_2^2\le \|A\|_\infty \cdot x^TAx </span> holds for all <span class=""math-container"" id=""17555060"">x</span>.</p>  <p>This is not true without positive definiteness of <span class=""math-container"" id=""17555061"">A</span> (take <span class=""math-container"" id=""17555062"">A=-I</span>).</p>  <p>It is true for positive definite <span class=""math-container"" id=""17555063"">A</span>: <span class=""math-container"" id=""17555064""> \|Ax\|_2^2 =\|A^{1/2}A^{1/2}x\|_2^2 \le \|A^{1/2}\|_2^2 \cdot \|A^{1/2}x\|_2^2 \le  \|A^{1/2}\|_2^2\cdot x^TAx. </span> Since <span class=""math-container"" id=""17555065"">A^{1/2}</span> is symmetric positive definite <span class=""math-container"" id=""17555066""> \|A^{1/2}\|_2^2 = \lambda_\max(A^{1/2})^2 = \lambda_\max(A) \le \|A\|_\infty, </span> which is the claim. Note that we can use any matrix norm on <span class=""math-container"" id=""17555067"">\mathbb R^{n,n}</span> instead of <span class=""math-container"" id=""17555068"">\|\cdot\|_\infty</span>.</p> "	This boils down to the question, whether for symmetric  A    \|Ax\|_2^2\le \|A\|_\infty \cdot x^TAx   holds for all  x .    This is not true without positive definiteness of  A  (take  A=-I ).    It is true for positive definite  A :   \|Ax\|_2^2 =\|A^{1/2}A^{1/2}x\|_2^2 \le \|A^{1/2}\|_2^2 \cdot \|A^{1/2}x\|_2^2 \le  \|A^{1/2}\|_2^2\cdot x^TAx.   Since  A^{1/2}  is symmetric positive definite   \|A^{1/2}\|_2^2 = \lambda_\max(A^{1/2})^2 = \lambda_\max(A) \le \|A\|_\infty,   which is the claim. Note that we can use any matrix norm on  \mathbb R^{n,n}  instead of  \|\cdot\|_\infty .  	0.9585137963294983
365304	"<p>As many others have pointed out, the matrix norm of <span class=""math-container"" id=""3893661"">I</span> is not necessarily equal to <span class=""math-container"" id=""3893662"">1</span>. In fact, if <span class=""math-container"" id=""3893663"">\|\cdot\|</span> is a matrix norm <span class=""math-container"" id=""3893664"">c\|\cdot\|</span> is also a matrix norm for any <span class=""math-container"" id=""3893665"">c\ge1</span>.</p>  <p>Yet, if your matrix norm is submultiplicative, we have <span class=""math-container"">$0<\|I\|=\|I^2\|\le\|I\|^2$</span> and hence <span class=""math-container"">$\|I\|\ge1$</span>, but strict inequality may still occur. E.g. the Frobenius norm <span class=""math-container"">$\|\cdot\|_F$</span> is submultiplicative, but <span class=""math-container"">$\|I_n\|_F=\sqrt{n}>1$</span> when <span class=""math-container"">$n>1$</span>.</p>  <p>If your textbook is not erred, it is perhaps talking about an <em>induced</em> matrix norm, i.e. one defined by <span class=""math-container"">$\|A\|=\sup_{x\neq0}\frac{\color{red}{\|}Ax\color{red}{\|}}{\color{red}{\|}x\color{red}{\|}}$</span> for some vector norm <span class=""math-container"">$\color{red}{\|\cdot\|}$</span> defined on <span class=""math-container"" id=""3893671"">K^n</span>. In this case, it is straight from the definition that <span class=""math-container"" id=""3893672"">\|I\|=1</span>.</p> "	As many others have pointed out, the matrix norm of  I  is not necessarily equal to  1 . In fact, if  \|\cdot\|  is a matrix norm  c\|\cdot\|  is also a matrix norm for any  c\ge1 .    Yet, if your matrix norm is submultiplicative, we have  $0<\|I\|=\|I^2\|\le\|I\|^2$  and hence  $\|I\|\ge1$ , but strict inequality may still occur. E.g. the Frobenius norm  $\|\cdot\|_F$  is submultiplicative, but  $\|I_n\|_F=\sqrt{n}>1$  when  $n>1$ .    If your textbook is not erred, it is perhaps talking about an  induced  matrix norm, i.e. one defined by  $\|A\|=\sup_{x\neq0}\frac{\color{red}{\|}Ax\color{red}{\|}}{\color{red}{\|}x\color{red}{\|}}$  for some vector norm  $\color{red}{\|\cdot\|}$  defined on  K^n . In this case, it is straight from the definition that  \|I\|=1 .  	0.9584485292434692
2520340	"<p>An infinite matrix <span class=""math-container"" id=""23305103"">A</span> satisfies a bound  <span class=""math-container"" id=""23305104"">\|Ax\|_p \le C\|x\|_p \tag1</span> if and only if it satisfies the bilinear bound <span class=""math-container"" id=""23305105"">|y^*Ax| \le C\|x\|_p\|y\|_q \tag2</span> (with same <span class=""math-container"" id=""23305106"">C</span> in both equations). Here   <span class=""math-container"" id=""23305107"">1/p+1/q=1</span> and <span class=""math-container"" id=""23305108"">{}^*</span> stands for conjugate transpose. Rewriting <span class=""math-container"" id=""23305109"">(2)</span> as <span class=""math-container"" id=""23305110"">|x^*A^*y| \le C\|x\|_p\|y\|_q \tag3</span> we see that it is also equivalent to  <span class=""math-container"" id=""23305111"">\|A^*y\|_q \le C \|y\|_q\tag4</span> Simply put, the adjoint of a bounded operator is a bounded operator on the <strong>dual</strong> space. As David C. Ullrich said, there is no reason for it to be bounded on the original space.</p>  <p>For a concrete example with <span class=""math-container"" id=""23305112"">A^*</span> not bounded on the same <span class=""math-container"" id=""23305113"">\ell^p</span> space, let <span class=""math-container"" id=""23305114"">p=4/3</span> and  <span class=""math-container"" id=""23305115""> A = \begin{pmatrix} 1 &amp; 1/\sqrt{2} &amp; 1/\sqrt{3} &amp; 1/\sqrt{4}  &amp; 1/\sqrt{5} &amp; \cdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots  \end{pmatrix} </span> where all rows except the first are zero. This is  a bounded rank-1 operator on <span class=""math-container"" id=""23305116"">\ell^{4/3}</span>, because the first row entries are a vector from the dual space <span class=""math-container"" id=""23305117"">\ell^{4}</span>. However, trying to apply the transpose <span class=""math-container"" id=""23305118"">A^*</span> on <span class=""math-container"" id=""23305119"">\ell^{4/3}</span>, we see that it maps the first basis vector to <span class=""math-container"" id=""23305120"">(1,  1/\sqrt{2},   1/\sqrt{3},  1/\sqrt{4}, 1/\sqrt{5},\dots)</span> which is not an element of <span class=""math-container"" id=""23305121"">\ell^{4/3}</span>.</p> "	An infinite matrix  A  satisfies a bound   \|Ax\|_p \le C\|x\|_p \tag1  if and only if it satisfies the bilinear bound  |y^*Ax| \le C\|x\|_p\|y\|_q \tag2  (with same  C  in both equations). Here    1/p+1/q=1  and  {}^*  stands for conjugate transpose. Rewriting  (2)  as  |x^*A^*y| \le C\|x\|_p\|y\|_q \tag3  we see that it is also equivalent to   \|A^*y\|_q \le C \|y\|_q\tag4  Simply put, the adjoint of a bounded operator is a bounded operator on the  dual  space. As David C. Ullrich said, there is no reason for it to be bounded on the original space.    For a concrete example with  A^*  not bounded on the same  \ell^p  space, let  p=4/3  and    A = \begin{pmatrix} 1 & 1/\sqrt{2} & 1/\sqrt{3} & 1/\sqrt{4}  & 1/\sqrt{5} & \cdots \\ 0 & 0 & 0 & 0 & 0 & \cdots \\ 0 & 0 & 0 & 0 & 0 & \cdots  \end{pmatrix}   where all rows except the first are zero. This is  a bounded rank-1 operator on  \ell^{4/3} , because the first row entries are a vector from the dual space  \ell^{4} . However, trying to apply the transpose  A^*  on  \ell^{4/3} , we see that it maps the first basis vector to  (1,  1/\sqrt{2},   1/\sqrt{3},  1/\sqrt{4}, 1/\sqrt{5},\dots)  which is not an element of  \ell^{4/3} .  	0.9584190249443054
1517342	"<p>Denote <span class=""math-container"" id=""14026161"">B=AA_1^{-1}=\left[\matrix{I\\A_2A_1^{-1}}\right]</span>. A proof strategy (among several others) could be</p>  <ol> <li>Show that <span class=""math-container"" id=""14026162"">B^*B-I</span> is positive-semidefinite. It would mean that the eigenvalues <span class=""math-container"" id=""14026163"">\lambda_i(B^*B)\ge 1</span> and, hence, the singular values <span class=""math-container"" id=""14026164"">\sigma_i(B)\ge 1</span>.</li> <li>Show (e.g. via SVD) that <span class=""math-container"" id=""14026165"">\|B^{+}\|_2=\frac{1}{\sigma_\min(B)}\le 1</span>.</li> <li>Show that <span class=""math-container"" id=""14026166"">B^{+}=A_1A^{+}</span>.</li> <li>Estimate <span class=""math-container"" id=""14026167""> \|A^{+}\|_2=\|A_1^{-1}A_1A^{+}\|_2\le\|A_1^{-1}\|_2\|A_1A^{+}\|_2\le \|A_1^{-1}\|_2. </span></li> </ol> "	Denote  B=AA_1^{-1}=\left[\matrix{I\\A_2A_1^{-1}}\right] . A proof strategy (among several others) could be      Show that  B^*B-I  is positive-semidefinite. It would mean that the eigenvalues  \lambda_i(B^*B)\ge 1  and, hence, the singular values  \sigma_i(B)\ge 1 .   Show (e.g. via SVD) that  \|B^{+}\|_2=\frac{1}{\sigma_\min(B)}\le 1 .   Show that  B^{+}=A_1A^{+} .   Estimate   \|A^{+}\|_2=\|A_1^{-1}A_1A^{+}\|_2\le\|A_1^{-1}\|_2\|A_1A^{+}\|_2\le \|A_1^{-1}\|_2.     	0.9583341479301453
321203	"<p>Similar solution to @uncookedfalcon's answer, but with different notation would be as follows:</p>  <p>\begin{align} |aX+bY| &amp;\leq |aX| + |bY|  &amp; \text{triangle inequality} \\ |aX+bY| &amp;\leq |a|\cdot|X| + |b|\cdot|Y|  &amp; \text{norm homogeneity} \\ \mathbb{E}\Big[|aX+bY|\Big] &amp;\leq \mathbb{E}\Big[|a|\cdot|X| + |b|\cdot|Y|\Big]  &amp; \text{expected value monotonicity} \\ \mathbb{E}\Big[|aX+bY|\Big] &amp;\leq |a|\cdot\mathbb{E}\Big[|X|\Big] + |b|\cdot\mathbb{E}\Big[|Y|\Big]  &amp; \text{expected value linearity} \\ \end{align}</p>  <p>In fact this exactly the same with <span class=""math-container"" id=""3472066"">\mathbb{E}[X] = \int_\Omega X\ \mathrm{d}P</span>, but is nicer if you don't want to mention the integral symbol while <span class=""math-container"" id=""3472067"">\mathbb{E}[X] = \sum_{k \in \mathrm{Cod}(k)}k\cdot P(X = k)</span> or <span class=""math-container"" id=""3472068"">\mathbb{E}[X] = \sum_{\omega \in \Omega}X(\omega) \cdot P(\omega)</span>. I know all this is just a difference in notation (i.e. the sum is still the integral for appropriate measure), but sometimes it matters.</p>  <p>I hope this helps ;-)</p> "	Similar solution to @uncookedfalcon's answer, but with different notation would be as follows:    \begin{align} |aX+bY| &\leq |aX| + |bY|  & \text{triangle inequality} \\ |aX+bY| &\leq |a|\cdot|X| + |b|\cdot|Y|  & \text{norm homogeneity} \\ \mathbb{E}\Big[|aX+bY|\Big] &\leq \mathbb{E}\Big[|a|\cdot|X| + |b|\cdot|Y|\Big]  & \text{expected value monotonicity} \\ \mathbb{E}\Big[|aX+bY|\Big] &\leq |a|\cdot\mathbb{E}\Big[|X|\Big] + |b|\cdot\mathbb{E}\Big[|Y|\Big]  & \text{expected value linearity} \\ \end{align}    In fact this exactly the same with  \mathbb{E}[X] = \int_\Omega X\ \mathrm{d}P , but is nicer if you don't want to mention the integral symbol while  \mathbb{E}[X] = \sum_{k \in \mathrm{Cod}(k)}k\cdot P(X = k)  or  \mathbb{E}[X] = \sum_{\omega \in \Omega}X(\omega) \cdot P(\omega) . I know all this is just a difference in notation (i.e. the sum is still the integral for appropriate measure), but sometimes it matters.    I hope this helps ;-)  	0.9582816958427429
333950	"<p>As copper.hat already pointed out, the inequality <span class=""math-container"" id=""3598757"">\|f\|_p \leq \mu(E)^{1-\frac{1}{p}} \cdot \|f\|_{\infty}</span> does not hold for all <span class=""math-container"" id=""3598758"">p \geq 1</span>. Instead, it should read <span class=""math-container"" id=""3598759"">\|f\|_p \leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty}</span></p>  <p>This follows from <span class=""math-container"" id=""3598760"">\|f\|_p \leq \mu(E)^{\frac{1}{p}-\frac{1}{r}} \cdot \|f\|_r \qquad (r&gt;p)</span>by letting <span class=""math-container"" id=""3598761"">r \to \infty</span>, using that <span class=""math-container"" id=""3598762"">\lim_{r \to \infty} \|f\|_r = \|f\|_{\infty}</span> (see for a <a href=""https://math.stackexchange.com/questions/242779/limit-of-lp-norm"">proof</a> here).</p>  <hr>  <p>Edit: Actually, there is a rather quick (and direct) proof of the inequality: <span class=""math-container"" id=""3598763"">\begin{align} \|f\|_p^p &amp;= \int_E \underbrace{|f|^p}_{\stackrel{p \geq 1}{\leq} \|f\|_{\infty}^p} \, d\mu \leq \mu(E) \cdot \|f\|_{\infty}^p \\ \Rightarrow \|f\|_p &amp;\leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty} \end{align}</span></p> "	As copper.hat already pointed out, the inequality  \|f\|_p \leq \mu(E)^{1-\frac{1}{p}} \cdot \|f\|_{\infty}  does not hold for all  p \geq 1 . Instead, it should read  \|f\|_p \leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty}    This follows from  \|f\|_p \leq \mu(E)^{\frac{1}{p}-\frac{1}{r}} \cdot \|f\|_r \qquad (r>p) by letting  r \to \infty , using that  \lim_{r \to \infty} \|f\|_r = \|f\|_{\infty}  (see for a  proof  here).       Edit: Actually, there is a rather quick (and direct) proof of the inequality:  \begin{align} \|f\|_p^p &= \int_E \underbrace{|f|^p}_{\stackrel{p \geq 1}{\leq} \|f\|_{\infty}^p} \, d\mu \leq \mu(E) \cdot \|f\|_{\infty}^p \\ \Rightarrow \|f\|_p &\leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty} \end{align}  	0.9581931829452515
2573309	"<p>If <em>natural norm</em> means a matrix norm induced by a vector norm, then it is generally not true. </p>  <p>For any induced matrix norm and a diagonal <span class=""math-container"" id=""23787761"">D</span>, we have <span class=""math-container"" id=""23787762""> \|D\|=\max_{x\neq 0}\frac{\|Dx\|}{\|x\|}\geq\max_{1\leq i\leq n}\frac{\|De_i\|}{\|e_i\|}=\max_{1\leq i\leq n}|D_{ii}| </span> obtained by picking <span class=""math-container"" id=""23787763"">x</span> to be columns of the identity matrix. So the maximum of <span class=""math-container"" id=""23787764"">|D_{ii}|</span> is at least a lower bound on <span class=""math-container"" id=""23787765"">\|D\|</span>.</p>  <p>There are cases, where the inequality becomes an equality. This is true for any matrix <span class=""math-container"" id=""23787766"">p</span>-norm with <span class=""math-container"" id=""23787767"">1\leq p\leq \infty</span>.</p>  <p>In order to construct a counterexample, consider a vector norm <span class=""math-container"" id=""23787768""> \|x\|_M:=\|Mx\|_\infty </span> with a nonsingular matrix <span class=""math-container"" id=""23787769"">M</span>. The induced matrix norm is related to the <span class=""math-container"" id=""23787770"">\infty</span>-norm by <span class=""math-container"" id=""23787771""> \|D\|_M=\|MDM^{-1}\|_\infty. </span> For example, in <span class=""math-container"" id=""23787772"">\mathbb{R}^2</span> and with <span class=""math-container"" id=""23787773""> D=\begin{bmatrix}1&amp;0\\0&amp;2\end{bmatrix}, \quad M=\begin{bmatrix}2&amp;1\\1&amp;2\end{bmatrix}, </span> we get <span class=""math-container"" id=""23787774""> \|D\|_M=3&gt; 2=\max_{1\leq i\leq2}|D_{ii}|. </span></p> "	If  natural norm  means a matrix norm induced by a vector norm, then it is generally not true.     For any induced matrix norm and a diagonal  D , we have   \|D\|=\max_{x\neq 0}\frac{\|Dx\|}{\|x\|}\geq\max_{1\leq i\leq n}\frac{\|De_i\|}{\|e_i\|}=\max_{1\leq i\leq n}|D_{ii}|   obtained by picking  x  to be columns of the identity matrix. So the maximum of  |D_{ii}|  is at least a lower bound on  \|D\| .    There are cases, where the inequality becomes an equality. This is true for any matrix  p -norm with  1\leq p\leq \infty .    In order to construct a counterexample, consider a vector norm   \|x\|_M:=\|Mx\|_\infty   with a nonsingular matrix  M . The induced matrix norm is related to the  \infty -norm by   \|D\|_M=\|MDM^{-1}\|_\infty.   For example, in  \mathbb{R}^2  and with   D=\begin{bmatrix}1&0\\0&2\end{bmatrix}, \quad M=\begin{bmatrix}2&1\\1&2\end{bmatrix},   we get   \|D\|_M=3> 2=\max_{1\leq i\leq2}|D_{ii}|.   	0.9581659436225891
722608	"<p>Note first that your definition of the operator norm is not correct. It should say that <span class=""math-container"" id=""7204631"">\|T\|_{op}</span> is the least <span class=""math-container"" id=""7204632"">c</span> such that <span class=""math-container"" id=""7204633"">\|T\vec v\|\leq c\,\|\vec v\|</span> for all <span class=""math-container"" id=""7204634"">\vec v</span>.</p>  <p>Note also that the operator norm depends on which norms you give to <span class=""math-container"" id=""7204635"">\mathbb R^n</span> and <span class=""math-container"" id=""7204636"">\mathbb R^m</span>. The canonical choice is to take the Euclidean norm in both, but that choice is by no means the only one. </p>  <p>As for those <span class=""math-container"" id=""7204637"">A</span> such that <span class=""math-container"" id=""7204638"">\|A\|_{op}\leq1</span>, you cannot expect any algebraic property to characterize them. Indeed, given any matrix <span class=""math-container"" id=""7204639"">B</span>, the matrix <span class=""math-container"" id=""7204640"">A=B/\|B\|_{op}</span> satisfies <span class=""math-container"" id=""7204641"">\|A\|_{op}=1</span>. </p>  <p>Of course there are some necessary conditions. If <span class=""math-container"" id=""7204642"">\|A\|_{op}\leq1</span>, then for all <span class=""math-container"" id=""7204643"">k,j</span> <span class=""math-container"" id=""7204644"">|A_{kj}|=|\langle Ae_j,e_k\rangle|\leq\|A\|_{op}\,\|e_j\|\,\|e_k\|\leq1.</span> More than that, if <span class=""math-container"" id=""7204645"">e</span> is the vector all entries equal to 1, then <span class=""math-container"" id=""7204646""> 1\geq\|A\|_{op}\geq\,\frac1n\,|\langle Ae,e\rangle|=\frac1n\,\left|\sum_{k=1}^m\sum_{j=1}^nA_{kj}\right|. </span> Or, for each <span class=""math-container"" id=""7204647"">j</span>, <span class=""math-container"" id=""7204648""> 1\geq\|A\|_{op}^2\geq\|Ae_j\|^2=\sum_{k=1}^m|A_{kj}|^2 </span></p> "	Note first that your definition of the operator norm is not correct. It should say that  \|T\|_{op}  is the least  c  such that  \|T\vec v\|\leq c\,\|\vec v\|  for all  \vec v .    Note also that the operator norm depends on which norms you give to  \mathbb R^n  and  \mathbb R^m . The canonical choice is to take the Euclidean norm in both, but that choice is by no means the only one.     As for those  A  such that  \|A\|_{op}\leq1 , you cannot expect any algebraic property to characterize them. Indeed, given any matrix  B , the matrix  A=B/\|B\|_{op}  satisfies  \|A\|_{op}=1 .     Of course there are some necessary conditions. If  \|A\|_{op}\leq1 , then for all  k,j   |A_{kj}|=|\langle Ae_j,e_k\rangle|\leq\|A\|_{op}\,\|e_j\|\,\|e_k\|\leq1.  More than that, if  e  is the vector all entries equal to 1, then   1\geq\|A\|_{op}\geq\,\frac1n\,|\langle Ae,e\rangle|=\frac1n\,\left|\sum_{k=1}^m\sum_{j=1}^nA_{kj}\right|.   Or, for each  j ,   1\geq\|A\|_{op}^2\geq\|Ae_j\|^2=\sum_{k=1}^m|A_{kj}|^2   	0.9580414295196533
2360759	"<p>Yes, if <span class=""math-container"" id=""21782850"">\hat{u} \in L^2(\mathbb{R}) \cap L^\infty(\mathbb{R})</span>, one can modify <span class=""math-container"" id=""21782851"">u</span> to satisfy the equality. I will provide justification below, but would like to note first that my answer is based off the <span class=""math-container"" id=""21782852"">2 \pi</span> definition of the Fourier transform. That is, I define the Fourier transform to be: <span class=""math-container"" id=""21782853"">\hat{u}(\omega) = \int_{\mathbb{R}} f(t) e^{-2 \pi i t \omega} \, dt</span></p>  <p>If <span class=""math-container"" id=""21782854"">u^2 \in L^1(\mathbb{R})</span>, then <span class=""math-container"" id=""21782855"">u \in L^2(\mathbb{R})</span> and so <span class=""math-container"" id=""21782856"">\hat{u} \in L^2(\mathbb{R})</span> by the <span class=""math-container"" id=""21782857"">L^2(\mathbb{R})</span> Fourier transform given by Plancherel's theorem. Now, suppose that <span class=""math-container"" id=""21782858"">\hat{u}</span> is also in <span class=""math-container"" id=""21782859"">L^\infty(\mathbb{R})</span>. Define a new function <span class=""math-container"" id=""21782860"">u_a(t) := \sqrt{a} u(at)</span> where <span class=""math-container"" id=""21782861"">a &gt; 0</span>. Note that <span class=""math-container"" id=""21782862"">\|u_a^2\|_1 = \|u^2\|_1</span> and that <span class=""math-container"" id=""21782863"">\widehat{u_a}(\omega) = (1/\sqrt{a}) \hat{u}(\omega/a)</span>. Thus, we have <span class=""math-container"" id=""21782864"">\|\widehat{u_a}\|_\infty^2 = \frac{1}{a} \|\hat{u}\|_\infty^2</span>. We can then force <span class=""math-container"" id=""21782865"">\|\widehat{u_a}\|_\infty^2 = \|u_a^2\|_1 = \|u^2\|_1</span> by letting <span class=""math-container"" id=""21782866"">a = \|\hat{u}\|_\infty^2/\|u^2\|_1</span>, and <span class=""math-container"" id=""21782867"">u_a</span> will satisfy the desired equality.</p> "	Yes, if  \hat{u} \in L^2(\mathbb{R}) \cap L^\infty(\mathbb{R}) , one can modify  u  to satisfy the equality. I will provide justification below, but would like to note first that my answer is based off the  2 \pi  definition of the Fourier transform. That is, I define the Fourier transform to be:  \hat{u}(\omega) = \int_{\mathbb{R}} f(t) e^{-2 \pi i t \omega} \, dt    If  u^2 \in L^1(\mathbb{R}) , then  u \in L^2(\mathbb{R})  and so  \hat{u} \in L^2(\mathbb{R})  by the  L^2(\mathbb{R})  Fourier transform given by Plancherel's theorem. Now, suppose that  \hat{u}  is also in  L^\infty(\mathbb{R}) . Define a new function  u_a(t) := \sqrt{a} u(at)  where  a > 0 . Note that  \|u_a^2\|_1 = \|u^2\|_1  and that  \widehat{u_a}(\omega) = (1/\sqrt{a}) \hat{u}(\omega/a) . Thus, we have  \|\widehat{u_a}\|_\infty^2 = \frac{1}{a} \|\hat{u}\|_\infty^2 . We can then force  \|\widehat{u_a}\|_\infty^2 = \|u_a^2\|_1 = \|u^2\|_1  by letting  a = \|\hat{u}\|_\infty^2/\|u^2\|_1 , and  u_a  will satisfy the desired equality.  	0.9580102562904358
2023056	"<p>First use the embedding of <span class=""math-container"" id=""18711785"">H^1</span> into some <span class=""math-container"" id=""18711786"">L^p</span>, <span class=""math-container"" id=""18711787"">p&gt;4</span>. Then interpolate <span class=""math-container"" id=""18711788"">L^p</span>-norms (H\""older inequality), then apply the result you mentioned: <span class=""math-container"" id=""18711789""> \|v\|_{L^p} \le c \|v\|_{H^1}, </span> <span class=""math-container"" id=""18711790""> \|v\|_{L^4} \le \|v\|_{L^2}^\theta \|v\|_{L^p}^{1-\theta} \le  c \|v\|_{L^2}^\theta \|v\|_{H^1}^{1-\theta} </span> Since you are dealing with 2-dimensional domain, this is true with <span class=""math-container"" id=""18711791"">\theta=1/2</span> (see Temam's book on Navier-Stokes). Set <span class=""math-container"" id=""18711792"">v:=\nabla u</span>. Then we get <span class=""math-container"" id=""18711793""> \|\nabla u\|_{L^4} \le c \|u\|_{H^1}^\theta \|u\|_{H^2}^{1-\theta} \le c \|u\|_{L^2}^{\theta'} \|u\|_{H^2}^{1-\theta'} </span> where in the last step I used the interpolation inequality for Sobolev spaces you mentioned in the comment. As written in the question, <span class=""math-container"" id=""18711794"">1-\theta'=3/4</span>.</p> "	"First use the embedding of  H^1  into some  L^p ,  p>4 . Then interpolate  L^p -norms (H\""older inequality), then apply the result you mentioned:   \|v\|_{L^p} \le c \|v\|_{H^1},     \|v\|_{L^4} \le \|v\|_{L^2}^\theta \|v\|_{L^p}^{1-\theta} \le  c \|v\|_{L^2}^\theta \|v\|_{H^1}^{1-\theta}   Since you are dealing with 2-dimensional domain, this is true with  \theta=1/2  (see Temam's book on Navier-Stokes). Set  v:=\nabla u . Then we get   \|\nabla u\|_{L^4} \le c \|u\|_{H^1}^\theta \|u\|_{H^2}^{1-\theta} \le c \|u\|_{L^2}^{\theta'} \|u\|_{H^2}^{1-\theta'}   where in the last step I used the interpolation inequality for Sobolev spaces you mentioned in the comment. As written in the question,  1-\theta'=3/4 .  "	0.9579982161521912
2170935	"<p>The necessary and sufficient condition is that the <a href=""https://en.wikipedia.org/wiki/Spectral_radius"" rel=""nofollow noreferrer"">spectral radius</a> of <span class=""math-container"" id=""20012810"">A</span> is less than <span class=""math-container"" id=""20012811"">1</span>. Equivalently, there exists a positive integer <span class=""math-container"" id=""20012812"">n</span> and a submultiplicative matrix norm <span class=""math-container"" id=""20012813"">\|\cdot \|</span> such that <span class=""math-container"" id=""20012814"">\|A^n\|&lt;1</span>. </p>  <p>If you are looking for an easy-to-check sufficient condition, it's reasonable to consider the <a href=""https://en.wikipedia.org/wiki/Matrix_norm#Induced_norm"" rel=""nofollow noreferrer"">induced norms</a> for either <span class=""math-container"" id=""20012815"">\ell^1</span>- or <span class=""math-container"" id=""20012816"">\ell^\infty</span>- vector norms. These are  <span class=""math-container"" id=""20012817""> \|A\|_1 = \max_{j}\sum_{i}|a_{ij}|\quad\text{ and }\quad \|A\|_\infty = \max_{i}\sum_{j}|a_{ij}| </span> If either of these norms is less than <span class=""math-container"" id=""20012818"">1</span>, then the inequality <span class=""math-container"" id=""20012819"">\|Ax\|_p\le \|A\|_p\|x\|_p</span> (with <span class=""math-container"" id=""20012820"">p</span> being <span class=""math-container"" id=""20012821"">1</span> or <span class=""math-container"" id=""20012822"">\infty</span>) yields convergence to zero.</p> "	The necessary and sufficient condition is that the  spectral radius  of  A  is less than  1 . Equivalently, there exists a positive integer  n  and a submultiplicative matrix norm  \|\cdot \|  such that  \|A^n\|<1 .     If you are looking for an easy-to-check sufficient condition, it's reasonable to consider the  induced norms  for either  \ell^1 - or  \ell^\infty - vector norms. These are    \|A\|_1 = \max_{j}\sum_{i}|a_{ij}|\quad\text{ and }\quad \|A\|_\infty = \max_{i}\sum_{j}|a_{ij}|   If either of these norms is less than  1 , then the inequality  \|Ax\|_p\le \|A\|_p\|x\|_p  (with  p  being  1  or  \infty ) yields convergence to zero.  	0.9579310417175293
1517109	"<p><strong>Hint:</strong></p>  <p><span class=""math-container"" id=""14024747"">\frac{1}{\|A^{-1}\|\|A\|}=\left|\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\right|\leq 1 \ [\text{applicable only for the normal matrices}]</span> </p>  <p><span class=""math-container"" id=""14024748"">\frac{1}{\|A^{-1}\|\|A\|}=\frac{\sigma_{min}(A)}{\sigma_{max}(A)}\leq 1 \ [\text{true only when, } \|\cdot\| \text{ denotes } \|\cdot\|_2]</span> For any matrix <span class=""math-container"" id=""14024749"">A, \ \kappa(A) \geq 1</span>.</p>  <p>[Please check the comments below for another approach to solve this problem.]</p> "	Hint:    \frac{1}{\|A^{-1}\|\|A\|}=\left|\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\right|\leq 1 \ [\text{applicable only for the normal matrices}]      \frac{1}{\|A^{-1}\|\|A\|}=\frac{\sigma_{min}(A)}{\sigma_{max}(A)}\leq 1 \ [\text{true only when, } \|\cdot\| \text{ denotes } \|\cdot\|_2]  For any matrix  A, \ \kappa(A) \geq 1 .    [Please check the comments below for another approach to solve this problem.]  	0.9578703045845032
870564	"<p>One way is to apply <a href=""http://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality"" rel=""nofollow"">Hölder's inequality</a> to <span class=""math-container"" id=""5956419"">\sqrt{fg}</span>, since we then find that</p>  <p>\begin{align*} \int_0^1 \sqrt{fg} dx &amp;= \|\sqrt{fg}\|_1 \\ &amp;\le \|\sqrt{f}\|_2 \|\sqrt g\|_2 \\ &amp;= \left(\int_0^1 f(x) dx\right)^{1/2}\left(\int_0^1 g(x) dx\right)^{1/2} \end{align*}</p>  <p>Now since we have <span class=""math-container"" id=""5956420"">\sqrt{fg} dx \ge 1</span> almost everywhere, monotonicity of the integral leads to <span class=""math-container"" id=""5956421"">1 \le \int_0^1 \sqrt{fg}</span>; the final result follows by squaring.</p> "	One way is to apply  Hölder's inequality  to  \sqrt{fg} , since we then find that    \begin{align*} \int_0^1 \sqrt{fg} dx &= \|\sqrt{fg}\|_1 \\ &\le \|\sqrt{f}\|_2 \|\sqrt g\|_2 \\ &= \left(\int_0^1 f(x) dx\right)^{1/2}\left(\int_0^1 g(x) dx\right)^{1/2} \end{align*}    Now since we have  \sqrt{fg} dx \ge 1  almost everywhere, monotonicity of the integral leads to  1 \le \int_0^1 \sqrt{fg} ; the final result follows by squaring.  	0.9578378796577454
892524	"<p>Define <span class=""math-container"" id=""8647486"">f:[0,2\pi]\to\mathbb{C}</span> as: <span class=""math-container"" id=""8647487"">f(\theta)=\sum_{n=1}^{+\infty}x_n e^{ni\theta}.</span> Then <span class=""math-container"" id=""8647488"">f</span> is a <span class=""math-container"" id=""8647489"">L_2</span>-function with zero mean over <span class=""math-container"" id=""8647490"">(0,2\pi)</span>, and we have: <span class=""math-container"" id=""8647491"">\|f\|_1 \leq \sqrt{2\pi}\,\|f\|_2 \tag{1}</span> due to the Cauchy-Schwarz inequality and  <span class=""math-container"" id=""8647492"">\|f\|_2 \leq \|f'\|_2\tag{2}</span> due to <a href=""http://en.wikipedia.org/wiki/Wirtinger%27s_inequality_for_functions"" rel=""nofollow"">Wirtinger's inequality</a>, or just Parseval's identity. <span class=""math-container"" id=""8647493"">(1)</span> and <span class=""math-container"" id=""8647494"">(2)</span> give: <span class=""math-container"" id=""8647495"">\frac{1}{2\pi}\int_{0}^{2\pi}\left|\sum_{n=1}^{+\infty}x_n e^{ni\theta}\right|\,d\theta\leq\frac{1}{\sqrt{2\pi}}.\tag{3}</span> Probably this can be improved a little, since equality cannot hold both in <span class=""math-container"" id=""8647496"">(1)</span> and in <span class=""math-container"" id=""8647497"">(2)</span>.</p> "	Define  f:[0,2\pi]\to\mathbb{C}  as:  f(\theta)=\sum_{n=1}^{+\infty}x_n e^{ni\theta}.  Then  f  is a  L_2 -function with zero mean over  (0,2\pi) , and we have:  \|f\|_1 \leq \sqrt{2\pi}\,\|f\|_2 \tag{1}  due to the Cauchy-Schwarz inequality and   \|f\|_2 \leq \|f'\|_2\tag{2}  due to  Wirtinger's inequality , or just Parseval's identity.  (1)  and  (2)  give:  \frac{1}{2\pi}\int_{0}^{2\pi}\left|\sum_{n=1}^{+\infty}x_n e^{ni\theta}\right|\,d\theta\leq\frac{1}{\sqrt{2\pi}}.\tag{3}  Probably this can be improved a little, since equality cannot hold both in  (1)  and in  (2) .  	0.9577962756156921
1377178	"<p><span class=""math-container"" id=""12798585"">k(A,2)=\|A\|_2\|A^{-1}\|_2= \max | \lambda_A|.\max|\lambda_{A^{-1}}|\leq\|A\|\|A^{-1}\|=k(A,\|.\|)</span> for each operator norm <span class=""math-container"" id=""12798586"">\|.\|</span>.</p>  <p>That <span class=""math-container"" id=""12798587"">\max|\lambda_A|\leq \|A\|</span> follows from:</p>  <p>Let <span class=""math-container"" id=""12798588"">x</span> be an eigenvector of <span class=""math-container"" id=""12798589"">A</span> for the eigenvalue <span class=""math-container"" id=""12798590"">\lambda</span>, where <span class=""math-container"" id=""12798591"">\max\limits_{i}|\lambda_i|=|\lambda|</span>. Then for arbitrary matrix norm <span class=""math-container"" id=""12798592"">\|.\|</span>, subordinate to the vector norm <span class=""math-container"" id=""12798593"">\|.\|</span>, we have <span class=""math-container"" id=""12798594"">\|A\|=\max\limits_{y\neq 0} \frac{\|Ay\|}{\|y\|}\ge \frac{\|Ax\|}{\|x\|}=\frac{\|\lambda x\|}{\|x\|}=|\lambda|</span></p> "	k(A,2)=\|A\|_2\|A^{-1}\|_2= \max | \lambda_A|.\max|\lambda_{A^{-1}}|\leq\|A\|\|A^{-1}\|=k(A,\|.\|)  for each operator norm  \|.\| .    That  \max|\lambda_A|\leq \|A\|  follows from:    Let  x  be an eigenvector of  A  for the eigenvalue  \lambda , where  \max\limits_{i}|\lambda_i|=|\lambda| . Then for arbitrary matrix norm  \|.\| , subordinate to the vector norm  \|.\| , we have  \|A\|=\max\limits_{y\neq 0} \frac{\|Ay\|}{\|y\|}\ge \frac{\|Ax\|}{\|x\|}=\frac{\|\lambda x\|}{\|x\|}=|\lambda|  	0.9577630162239075
1154907	"<p>First, if <span class=""math-container"" id=""10852190"">P</span> is a <em>nontrivial</em> projection (<span class=""math-container"" id=""10852191"">P\neq 0</span>), any submultiplicative norm of <span class=""math-container"" id=""10852192"">P</span> is bounded from <em>below</em> by one. This follows simply from the fact that <span class=""math-container"" id=""10852193"">P</span> is idempotent and <span class=""math-container"" id=""10852194"">\|P\|=\|P^2\|\leq\|P\|^2</span> which gives <span class=""math-container"" id=""10852195"">\|P\|\geq 1</span>. Consequently, if <span class=""math-container"" id=""10852196"">\|\cdot\|</span> is the matrix norm induced by the vector norm induced by the scalar product w.r.t. which <span class=""math-container"" id=""10852197"">P</span> is orthogonal, then either <span class=""math-container"" id=""10852198"">\|P\|=0</span> or <span class=""math-container"" id=""10852199"">\|P\|=1</span> with the first option possible if and only if <span class=""math-container"" id=""10852200"">P=0</span>.</p>  <p>The only way how to make a norm of a nontrivial projection smaller than one is to use a matrix norm which is not submultiplicative. This excludes, e.g., <span class=""math-container"" id=""10852201"">p</span>-norms, Frobenius norm, and any matrix norm induced by a vector norm. On the other hand, some matrix norms are not submultiplicative; e.g., the <span class=""math-container"" id=""10852202"">\max</span>-norm <span class=""math-container"" id=""10852203"">\|P\|_\max:=\max\limits_{i,j}|p_{ij}|</span>.</p>  <p>E.g., with (stealing the <span class=""math-container"" id=""10852204"">P</span> from Omnomnomnom) <span class=""math-container"" id=""10852205""> P=\frac{1}{5}\pmatrix{1&amp;2\\2&amp;4}, </span> <span class=""math-container"" id=""10852206""> \|P\|_2=1, \quad \|P\|_1=\|P\|_\infty=\frac{6}{5}&gt;1, \quad \|P\|_\max=\frac{4}{5}&lt;1. </span></p> "	First, if  P  is a  nontrivial  projection ( P\neq 0 ), any submultiplicative norm of  P  is bounded from  below  by one. This follows simply from the fact that  P  is idempotent and  \|P\|=\|P^2\|\leq\|P\|^2  which gives  \|P\|\geq 1 . Consequently, if  \|\cdot\|  is the matrix norm induced by the vector norm induced by the scalar product w.r.t. which  P  is orthogonal, then either  \|P\|=0  or  \|P\|=1  with the first option possible if and only if  P=0 .    The only way how to make a norm of a nontrivial projection smaller than one is to use a matrix norm which is not submultiplicative. This excludes, e.g.,  p -norms, Frobenius norm, and any matrix norm induced by a vector norm. On the other hand, some matrix norms are not submultiplicative; e.g., the  \max -norm  \|P\|_\max:=\max\limits_{i,j}|p_{ij}| .    E.g., with (stealing the  P  from Omnomnomnom)   P=\frac{1}{5}\pmatrix{1&2\\2&4},     \|P\|_2=1, \quad \|P\|_1=\|P\|_\infty=\frac{6}{5}>1, \quad \|P\|_\max=\frac{4}{5}<1.   	0.9577411413192749
2231328	"<p><strong><em>True for the edited version.</em></strong></p>  <p>One can assume that <span class=""math-container"" id=""20606210"">\mu</span> is a unitary measure since one can divide by <span class=""math-container"" id=""20606211"">\mu(\Omega)</span> and obtain a new measure where all norms will be multiplied by <span class=""math-container"" id=""20606212"">\dfrac{1}{\mu(\Omega)}.</span> So, I shall assume <span class=""math-container"" id=""20606213"">\mu</span> is unitary.</p>  <p>In this case, a direct application of Holder inequality allows concluding at once <span class=""math-container"" id=""20606214"">\|f\|_1 \leq \|f\|_2 \|1\|_2 = \|f\|_2;</span> by translation invariance of the norm, convergence in <span class=""math-container"" id=""20606215"">\mathscr{L}^2</span> implies convergence in <span class=""math-container"" id=""20606216"">\mathscr{L}^1.</span> (Plus, if you divide a function <span class=""math-container"" id=""20606217"">f</span> in <span class=""math-container"" id=""20606218"">\mathscr{L}^2</span> as <span class=""math-container"" id=""20606219"">|f| = |f|\mathbf{1}_{\{|f|\leq 1\}}+|f|\mathbf{1}_{\{|f|&gt;1\}} \leq |f|^2+1,</span> once gets that <span class=""math-container"" id=""20606220"">\mathscr{L}^2 \subset \mathscr{L}^1</span>).</p> "	True for the edited version.    One can assume that  \mu  is a unitary measure since one can divide by  \mu(\Omega)  and obtain a new measure where all norms will be multiplied by  \dfrac{1}{\mu(\Omega)}.  So, I shall assume  \mu  is unitary.    In this case, a direct application of Holder inequality allows concluding at once  \|f\|_1 \leq \|f\|_2 \|1\|_2 = \|f\|_2;  by translation invariance of the norm, convergence in  \mathscr{L}^2  implies convergence in  \mathscr{L}^1.  (Plus, if you divide a function  f  in  \mathscr{L}^2  as  |f| = |f|\mathbf{1}_{\{|f|\leq 1\}}+|f|\mathbf{1}_{\{|f|>1\}} \leq |f|^2+1,  once gets that  \mathscr{L}^2 \subset \mathscr{L}^1 ).  	0.9577218890190125
1679092	"<p>I'm afraid your intition is leading you in a wrong direction. Consider <span class=""math-container"" id=""15518079"">\mathbb R^2</span> equipped with the <span class=""math-container"" id=""15518080"">1-</span>norm <span class=""math-container"" id=""15518081"">\left\|\begin{pmatrix}x_1\\x_2\end{pmatrix}\right\|_1=|x_1|+|x_2|</span>. </p>  <p>Let <span class=""math-container"" id=""15518082"">n\in \mathbb N</span> and <span class=""math-container"" id=""15518083"">a=\begin{pmatrix}n\\0\end{pmatrix}</span>, <span class=""math-container"" id=""15518084"">b=\begin{pmatrix}0\\n\end{pmatrix}</span>, <span class=""math-container"" id=""15518085"">c=\begin{pmatrix}n\\n\end{pmatrix}</span>. Then <span class=""math-container"" id=""15518086"">\|a\|_1=\|b\|_1=\frac{1}{2}\|c\|_1 = n,</span> but <span class=""math-container"" id=""15518087"">\|a-b\|_1 = 2n,</span> i.e. there is no upper bound.</p>  <p>I'm not sure for which norms you can generalize this kind of argument.</p> "	I'm afraid your intition is leading you in a wrong direction. Consider  \mathbb R^2  equipped with the  1- norm  \left\|\begin{pmatrix}x_1\\x_2\end{pmatrix}\right\|_1=|x_1|+|x_2| .     Let  n\in \mathbb N  and  a=\begin{pmatrix}n\\0\end{pmatrix} ,  b=\begin{pmatrix}0\\n\end{pmatrix} ,  c=\begin{pmatrix}n\\n\end{pmatrix} . Then  \|a\|_1=\|b\|_1=\frac{1}{2}\|c\|_1 = n,  but  \|a-b\|_1 = 2n,  i.e. there is no upper bound.    I'm not sure for which norms you can generalize this kind of argument.  	0.9576398730278015
1837498	"<p>The standard <a href=""https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality"" rel=""nofollow"">Holder's inequality</a> (for points in <span class=""math-container"" id=""16997702"">\mathbb{R}^n</span>) can be written as</p>  <p><span class=""math-container"" id=""16997703""> \|fg\|_p \leq \|f\|_q \|g\|_r </span></p>  <p>where <span class=""math-container"" id=""16997704"">p^{-1} = q^{-1} + r^{-1}</span>; note that this requires <span class=""math-container"" id=""16997705"">q, r \geq p</span>. </p>  <p>The inequality you wrote above follows by taking <span class=""math-container"" id=""16997706"">r^{-1} = p^{-1} - q^{-1}</span> and <span class=""math-container"" id=""16997707"">g = \mathbf{1}</span>. </p>  <hr>  <p>The version above can be proved by taking <span class=""math-container"" id=""16997708"">\tilde{f} = |f|^p</span> and <span class=""math-container"" id=""16997709"">\tilde{g} = |g|^p</span> and applying the standard version quoted in Wikipedia. </p>  <hr>  <p>In arbitrary measure spaces, what you have is the estimate</p>  <p><span class=""math-container"" id=""16997710""> \|x\|_p \leq |\mathrm{supp}(x)|^{\frac1p -\frac1q} \|x\|_q </span></p>  <p>for every <span class=""math-container"" id=""16997711"">q \geq p</span>, provided the support of the function <span class=""math-container"" id=""16997712"">x</span> has finite measure. This is sometimes useful in probability theory. </p> "	The standard  Holder's inequality  (for points in  \mathbb{R}^n ) can be written as     \|fg\|_p \leq \|f\|_q \|g\|_r     where  p^{-1} = q^{-1} + r^{-1} ; note that this requires  q, r \geq p .     The inequality you wrote above follows by taking  r^{-1} = p^{-1} - q^{-1}  and  g = \mathbf{1} .        The version above can be proved by taking  \tilde{f} = |f|^p  and  \tilde{g} = |g|^p  and applying the standard version quoted in Wikipedia.        In arbitrary measure spaces, what you have is the estimate     \|x\|_p \leq |\mathrm{supp}(x)|^{\frac1p -\frac1q} \|x\|_q     for every  q \geq p , provided the support of the function  x  has finite measure. This is sometimes useful in probability theory.   	0.957619309425354
268353	"<p>The inequality is true. It is obvious when <span class=""math-container"" id=""2968090"">A</span> is singular. When <span class=""math-container"" id=""2968091"">A</span> is invertible, for any unit vector <span class=""math-container"" id=""2968092"">x</span>, we have <span class=""math-container"" id=""2968093"">\|x^TA\|\ge\sigma_\min(A)</span>. Therefore \begin{align} \|AB\| &amp;= \max_{\|x\|=1} \|x^TAB\|\\ &amp;= \max_{\|x\|=1} \|x^TA\|\left\|\frac{x^TA}{\|x^TA\|}B\right\|\tag{1}\\ &amp;\ge \max_{\|x\|=1} \sigma_\min(A)\left\|\frac{x^TA}{\|x^TA\|}B\right\|\\ &amp;= \max_{\|y\|=1} \sigma_\min(A)\left\|y^TB\right\|\tag{2}\\ &amp;= \sigma_\min(A)\|B\|, \end{align} where we have used the assumption that <span class=""math-container"" id=""2968094"">A</span> is invertible in <span class=""math-container"" id=""2968095"">(1)</span> (so that we can divide by <span class=""math-container"" id=""2968096"">\|x^TA\|\neq0</span>) and <span class=""math-container"" id=""2968097"">(2)</span> (so that every <span class=""math-container"" id=""2968098"">x</span> corresponds to a unique <span class=""math-container"" id=""2968099"">y</span> and vice versa).</p> "	The inequality is true. It is obvious when  A  is singular. When  A  is invertible, for any unit vector  x , we have  \|x^TA\|\ge\sigma_\min(A) . Therefore \begin{align} \|AB\| &= \max_{\|x\|=1} \|x^TAB\|\\ &= \max_{\|x\|=1} \|x^TA\|\left\|\frac{x^TA}{\|x^TA\|}B\right\|\tag{1}\\ &\ge \max_{\|x\|=1} \sigma_\min(A)\left\|\frac{x^TA}{\|x^TA\|}B\right\|\\ &= \max_{\|y\|=1} \sigma_\min(A)\left\|y^TB\right\|\tag{2}\\ &= \sigma_\min(A)\|B\|, \end{align} where we have used the assumption that  A  is invertible in  (1)  (so that we can divide by  \|x^TA\|\neq0 ) and  (2)  (so that every  x  corresponds to a unique  y  and vice versa).  	0.9574840068817139
2188338	"<p>I think it is best to reserve <span class=""math-container"" id=""20220293"">\|A\|_2</span> for the usual <span class=""math-container"" id=""20220294"">2</span>-norm <span class=""math-container"" id=""20220295"">\displaystyle \|A\|_2 = \sup_{\|x\|_2 = 1} \|Ax\|_2</span>.</p>  <p>You can use the singular value decomposition: there exist unitary matrices <span class=""math-container"" id=""20220296"">U</span> and <span class=""math-container"" id=""20220297"">V</span> and a matrix <span class=""math-container"" id=""20220298"">S</span> with singular values of <span class=""math-container"" id=""20220299"">A</span> along the diagonal and zeros elsewhere satisfying <span class=""math-container"" id=""20220300"">A = USV.</span></p>  <p>Suppose that <span class=""math-container"" id=""20220301"">x</span> is a vector with <span class=""math-container"" id=""20220302"">\|x\|_2 = 1</span>. Then you have <span class=""math-container"" id=""20220303"">\|Ax\|_2 = \|USVx\|_2 = \|SVx\|_2 \le \|S\|_2 \|Vx\|_2 = \|S\|_2.</span> Because of its particular form, the <span class=""math-container"" id=""20220304"">2</span>-norm of <span class=""math-container"" id=""20220305"">S</span> is its largest entry. Since its nonzero entries are the singular values of <span class=""math-container"" id=""20220306"">A</span>, this means <span class=""math-container"" id=""20220307"">\|S\|_2 = \sigma_1(A)</span> so that <span class=""math-container"" id=""20220308"">\|A\|_2 \le \sigma_1(A) .</span></p>  <p>The fact that <span class=""math-container"" id=""20220309"">\|A\|_\infty \le \sqrt n \|A\|_2</span> is easy to prove, giving you <span class=""math-container"" id=""20220310""> \frac 1 {\sqrt n} \|A\|_\infty \le \sigma_1(A).</span></p> "	I think it is best to reserve  \|A\|_2  for the usual  2 -norm  \displaystyle \|A\|_2 = \sup_{\|x\|_2 = 1} \|Ax\|_2 .    You can use the singular value decomposition: there exist unitary matrices  U  and  V  and a matrix  S  with singular values of  A  along the diagonal and zeros elsewhere satisfying  A = USV.    Suppose that  x  is a vector with  \|x\|_2 = 1 . Then you have  \|Ax\|_2 = \|USVx\|_2 = \|SVx\|_2 \le \|S\|_2 \|Vx\|_2 = \|S\|_2.  Because of its particular form, the  2 -norm of  S  is its largest entry. Since its nonzero entries are the singular values of  A , this means  \|S\|_2 = \sigma_1(A)  so that  \|A\|_2 \le \sigma_1(A) .    The fact that  \|A\|_\infty \le \sqrt n \|A\|_2  is easy to prove, giving you   \frac 1 {\sqrt n} \|A\|_\infty \le \sigma_1(A).  	0.9574557542800903
409819	"<p>Since <span class=""math-container"" id=""4332146"">\|f(x)\| = \|Ax\| \leq \|A\| \|x\|,</span> by the definition of the operator norm: <span class=""math-container"" id=""4332147""> \|A\| = \sup_{x\in \mathbb{R}^3}\frac{\|Ax\|}{\|x\|}. </span> In case of Euclidean norm and <span class=""math-container"" id=""4332148"">A: \mathbb{R}^3\to  \mathbb{R}^3</span> is a linear transformation: <span class=""math-container"" id=""4332149"">  \|A\|  = \sqrt{\lambda_{\mathrm{max}}(A^T A)}, </span> where <span class=""math-container"" id=""4332150"">\lambda_{\mathrm{max}}(A^T A)</span> is the largest eigenvalue of <span class=""math-container"" id=""4332151"">A^TA</span>, and you wanna check this is less than <span class=""math-container"" id=""4332152"">1</span>.</p>  <p>In your case:  <span class=""math-container"" id=""4332153""> A^T A = \begin{pmatrix}5/16 &amp; 0 &amp; 0 \\ 0 &amp; 1/9 &amp; 0\\ 0 &amp; 0 &amp; 5/16 \end{pmatrix}, </span> which gives the <span class=""math-container"" id=""4332154"">\lambda_{\mathrm{max}} = 5/16</span>, and <span class=""math-container"" id=""4332155"">\|A\| = \sqrt{5}/4 &lt; 1</span>, thus  <span class=""math-container"" id=""4332156""> \|f(x)\| = \|Ax\| \leq \|A\| \|x\| &lt; \frac{\sqrt{5}}{4}\|x\| </span> and <span class=""math-container"" id=""4332157"">f</span> is a contraction.</p> "	Since  \|f(x)\| = \|Ax\| \leq \|A\| \|x\|,  by the definition of the operator norm:   \|A\| = \sup_{x\in \mathbb{R}^3}\frac{\|Ax\|}{\|x\|}.   In case of Euclidean norm and  A: \mathbb{R}^3\to  \mathbb{R}^3  is a linear transformation:    \|A\|  = \sqrt{\lambda_{\mathrm{max}}(A^T A)},   where  \lambda_{\mathrm{max}}(A^T A)  is the largest eigenvalue of  A^TA , and you wanna check this is less than  1 .    In your case:    A^T A = \begin{pmatrix}5/16 & 0 & 0 \\ 0 & 1/9 & 0\\ 0 & 0 & 5/16 \end{pmatrix},   which gives the  \lambda_{\mathrm{max}} = 5/16 , and  \|A\| = \sqrt{5}/4 < 1 , thus    \|f(x)\| = \|Ax\| \leq \|A\| \|x\| < \frac{\sqrt{5}}{4}\|x\|   and  f  is a contraction.  	0.9573177695274353
2564840	"<p>It is also possible to reduce the problem to the better-known version of Poincare's inequality  <span class=""math-container"" id=""23690522""> \int_U |u - \bar{u}|^2 \le C(U)^2 \int_U |\nabla u|^2, </span> where <span class=""math-container"" id=""23690523"">\bar{u}</span> is the average of <span class=""math-container"" id=""23690524"">u</span> over <span class=""math-container"" id=""23690525"">U</span>. This way we can obtain a constant that depends on <span class=""math-container"" id=""23690526"">\alpha</span> explicitly. </p>  <p>Let <span class=""math-container"" id=""23690527"">Z = \{ u = 0 \}</span> be the zero set. Using Holder's inequality on <span class=""math-container"" id=""23690528"">U \setminus Z</span>, we get  \begin{align*} \int_U \bar{u}^2  &amp; = |U| \cdot \bar{u}^2 \\ &amp; = \frac{1}{|U|} \left( \int_{U \setminus Z} u \right)^2 \\ &amp; \le \frac{|U \setminus Z|}{|U|} \int_{U \setminus Z} u^2. \end{align*} It is important that the coefficient above is strictly smaller than <span class=""math-container"" id=""23690529"">1</span>, since this enables us to estimate  <span class=""math-container"" id=""23690530""> \| u \|_{L^2} \le \| u-\bar{u} \|_{L^2} + \| \bar{u} \|_{L^2} \le \| u-\bar{u} \|_{L^2} + \sqrt{1 - \frac{\alpha}{|U|}} \| u \|_{L^2}, </span> <span class=""math-container"" id=""23690531""> \left( 1 - \sqrt{1 - \frac{\alpha}{|U|}} \right) \| u \|_{L^2} \le \| u-\bar{u} \|_{L^2} \le C(U) \| \nabla u \|_{L^2} </span></p> "	It is also possible to reduce the problem to the better-known version of Poincare's inequality    \int_U |u - \bar{u}|^2 \le C(U)^2 \int_U |\nabla u|^2,   where  \bar{u}  is the average of  u  over  U . This way we can obtain a constant that depends on  \alpha  explicitly.     Let  Z = \{ u = 0 \}  be the zero set. Using Holder's inequality on  U \setminus Z , we get  \begin{align*} \int_U \bar{u}^2  & = |U| \cdot \bar{u}^2 \\ & = \frac{1}{|U|} \left( \int_{U \setminus Z} u \right)^2 \\ & \le \frac{|U \setminus Z|}{|U|} \int_{U \setminus Z} u^2. \end{align*} It is important that the coefficient above is strictly smaller than  1 , since this enables us to estimate    \| u \|_{L^2} \le \| u-\bar{u} \|_{L^2} + \| \bar{u} \|_{L^2} \le \| u-\bar{u} \|_{L^2} + \sqrt{1 - \frac{\alpha}{|U|}} \| u \|_{L^2},     \left( 1 - \sqrt{1 - \frac{\alpha}{|U|}} \right) \| u \|_{L^2} \le \| u-\bar{u} \|_{L^2} \le C(U) \| \nabla u \|_{L^2}   	0.9571924209594727
988765	"<p>Well, actually it is well known that <span class=""math-container"" id=""748410"">     \|A\| = \|A\|_{1\rightarrow\infty} = \max_{x\ne 0}\frac{\|Ax\|_{\infty}}{\|x\|_1}\,. </span> Thus, in particular, it is consistent with the corresponding norms: <span class=""math-container"" id=""748411"">     \|Ax\|_{\infty} \le \|A\| \|x\|_1\,. </span></p> "	Well, actually it is well known that       \|A\| = \|A\|_{1\rightarrow\infty} = \max_{x\ne 0}\frac{\|Ax\|_{\infty}}{\|x\|_1}\,.   Thus, in particular, it is consistent with the corresponding norms:       \|Ax\|_{\infty} \le \|A\| \|x\|_1\,.   	0.9571772217750549
1390361	"<p>Let <span class=""math-container"" id=""12913415"">\left \| \cdot  \right \| _{a}</span> and <span class=""math-container"" id=""12913416"">\left \| \cdot  \right \| _{b}</span> be norms on <span class=""math-container"" id=""12913417"">V</span>. </p>  <p>The key idea is that if <span class=""math-container"" id=""12913418"">\dim V&lt;\infty </span> then the unit ball in the topology induced by both norms is compact. </p>  <p>This means in particular that there is an <span class=""math-container"" id=""12913419"">M\geq 0</span> such that <span class=""math-container"" id=""12913420"">\left \| x \right \|_{a}\leq M</span> for all <span class=""math-container"" id=""12913421"">x\in V</span> such that <span class=""math-container"" id=""12913422"">\left \| x \right \|_{b}\leq1</span>. </p>  <p>But then <span class=""math-container"" id=""12913423"">\left \| \frac{x}{\left \| x \right \|_{b}} \right \|_{a}\leq M</span> for all <span class=""math-container"" id=""12913424"">x\in V</span>, or what is the same thing <span class=""math-container"" id=""12913425"">\left \| x \right \|_{a}\leq M\left \| x \right \|_{b}</span>. Reversing the roles of the norms, we get <span class=""math-container"" id=""12913426"">\left \| x \right \|_{b}\leq M'\left \| x \right \|_{1}</span> and the result follows. </p>  <p>Note: one does need to show that <span class=""math-container"" id=""12913427"">\left \| \cdot \right \|_{a}:V\to \mathbb R</span> is continuous on <span class=""math-container"" id=""12913428"">V</span> in the topology induced by <span class=""math-container"" id=""12913429"">\left \| \cdot \right \|_{b}</span> but this is fairly routine. </p> "	Let  \left \| \cdot  \right \| _{a}  and  \left \| \cdot  \right \| _{b}  be norms on  V .     The key idea is that if  \dim V<\infty   then the unit ball in the topology induced by both norms is compact.     This means in particular that there is an  M\geq 0  such that  \left \| x \right \|_{a}\leq M  for all  x\in V  such that  \left \| x \right \|_{b}\leq1 .     But then  \left \| \frac{x}{\left \| x \right \|_{b}} \right \|_{a}\leq M  for all  x\in V , or what is the same thing  \left \| x \right \|_{a}\leq M\left \| x \right \|_{b} . Reversing the roles of the norms, we get  \left \| x \right \|_{b}\leq M'\left \| x \right \|_{1}  and the result follows.     Note: one does need to show that  \left \| \cdot \right \|_{a}:V\to \mathbb R  is continuous on  V  in the topology induced by  \left \| \cdot \right \|_{b}  but this is fairly routine.   	0.9571467041969299
209991	"<p>In the space of positive semi-definite matrices, trace is a proper inner-product (it is easy to show that), i.e. it obeys the Cauchy-Schwarz inequality: <span class=""math-container"" id=""2396897"">\langle x,y \rangle \leq \sqrt{ \langle x,x \rangle \langle y,y\rangle}</span>. So</p>  <p><span class=""math-container"" id=""2396898"">\mbox{tr}\{AB\}\leq \sqrt{\mbox{tr}\{A^2\} \mbox{tr}\{B^2\}}</span></p>  <p>Now, since <span class=""math-container"" id=""2396899"">A</span> is positive semidefinite, <span class=""math-container"" id=""2396900"">\mbox{tr}\{A^2\} \leq \mbox{tr}\{A\}^2</span>, i.e., the eigenvalues of <span class=""math-container"" id=""2396901"">A^2</span> are squared eigenvalues of <span class=""math-container"" id=""2396902"">A</span>, and since they are positive</p>  <p><span class=""math-container"" id=""2396903"">\mbox{tr} \{A^2\} = \sum_{i=1}^{N}\lambda_{i}^{2}\leq \left( \sum_{i=1}^{N}\lambda_{i} \right)^{2} = \mbox{tr}\{A\}^2 \leq 1</span> </p>  <p>A similar argument for B proves <span class=""math-container"" id=""2396904"">\mbox{tr}\{B^2\}\leq 1</span> . So <span class=""math-container"" id=""2396905"">\mbox{tr}\{AB\}\leq 1</span>. Hope this answers your question. </p> "	In the space of positive semi-definite matrices, trace is a proper inner-product (it is easy to show that), i.e. it obeys the Cauchy-Schwarz inequality:  \langle x,y \rangle \leq \sqrt{ \langle x,x \rangle \langle y,y\rangle} . So    \mbox{tr}\{AB\}\leq \sqrt{\mbox{tr}\{A^2\} \mbox{tr}\{B^2\}}    Now, since  A  is positive semidefinite,  \mbox{tr}\{A^2\} \leq \mbox{tr}\{A\}^2 , i.e., the eigenvalues of  A^2  are squared eigenvalues of  A , and since they are positive    \mbox{tr} \{A^2\} = \sum_{i=1}^{N}\lambda_{i}^{2}\leq \left( \sum_{i=1}^{N}\lambda_{i} \right)^{2} = \mbox{tr}\{A\}^2 \leq 1      A similar argument for B proves  \mbox{tr}\{B^2\}\leq 1  . So  \mbox{tr}\{AB\}\leq 1 . Hope this answers your question.   	0.9571452736854553
1208285	"<p>The operator norm is defined (equivalently) as any of :<br> <span class=""math-container"" id=""11307766"">\|A\| = \max_{\|x\|=1}\|Ax\|</span><br> <span class=""math-container"" id=""11307767"">\|A\| = \max_{\|x\|\leq1}\|Ax\|</span><br> <span class=""math-container"" id=""11307768"">\|A\| = \max_{\|x\|\neq0}\frac{\|Ax\|}{\|x\|}</span></p>  <p>Above, you test against <span class=""math-container"" id=""11307769"">k = (0,1,1)</span> (I'm not sure what you mean by <span class=""math-container"" id=""11307770"">e_1k_2+e_2k_3)</span> - you're multiplying vectors here, which makes little sense in context, and none at all without specifying if you are using the cross or dot product), which has (Euclidian) norm <span class=""math-container"" id=""11307771"">\sqrt{2}</span>, so if you wish to use it to test your operator norm, you need to use the third formula above, and <span class=""math-container"" id=""11307772"">\frac{\|Ak\|_2}{\|k\|_2} = \frac{\|(1,1,0)\|_2}{\|(0,1,1)\|_2} = \frac{\sqrt{2}}{\sqrt{2}} = 1</span>, so you have a witness that <span class=""math-container"" id=""11307773"">\|A\| \geq 1</span>. In order to prove that <span class=""math-container"" id=""11307774"">\|A\| = 1</span>, you now need to show that <span class=""math-container"" id=""11307775"">\|Ax\|_2 \leq \|x\|_2</span> for all <span class=""math-container"" id=""11307776"">x \in\mathbb{R}^3</span> (or possibly faster, using the first formula, <span class=""math-container"" id=""11307777"">\|Ax\|_2 \leq 1</span> for all <span class=""math-container"" id=""11307778"">x \in \mathbb{R}^3</span> with <span class=""math-container"" id=""11307779"">\|x\|_2 = 1</span>.</p>  <p>(NB: Throughout (except in the definition of the operator norm, which applies on any normed linear space) I've made the use of the Euclidian norm explicit by denoting it <span class=""math-container"" id=""11307780"">\|\cdot\|_2</span> - this is not actually important).</p> "	The operator norm is defined (equivalently) as any of :   \|A\| = \max_{\|x\|=1}\|Ax\|   \|A\| = \max_{\|x\|\leq1}\|Ax\|   \|A\| = \max_{\|x\|\neq0}\frac{\|Ax\|}{\|x\|}    Above, you test against  k = (0,1,1)  (I'm not sure what you mean by  e_1k_2+e_2k_3)  - you're multiplying vectors here, which makes little sense in context, and none at all without specifying if you are using the cross or dot product), which has (Euclidian) norm  \sqrt{2} , so if you wish to use it to test your operator norm, you need to use the third formula above, and  \frac{\|Ak\|_2}{\|k\|_2} = \frac{\|(1,1,0)\|_2}{\|(0,1,1)\|_2} = \frac{\sqrt{2}}{\sqrt{2}} = 1 , so you have a witness that  \|A\| \geq 1 . In order to prove that  \|A\| = 1 , you now need to show that  \|Ax\|_2 \leq \|x\|_2  for all  x \in\mathbb{R}^3  (or possibly faster, using the first formula,  \|Ax\|_2 \leq 1  for all  x \in \mathbb{R}^3  with  \|x\|_2 = 1 .    (NB: Throughout (except in the definition of the operator norm, which applies on any normed linear space) I've made the use of the Euclidian norm explicit by denoting it  \|\cdot\|_2  - this is not actually important).  	0.9571279883384705
2391721	"<p>Since <span class=""math-container"" id=""22134764"">\|A_1^{-1}\|_2=\|(A_1^\ast)^{-1}\|_2</span>, it suffices to prove that for every nonzero vector <span class=""math-container"" id=""22134765"">x\in\mathbb C^m</span>, there exists some nonzero vector <span class=""math-container"" id=""22134766"">u\in\mathbb C^n</span> such that <span class=""math-container"" id=""22134767""> \|A^+x\|_2/\|x\|_2\le \|(A_1^\ast)^{-1}u\|_2/\|u\|_2.\tag{1} </span> <span class=""math-container"" id=""22134768"">(1)</span> is evident when <span class=""math-container"" id=""22134769"">A^+x=0</span>. So, suppose <span class=""math-container"" id=""22134770"">A^+x\ne0</span>. Pick <span class=""math-container"" id=""22134771"">u=A^+x</span>. Then it suffices to show that <span class=""math-container"" id=""22134772""> \|u\|_2^2\le \|x\|_2\|(A_1^\ast)^{-1}u\|_2.\tag{2} </span> Since <span class=""math-container"" id=""22134773"">AA^+</span> is an orthogonal projection, <span class=""math-container"" id=""22134774"">\|x\|_2\ge\|AA^+x\|_2=\|Au\|_2\ge\|A_1u\|_2</span>. Hence it suffices to prove that <span class=""math-container"" id=""22134775""> \|u\|_2^2\le \|A_1u\|_2\|(A_1^\ast)^{-1}u\|_2,\tag{3} </span> but this is just Cauchy-Schwarz inequality.</p> "	Since  \|A_1^{-1}\|_2=\|(A_1^\ast)^{-1}\|_2 , it suffices to prove that for every nonzero vector  x\in\mathbb C^m , there exists some nonzero vector  u\in\mathbb C^n  such that   \|A^+x\|_2/\|x\|_2\le \|(A_1^\ast)^{-1}u\|_2/\|u\|_2.\tag{1}    (1)  is evident when  A^+x=0 . So, suppose  A^+x\ne0 . Pick  u=A^+x . Then it suffices to show that   \|u\|_2^2\le \|x\|_2\|(A_1^\ast)^{-1}u\|_2.\tag{2}   Since  AA^+  is an orthogonal projection,  \|x\|_2\ge\|AA^+x\|_2=\|Au\|_2\ge\|A_1u\|_2 . Hence it suffices to prove that   \|u\|_2^2\le \|A_1u\|_2\|(A_1^\ast)^{-1}u\|_2,\tag{3}   but this is just Cauchy-Schwarz inequality.  	0.9571136236190796
1589238	"<p>The desired inequality is false, even with constants, since it does not have the right sort of homogeneity. Take a function <span class=""math-container"" id=""14682723"">f</span> for which the inequality does hold, and define</p>  <p><span class=""math-container"" id=""14682724"">g_{\lambda}(x) = \lambda^{d} f(\lambda x)</span></p>  <p>This is the <span class=""math-container"" id=""14682725"">L^1</span> dilation of <span class=""math-container"" id=""14682726"">f</span>, and <span class=""math-container"" id=""14682727"">\|g_{\lambda} \|_1 = \|f\|_1</span> for all <span class=""math-container"" id=""14682728"">\lambda &gt; 0</span>. On the other hand,</p>  <p>\begin{align*} \big| |x|^{2d} g_{\lambda}(x) \big| = \lambda^{-d} \big| |\lambda x|^{2d} f(\lambda x)\big| = \lambda^{-d} |y|^{2d} |f(y)| \end{align*} It follows that  <span class=""math-container"" id=""14682729"">\big\| |.|^{2d} g_{\lambda} \big\| = \lambda^{-d} \big\||.|^{2d} f\big\|</span> But then we have <span class=""math-container"" id=""14682730"">\|g_{\lambda}\|_1 = \|f\|_1 \le \big\||.|^{2d} f\big\|_{\infty} = \lambda^d \big\||.|^{2d} g_{\lambda}\big\|_{\infty}</span> Now send <span class=""math-container"" id=""14682731"">\lambda</span> to zero to find a counterexample.</p> "	The desired inequality is false, even with constants, since it does not have the right sort of homogeneity. Take a function  f  for which the inequality does hold, and define    g_{\lambda}(x) = \lambda^{d} f(\lambda x)    This is the  L^1  dilation of  f , and  \|g_{\lambda} \|_1 = \|f\|_1  for all  \lambda > 0 . On the other hand,    \begin{align*} \big| |x|^{2d} g_{\lambda}(x) \big| = \lambda^{-d} \big| |\lambda x|^{2d} f(\lambda x)\big| = \lambda^{-d} |y|^{2d} |f(y)| \end{align*} It follows that   \big\| |.|^{2d} g_{\lambda} \big\| = \lambda^{-d} \big\||.|^{2d} f\big\|  But then we have  \|g_{\lambda}\|_1 = \|f\|_1 \le \big\||.|^{2d} f\big\|_{\infty} = \lambda^d \big\||.|^{2d} g_{\lambda}\big\|_{\infty}  Now send  \lambda  to zero to find a counterexample.  	0.9570361971855164
660195	"<p>The reverse inequality follows from the fact that <span class=""math-container"" id=""6624094"">\|B\|_2 = \sigma</span>.</p>  <p>Let <span class=""math-container"" id=""6624095"">u</span> and <span class=""math-container"" id=""6624096"">v</span> be unit vectors. Then \begin{align*} | u^* B v | &amp;\leq \|u \|_2 \|B v\|_2 \qquad \text{(by Cauchy-Schwarz inequality)}\\ &amp;\leq \|u\|_2 \|B\|_2 \|v\|_2 \\ &amp;= \sigma. \end{align*}</p>  <p>It follows that  \begin{equation} \max_{\|u\|_2=1,\|v\|_2=1} | u^* B v| \leq \sigma. \end{equation}</p> "	The reverse inequality follows from the fact that  \|B\|_2 = \sigma .    Let  u  and  v  be unit vectors. Then \begin{align*} | u^* B v | &\leq \|u \|_2 \|B v\|_2 \qquad \text{(by Cauchy-Schwarz inequality)}\\ &\leq \|u\|_2 \|B\|_2 \|v\|_2 \\ &= \sigma. \end{align*}    It follows that  \begin{equation} \max_{\|u\|_2=1,\|v\|_2=1} | u^* B v| \leq \sigma. \end{equation}  	0.9569337964057922
1668926	"<p>I think it is worth mentioning a proof using the Fourier analytic definition of <span class=""math-container"" id=""15403170"">H^s</span>, if only for its succinctness.</p>  <p>We have <span class=""math-container"" id=""15403171""> \| u \|_{L^\infty} \leq \| \hat{u} \|_1 \leq \| \langle \xi \rangle^{-s} \|_2 \| \langle \xi \rangle^s \hat{u} \|_2 \leq C \| u \|_{H^s}.</span> Here <span class=""math-container"" id=""15403172"">\langle \xi \rangle = \sqrt{1 + \lvert \xi \rvert^2}</span>. Interpolation with <span class=""math-container"" id=""15403173"">L^2</span> implies <span class=""math-container"" id=""15403174""> \| u \|_p \leq C(s) \| u \|_{H^s} \quad \forall 2 \leq p \leq \infty. </span></p> "	I think it is worth mentioning a proof using the Fourier analytic definition of  H^s , if only for its succinctness.    We have   \| u \|_{L^\infty} \leq \| \hat{u} \|_1 \leq \| \langle \xi \rangle^{-s} \|_2 \| \langle \xi \rangle^s \hat{u} \|_2 \leq C \| u \|_{H^s}.  Here  \langle \xi \rangle = \sqrt{1 + \lvert \xi \rvert^2} . Interpolation with  L^2  implies   \| u \|_p \leq C(s) \| u \|_{H^s} \quad \forall 2 \leq p \leq \infty.   	0.9568707346916199
391484	"<p>We can (for example) consider two cases:</p>  <p>Case 1 - <span class=""math-container"" id=""4151320"">q&gt; p</span></p>  <p>We have that <span class=""math-container"" id=""4151321"">\|u\|_p^p=\int_I |u|^p</span>. Because <span class=""math-container"" id=""4151322"">q&gt;p</span>, we have that <span class=""math-container"" id=""4151323"">\frac{q}{p}&gt;1</span>, hence we can apply Holder inequality, to conclude that <span class=""math-container"" id=""4151324"">\|u\|_p^p\leq C \|u\|_q^{p}</span>, where <span class=""math-container"" id=""4151325"">C&gt;0</span> is a positive constant (that can change in every line). From it we have that <span class=""math-container"" id=""4151326"">\|u\|_{1,p}=\|u\|_p+\|u'\|_p\leq C\|u\|_q+\|u'\|_p\leq C\||u\||</span></p>  <p>Case 2- <span class=""math-container"" id=""4151327"">q\leq p</span></p>  <p>In this case <span class=""math-container"" id=""4151328"">W^{1,p}(I)</span> is embedded in <span class=""math-container"" id=""4151329"">W^{1,q}(I)</span>, hence <span class=""math-container"" id=""4151330"">\|u\|_p=\Big(\int_I|u|^p\Big)^{1/p}\leq C\|u\|_\infty\leq C\|u\|_{1,q}</span>. From here you can conclude.</p> "	We can (for example) consider two cases:    Case 1 -  q> p    We have that  \|u\|_p^p=\int_I |u|^p . Because  q>p , we have that  \frac{q}{p}>1 , hence we can apply Holder inequality, to conclude that  \|u\|_p^p\leq C \|u\|_q^{p} , where  C>0  is a positive constant (that can change in every line). From it we have that  \|u\|_{1,p}=\|u\|_p+\|u'\|_p\leq C\|u\|_q+\|u'\|_p\leq C\||u\||    Case 2-  q\leq p    In this case  W^{1,p}(I)  is embedded in  W^{1,q}(I) , hence  \|u\|_p=\Big(\int_I|u|^p\Big)^{1/p}\leq C\|u\|_\infty\leq C\|u\|_{1,q} . From here you can conclude.  	0.956832766532898
1232057	"<p>You need to cut the study in two :</p>  <p>On <span class=""math-container"" id=""11509660"">A=[-1,1]</span>, you have <span class=""math-container"" id=""11509661"">\|f\|_{1,A}^2 \leq 2\|f\|_{2,A}^2 \leq 2\|f\|_2^2</span> (Jensen inequality)</p>  <p>On <span class=""math-container"" id=""11509662"">A^c=[-1,1]^c</span>, you have </p>  <p><span class=""math-container"" id=""11509663"">\|f\|_{1,A^c} = \left\| \frac{g}{x} \right\|_{1,A^c} \leq \|g\|_{2,A^c}\|\frac{1}{x}\|_{2,A^c} \leq \sqrt{2} \|g\|_2</span></p>  <p>Hence </p>  <p><span class=""math-container"" id=""11509664"">\|f\|_1 \leq \sqrt{2}(\|f\|_2+\|g\|_2)</span> </p> "	You need to cut the study in two :    On  A=[-1,1] , you have  \|f\|_{1,A}^2 \leq 2\|f\|_{2,A}^2 \leq 2\|f\|_2^2  (Jensen inequality)    On  A^c=[-1,1]^c , you have     \|f\|_{1,A^c} = \left\| \frac{g}{x} \right\|_{1,A^c} \leq \|g\|_{2,A^c}\|\frac{1}{x}\|_{2,A^c} \leq \sqrt{2} \|g\|_2    Hence     \|f\|_1 \leq \sqrt{2}(\|f\|_2+\|g\|_2)    	0.95682692527771
2860734	"<p>If you read <a href=""http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"" rel=""nofollow noreferrer"">Boyd</a> in chapter six there is regularization and least squares problems. Regularization follows the following problem like this.</p>  <p><span class=""math-container"" id=""26439320""> \textrm{ minimize w.r.t }R_{+}^{2} (\| Ax -b\|,\|x \|) </span></p>  <p>this is called the bi-criterion problem which is a convex optimization problem.</p>  <p>Regularization has a general pattern which looks like this  <span class=""math-container"" id=""26439321""> \textrm{ minimize}  \| Ax -b\| +  \gamma \|x \| </span></p>  <p>Where we have a parameter <span class=""math-container"" id=""26439322""> \gamma \in (0,\infty) </span>  which is our regularization parameter. In the case of <span class=""math-container"" id=""26439323"">\ell_{2}</span> regularization we have </p>  <p><span class=""math-container"" id=""26439324""> \textrm{ minimize}  \| Ax -b\|_{2} +  \delta \|x \|_{2} </span></p>  <p>where our 2-norm  here <span class=""math-container"" id=""26439325"">\|x \|_{2} = \left( \sum_{i=1}^{m} |x_{i} |^{2} \right)^{\frac{1}{2}}</span></p>  <p>The superscript simply means</p>  <p><span class=""math-container"" id=""26439326""> \| x \|_{2}^{2} =  \sum_{i=1}^{m} |x_{i} |^{2} </span></p> "	If you read  Boyd  in chapter six there is regularization and least squares problems. Regularization follows the following problem like this.     \textrm{ minimize w.r.t }R_{+}^{2} (\| Ax -b\|,\|x \|)     this is called the bi-criterion problem which is a convex optimization problem.    Regularization has a general pattern which looks like this    \textrm{ minimize}  \| Ax -b\| +  \gamma \|x \|     Where we have a parameter   \gamma \in (0,\infty)    which is our regularization parameter. In the case of  \ell_{2}  regularization we have      \textrm{ minimize}  \| Ax -b\|_{2} +  \delta \|x \|_{2}     where our 2-norm  here  \|x \|_{2} = \left( \sum_{i=1}^{m} |x_{i} |^{2} \right)^{\frac{1}{2}}    The superscript simply means     \| x \|_{2}^{2} =  \sum_{i=1}^{m} |x_{i} |^{2}   	0.9568090438842773
2558986	"<p>Hölder's inequality <span class=""math-container"" id=""23658423"">\Rightarrow</span> <span class=""math-container"" id=""23658424"">|f(x)|\leq  \displaystyle\bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}\displaystyle\bigg(\sum_{k=1}^{\infty} |x_k|^p\bigg)^{1/p}\leq \displaystyle\bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}\|x\|_{\ell_p}</span> <span class=""math-container"" id=""23658425"">\Longrightarrow</span> <span class=""math-container"" id=""23658426"">\|f\|\leq \bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}.</span></p>  <p>But here <span class=""math-container"" id=""23658427"">q</span> must be greater than <span class=""math-container"" id=""23658428"">2</span> in order for the sum to exist. For the other direction think about a specific element of <span class=""math-container"" id=""23658429"">\ell_p</span>. Tha is, take an <span class=""math-container"" id=""23658430"">x\in\ell_p</span> and use the fact that <span class=""math-container"" id=""23658431"">|f(x)|\leq \|f\|\|x\|_{\ell_p}.</span></p> "	Hölder's inequality  \Rightarrow   |f(x)|\leq  \displaystyle\bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}\displaystyle\bigg(\sum_{k=1}^{\infty} |x_k|^p\bigg)^{1/p}\leq \displaystyle\bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}\|x\|_{\ell_p}   \Longrightarrow   \|f\|\leq \bigg(\sum_{k=1}^{\infty}\frac{1}{k^{q/2}}\bigg)^{1/q}.    But here  q  must be greater than  2  in order for the sum to exist. For the other direction think about a specific element of  \ell_p . Tha is, take an  x\in\ell_p  and use the fact that  |f(x)|\leq \|f\|\|x\|_{\ell_p}.  	0.9567997455596924
1808529	"<p>Let <span class=""math-container"" id=""16717525"">A</span> denote the transformation defined by <span class=""math-container"" id=""16717526"">Ae_{j}=v_{j}</span> for <span class=""math-container"" id=""16717527"">j=1,\ldots,n</span>. First, I claim that <span class=""math-container"" id=""16717528"">\|A\|=\|A\|_{op}\leq \sqrt{n}</span>. Indeed, for <span class=""math-container"" id=""16717529"">x=\sum_{j}\lambda_{j}e_{j}</span>, we have</p>  <p><span class=""math-container"" id=""16717530"">\|Ax\|\leq\sum_{j}|\lambda_{j}|\|Ae_{j}\|=\sum_{j}|\lambda_{j}|\leq\sqrt{n}(\sum_{j}|\lambda_{j}|^{2})^{1/2}</span></p>  <p>where we use the hypothesis that the <span class=""math-container"" id=""16717531"">v_{j}</span> are unit vectors and convexity.</p>  <p>By the SVD, w.l.o.g. we may assume that the matrix of <span class=""math-container"" id=""16717532"">A</span> relative to the standard basis is diagonal with entries <span class=""math-container"" id=""16717533"">\sigma_{1},\ldots,\sigma_{n}</span>. Since <span class=""math-container"" id=""16717534"">\|n^{-1/2}A\|\leq 1</span>, we have that <span class=""math-container"" id=""16717535"">|n^{-1/2}\sigma_{j}|\leq 1</span> for <span class=""math-container"" id=""16717536"">j=1,\ldots,n</span>. Whence,</p>  <p><span class=""math-container"" id=""16717537"">\frac{\theta}{n^{n/2}}=\frac{|\sigma_{1}\cdots\sigma_{n}|}{n^{n/2}}\leq\frac{|\sigma_{j}|}{n^{1/2}}</span> for any <span class=""math-container"" id=""16717538"">j=1,\ldots,n</span>. Therefore <span class=""math-container"" id=""16717539"">|\sigma_{j}|\geq n^{\frac{n-1}{2}}\theta</span>, from which it follows that <span class=""math-container"" id=""16717540"">\|T\|=\|A^{-1}\|\lesssim_{n}\theta^{-1}</span>.</p> "	Let  A  denote the transformation defined by  Ae_{j}=v_{j}  for  j=1,\ldots,n . First, I claim that  \|A\|=\|A\|_{op}\leq \sqrt{n} . Indeed, for  x=\sum_{j}\lambda_{j}e_{j} , we have    \|Ax\|\leq\sum_{j}|\lambda_{j}|\|Ae_{j}\|=\sum_{j}|\lambda_{j}|\leq\sqrt{n}(\sum_{j}|\lambda_{j}|^{2})^{1/2}    where we use the hypothesis that the  v_{j}  are unit vectors and convexity.    By the SVD, w.l.o.g. we may assume that the matrix of  A  relative to the standard basis is diagonal with entries  \sigma_{1},\ldots,\sigma_{n} . Since  \|n^{-1/2}A\|\leq 1 , we have that  |n^{-1/2}\sigma_{j}|\leq 1  for  j=1,\ldots,n . Whence,    \frac{\theta}{n^{n/2}}=\frac{|\sigma_{1}\cdots\sigma_{n}|}{n^{n/2}}\leq\frac{|\sigma_{j}|}{n^{1/2}}  for any  j=1,\ldots,n . Therefore  |\sigma_{j}|\geq n^{\frac{n-1}{2}}\theta , from which it follows that  \|T\|=\|A^{-1}\|\lesssim_{n}\theta^{-1} .  	0.9567233920097351
2711529	"<p>About Hölder and your upper bound, OK: <span class=""math-container"" id=""25027325""> \|Tx\|_1 = \sum_{k=1}^\infty\left|\frac{x_k}{2^k}\right|\le \left(\sum_{k=1}^\infty 2^{-2k}\right)^{1/2} \left(\sum_{k=1}^\infty x_k^2\right)^{1/2} = \frac1{\sqrt 3}\|x\|_2. </span> About finding <span class=""math-container"" id=""25027326"">x</span> with <span class=""math-container"" id=""25027327"">\|Tx\|_1\approx\frac1{\sqrt 3}\|x\|_2</span> (equality <em>can</em> be impossible), see the conditions for equality in the Hölder inequality.</p> "	About Hölder and your upper bound, OK:   \|Tx\|_1 = \sum_{k=1}^\infty\left|\frac{x_k}{2^k}\right|\le \left(\sum_{k=1}^\infty 2^{-2k}\right)^{1/2} \left(\sum_{k=1}^\infty x_k^2\right)^{1/2} = \frac1{\sqrt 3}\|x\|_2.   About finding  x  with  \|Tx\|_1\approx\frac1{\sqrt 3}\|x\|_2  (equality  can  be impossible), see the conditions for equality in the Hölder inequality.  	0.956555962562561
1589145	"<p>A possible solution steps:</p>  <ol> <li>Prove that  <span class=""math-container"" id=""14681699""> \int_0^1\frac{1}{|x-t|^{1/2}}\,dt=2\sqrt{x}+2\sqrt{1-x}\le 2\sqrt{2}. </span></li> <li>Prove that <span class=""math-container"" id=""14681700"">\|g\|_\infty\le 2\sqrt{2}\|f\|_\infty</span> (simple estimation by 1).</li> <li>Prove that <span class=""math-container"" id=""14681701"">\|g\|_1\le 2\sqrt{2}\|f\|_1</span> (using e.g. <a href=""https://en.wikipedia.org/wiki/Fubini%27s_theorem#Tonelli.27s_theorem_for_non-negative_functions"" rel=""nofollow"">Tonelli's theorem</a> and 1).</li> <li>Conclude that <span class=""math-container"" id=""14681702"">\|g\|_2\le 2\sqrt{2}\|f\|_2</span> by the <a href=""https://en.wikipedia.org/wiki/Riesz%E2%80%93Thorin_theorem#Statement_of_the_theorem"" rel=""nofollow"">Riesz-Thorin theorem</a>.</li> </ol> "	A possible solution steps:      Prove that    \int_0^1\frac{1}{|x-t|^{1/2}}\,dt=2\sqrt{x}+2\sqrt{1-x}\le 2\sqrt{2}.    Prove that  \|g\|_\infty\le 2\sqrt{2}\|f\|_\infty  (simple estimation by 1).   Prove that  \|g\|_1\le 2\sqrt{2}\|f\|_1  (using e.g.  Tonelli's theorem  and 1).   Conclude that  \|g\|_2\le 2\sqrt{2}\|f\|_2  by the  Riesz-Thorin theorem .    	0.956528902053833
1155831	"<p>No. Chose the norm <span class=""math-container"" id=""10861290"">\|\cdot\|_\text{me}</span> defined by <span class=""math-container"" id=""10861291"">\|x\|_\text{me} :=\frac12 \|x\|_\infty</span>. You can easily check that this is in fact a norm, but <span class=""math-container"" id=""10861292"">\|2\cdot e_1\|_\text{me} = 1</span>.</p>  <p>There is however the following theorem (a special case of the fact that all norms on <span class=""math-container"" id=""10861293"">\mathbb R^n</span> are equivalent):</p>  <blockquote>   <p>For any norm <span class=""math-container"" id=""10861294"">\|\cdot\|</span> on <span class=""math-container"" id=""10861295"">\mathbb R^n</span> there are constants <span class=""math-container"" id=""10861296"">c</span> and <span class=""math-container"" id=""10861297"">C</span> depending on the norm such that   <span class=""math-container"" id=""10861298"">\|x\| &lt; 1 \Rightarrow \|x\|_\infty &lt; C\\ \|x\|_\infty &lt; c \Rightarrow \|x\| &lt; 1</span>   Or equivalently   <span class=""math-container"" id=""10861299"">\frac1C\|x\|_\infty \le \|x\| \le \frac1c\|x\|_\infty</span></p> </blockquote> "	No. Chose the norm  \|\cdot\|_\text{me}  defined by  \|x\|_\text{me} :=\frac12 \|x\|_\infty . You can easily check that this is in fact a norm, but  \|2\cdot e_1\|_\text{me} = 1 .    There is however the following theorem (a special case of the fact that all norms on  \mathbb R^n  are equivalent):        For any norm  \|\cdot\|  on  \mathbb R^n  there are constants  c  and  C  depending on the norm such that    \|x\| < 1 \Rightarrow \|x\|_\infty < C\\ \|x\|_\infty < c \Rightarrow \|x\| < 1    Or equivalently    \frac1C\|x\|_\infty \le \|x\| \le \frac1c\|x\|_\infty    	0.9564502239227295
288740	"<p>Presumably <span class=""math-container"" id=""3156724"">A</span> is a real <span class=""math-container"" id=""3156725"">n\times n</span> matrix with all its eigenvalues being real. The inequality is false in general (see chaohuang's answer for a counterexample), but it is true in each of the following circumstances (exercises):</p>  <ul> <li><span class=""math-container"" id=""3156726"">A</span> is real symmetric (in this case, <span class=""math-container"" id=""3156727"">A</span> is guaranteed to have only real eigenvalues),</li> <li><span class=""math-container"" id=""3156728"">A</span> is a <a href=""http://en.wikipedia.org/wiki/Doubly_stochastic_matrix"" rel=""nofollow"">doubly stochastic matrix</a> (hint: <a href=""http://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem"" rel=""nofollow"">Perron-Frobenius theorem</a>),</li> <li>the norm of every column of <span class=""math-container"" id=""3156729"">A</span> does not exceed <span class=""math-container"" id=""3156730"">\frac1{\sqrt{n}}</span> (hint: for any unit vector <span class=""math-container"" id=""3156731"">x</span>, we have <span class=""math-container"" id=""3156732"">\|Ax\|\le\sum_i|x_j|\|a_{\ast j}\|\le\|x\|\ \left\|\left(\|a_{\ast 1}\|,\ldots,\|a_{\ast n}\|\right)\right\|</span> by Cauchy-Schwarz inequalty),</li> <li>the norm of every row of <span class=""math-container"" id=""3156733"">A</span> does not exceed <span class=""math-container"" id=""3156734"">\frac1{\sqrt{n}}</span>,</li> <li><span class=""math-container"" id=""3156735"">\|A\|_1\|A\|_\infty\le1</span>, where <span class=""math-container"" id=""3156736"">\|A\|_1</span> and <span class=""math-container"" id=""3156737"">\|A\|_\infty</span> are respectively the maximum absolute column sum norm and the maximum absolute row sum norm of <span class=""math-container"" id=""3156738"">A</span> (hint: <span class=""math-container"" id=""3156739"">\rho(A^TA)\le\|A^TA\|_1\le\|A^T\|_1\|A\|_1</span>).</li> </ul> "	Presumably  A  is a real  n\times n  matrix with all its eigenvalues being real. The inequality is false in general (see chaohuang's answer for a counterexample), but it is true in each of the following circumstances (exercises):      A  is real symmetric (in this case,  A  is guaranteed to have only real eigenvalues),   A  is a  doubly stochastic matrix  (hint:  Perron-Frobenius theorem ),   the norm of every column of  A  does not exceed  \frac1{\sqrt{n}}  (hint: for any unit vector  x , we have  \|Ax\|\le\sum_i|x_j|\|a_{\ast j}\|\le\|x\|\ \left\|\left(\|a_{\ast 1}\|,\ldots,\|a_{\ast n}\|\right)\right\|  by Cauchy-Schwarz inequalty),   the norm of every row of  A  does not exceed  \frac1{\sqrt{n}} ,   \|A\|_1\|A\|_\infty\le1 , where  \|A\|_1  and  \|A\|_\infty  are respectively the maximum absolute column sum norm and the maximum absolute row sum norm of  A  (hint:  \rho(A^TA)\le\|A^TA\|_1\le\|A^T\|_1\|A\|_1 ).    	0.9563927054405212
295569	"<p>It is not true in general that <span class=""math-container"" id=""3226082"">\rho(AB)\leq \rho(A)\rho(B)</span>. Consider: <span class=""math-container"" id=""3226083""> A=\left( \matrix{1&amp;0\\ 1&amp; 1}\right)\quad B=\left( \matrix{1&amp;1\\ 0&amp; 1}\right) </span> Then <span class=""math-container"" id=""3226084"">\rho(A)=\rho(B)=1</span>. But <span class=""math-container"" id=""3226085""> AB=\left( \matrix{1&amp;1\\ 1&amp; 2}\right) </span> has <span class=""math-container"" id=""3226086"">\rho(AB)=(3+\sqrt{5})/2</span>.</p>  <p>If <span class=""math-container"" id=""3226087"">A</span> and <span class=""math-container"" id=""3226088"">B</span> commute, we have <span class=""math-container"" id=""3226089""> \|(AB)^n\|=\|A^nB^n\|\leq \|A^n\|\|B^n\| </span> hence <span class=""math-container"" id=""3226090""> \|(AB)^n\|^{1/n}\leq \|A^n\|^{1/n}\|B^n\|^{1/n}. </span></p>  <p>Letting <span class=""math-container"" id=""3226091"">n</span> tend to <span class=""math-container"" id=""3226092"">+\infty</span>, we find the desired inequality thanks to the Spectral Radius Formula (or Gelfand's formula): <a href=""http://en.wikipedia.org/wiki/Spectral_radius"">http://en.wikipedia.org/wiki/Spectral_radius</a> <span class=""math-container"" id=""3226093"">\rho(C)=\lim_{n\rightarrow +\infty}\|C^n\|^{1/n}.</span></p> "	It is not true in general that  \rho(AB)\leq \rho(A)\rho(B) . Consider:   A=\left( \matrix{1&0\\ 1& 1}\right)\quad B=\left( \matrix{1&1\\ 0& 1}\right)   Then  \rho(A)=\rho(B)=1 . But   AB=\left( \matrix{1&1\\ 1& 2}\right)   has  \rho(AB)=(3+\sqrt{5})/2 .    If  A  and  B  commute, we have   \|(AB)^n\|=\|A^nB^n\|\leq \|A^n\|\|B^n\|   hence   \|(AB)^n\|^{1/n}\leq \|A^n\|^{1/n}\|B^n\|^{1/n}.     Letting  n  tend to  +\infty , we find the desired inequality thanks to the Spectral Radius Formula (or Gelfand's formula):  http://en.wikipedia.org/wiki/Spectral_radius   \rho(C)=\lim_{n\rightarrow +\infty}\|C^n\|^{1/n}.  	0.9563659429550171
2739716	"<p><strong>A Rayleigh quotient approach:</strong></p>  <p>Assuming <span class=""math-container"" id=""25280807"">\|T\|=\max_{x\neq 0}\frac{\|Tx\|_2}{\|x\|_2},</span> and <span class=""math-container"" id=""25280808"">T</span> is symmetric, then, note that  <span class=""math-container"" id=""25280809"">R(x)=\frac{\|Tx\|_2^2}{\|x\|_2^2}=\frac{\langle Tx,Tx\rangle}{\langle x,x\rangle}=\frac{\langle T^2x,x\rangle}{\langle x,x\rangle}</span> which is nothing else than the <a href=""https://en.wikipedia.org/wiki/Rayleigh_quotient"" rel=""nofollow noreferrer"">Rayleigh quotient</a> of <span class=""math-container"" id=""25280810"">T^2</span>. Hence, we have  \begin{align*} \|T\|^2&amp;=\max_{x\neq 0} R(x) \\ &amp;=\max\{|\tilde \lambda|\colon \tilde\lambda \text{ is an eigenvalue of }T^2\}\\ &amp;=\max\{|\lambda|^2\colon \lambda \text{ is an eigenvalue of }T\}\\ &amp;=\max\{|\lambda|\colon \lambda \text{ is an eigenvalue of }T\}^2. \end{align*} Of course, any diagonal matrix is symmetric so the first part follows from this argument.</p> "	A Rayleigh quotient approach:    Assuming  \|T\|=\max_{x\neq 0}\frac{\|Tx\|_2}{\|x\|_2},  and  T  is symmetric, then, note that   R(x)=\frac{\|Tx\|_2^2}{\|x\|_2^2}=\frac{\langle Tx,Tx\rangle}{\langle x,x\rangle}=\frac{\langle T^2x,x\rangle}{\langle x,x\rangle}  which is nothing else than the  Rayleigh quotient  of  T^2 . Hence, we have  \begin{align*} \|T\|^2&=\max_{x\neq 0} R(x) \\ &=\max\{|\tilde \lambda|\colon \tilde\lambda \text{ is an eigenvalue of }T^2\}\\ &=\max\{|\lambda|^2\colon \lambda \text{ is an eigenvalue of }T\}\\ &=\max\{|\lambda|\colon \lambda \text{ is an eigenvalue of }T\}^2. \end{align*} Of course, any diagonal matrix is symmetric so the first part follows from this argument.  	0.956311821937561
1872311	"<p>Let <span class=""math-container"" id=""17343943"">\mathrm A \in [0,1)^{n \times n}</span> be a symmetric, positive definite, nonnegative matrix  whose diagonal entries are equal to <span class=""math-container"" id=""17343944"">1</span>. Using the <a href=""https://en.wikipedia.org/wiki/Gershgorin_circle_theorem"" rel=""nofollow"">Gershgorin circle theorem</a>, the minimum eigenvalue of <span class=""math-container"" id=""17343945"">\mathrm A</span> is bounded by</p>  <p><span class=""math-container"" id=""17343946"">\lambda_{\min} (\mathrm A) \geq 1 - \max_{1 \leq i \leq n} \, \sum_{j \neq i} a_{ij} = 1 - \| \mathrm A - \mathrm I_n\|_{\infty}</span></p>  <p>Unfortunately, unless <span class=""math-container"" id=""17343947"">a_{ij} \ll 1</span>, this bound is likely too loose. I suspect that this bound is only potentially useful for small perturbations of the identity matrix.</p>  <p>If the bound is not too loose, then, using the inequality <span class=""math-container"" id=""17343948"">\| \cdot \|_{\infty} \leq \sqrt n \, \| \cdot \|_{F}</span>, we obtain</p>  <p><span class=""math-container"" id=""17343949"">\lambda_{\min} (\mathrm A) \geq 1 - \| \mathrm A - \mathrm I_n \|_{\infty} \geq 1 - \sqrt n \, \| \mathrm A - \mathrm I_n \|_{F}</span></p> "	Let  \mathrm A \in [0,1)^{n \times n}  be a symmetric, positive definite, nonnegative matrix  whose diagonal entries are equal to  1 . Using the  Gershgorin circle theorem , the minimum eigenvalue of  \mathrm A  is bounded by    \lambda_{\min} (\mathrm A) \geq 1 - \max_{1 \leq i \leq n} \, \sum_{j \neq i} a_{ij} = 1 - \| \mathrm A - \mathrm I_n\|_{\infty}    Unfortunately, unless  a_{ij} \ll 1 , this bound is likely too loose. I suspect that this bound is only potentially useful for small perturbations of the identity matrix.    If the bound is not too loose, then, using the inequality  \| \cdot \|_{\infty} \leq \sqrt n \, \| \cdot \|_{F} , we obtain    \lambda_{\min} (\mathrm A) \geq 1 - \| \mathrm A - \mathrm I_n \|_{\infty} \geq 1 - \sqrt n \, \| \mathrm A - \mathrm I_n \|_{F}  	0.9561598896980286
1728844	"<p>No, it is not true. It can be confusing because they are usually indicated with the same symbol.</p>  <p>One is the Frobenius norm <span class=""math-container"" id=""15969485""> \|A\|_F = \left(\sum_{i,j} |a_{ij}|^2 \right)^{1/2} = \sqrt{\mathrm{Tr}(A^\dagger A)} = \left(\sum_i \sigma_i^2\right)^{1/2}, </span> where <span class=""math-container"" id=""15969486"">\sigma_i^2</span> are the eigenvalues of the positive matrix <span class=""math-container"" id=""15969487"">A^\dagger A</span>.</p>  <p>The Frobenius norm is the Euclidean norm of the vector of the singular values <span class=""math-container"" id=""15969488"">\sigma_i</span> of <span class=""math-container"" id=""15969489"">A</span>.</p>  <p>The other is the induced norm <span class=""math-container"" id=""15969490""> \|A\|_2 = \mathrm{sup}_{x\neq 0} \frac{|A x|_2}{|x|_2} = \mathrm{max}_i \sigma_i, </span> the largest singular value of <span class=""math-container"" id=""15969491"">A</span>.</p>  <p><span class=""math-container"" id=""15969492""> \|A\|_F = |\boldsymbol{\sigma}|_2 \leq |\boldsymbol{\sigma}|_\infty = \|A\|_2</span></p> "	No, it is not true. It can be confusing because they are usually indicated with the same symbol.    One is the Frobenius norm   \|A\|_F = \left(\sum_{i,j} |a_{ij}|^2 \right)^{1/2} = \sqrt{\mathrm{Tr}(A^\dagger A)} = \left(\sum_i \sigma_i^2\right)^{1/2},   where  \sigma_i^2  are the eigenvalues of the positive matrix  A^\dagger A .    The Frobenius norm is the Euclidean norm of the vector of the singular values  \sigma_i  of  A .    The other is the induced norm   \|A\|_2 = \mathrm{sup}_{x\neq 0} \frac{|A x|_2}{|x|_2} = \mathrm{max}_i \sigma_i,   the largest singular value of  A .     \|A\|_F = |\boldsymbol{\sigma}|_2 \leq |\boldsymbol{\sigma}|_\infty = \|A\|_2  	0.9561312198638916
2255376	"<p>Let's denote <span class=""math-container"" id=""20831975"">H=\partial^2 f</span> and <span class=""math-container"" id=""20831976"">v=x-a</span>. Your question why (I assume you are talking about the spectral norm) <span class=""math-container"" id=""20831977"">\|H\|\le\lambda_{\max}</span> is true can be answered as, in fact, <span class=""math-container"" id=""20831978""> \|H\|=\lambda_{\max} </span> for <a href=""https://en.wikipedia.org/wiki/Hermitian_matrix"" rel=""nofollow noreferrer"">Hermitian matrices</a>. It can be proved via unitary diagonalisation of Hermitian matrix <span class=""math-container"" id=""20831979""> H=UDU^* </span> where <span class=""math-container"" id=""20831980"">D=\text{diag}\{\lambda_1,\lambda_2,\ldots,\lambda_n\}</span> is the diagonal matrix of eigenvalues and <span class=""math-container"" id=""20831981"">U</span> is unitary, because <span class=""math-container"" id=""20831982"">\|H\|=\|D\|=\lambda_{\max}</span> (the spectral norm is unitary invariant and the norm of the diagonal matrix is an easy exercise).</p>  <p>It makes the answer to your second question trivial as <span class=""math-container"" id=""20831983"">\|H\|=\lambda_{\max}\ge\lambda_{\min}</span>. What is more interesting is to take a bit better estimations for <span class=""math-container"" id=""20831984""> \langle Hv,v\rangle. </span>  Using the property of <a href=""https://en.wikipedia.org/wiki/Rayleigh_quotient"" rel=""nofollow noreferrer"">Rayleigh quotient</a> it is easy to conclude that <span class=""math-container"" id=""20831985""> \lambda_{\min}\|v\|^2\le\langle Hv,v\rangle\le\lambda_{\max}\|v\|^2, </span> which fits your needs better.</p> "	Let's denote  H=\partial^2 f  and  v=x-a . Your question why (I assume you are talking about the spectral norm)  \|H\|\le\lambda_{\max}  is true can be answered as, in fact,   \|H\|=\lambda_{\max}   for  Hermitian matrices . It can be proved via unitary diagonalisation of Hermitian matrix   H=UDU^*   where  D=\text{diag}\{\lambda_1,\lambda_2,\ldots,\lambda_n\}  is the diagonal matrix of eigenvalues and  U  is unitary, because  \|H\|=\|D\|=\lambda_{\max}  (the spectral norm is unitary invariant and the norm of the diagonal matrix is an easy exercise).    It makes the answer to your second question trivial as  \|H\|=\lambda_{\max}\ge\lambda_{\min} . What is more interesting is to take a bit better estimations for   \langle Hv,v\rangle.    Using the property of  Rayleigh quotient  it is easy to conclude that   \lambda_{\min}\|v\|^2\le\langle Hv,v\rangle\le\lambda_{\max}\|v\|^2,   which fits your needs better.  	0.956122100353241
2030572	"<p><strong>Hint:</strong> Show that <span class=""math-container"" id=""18794574"">\lambda_{min}(A) \|x\|_2 \leq \|x\|_A \leq \lambda_{max}(A) \|x\|_2</span>.  It is helpful to start with the case in which <span class=""math-container"" id=""18794575"">A</span> is diagonal.</p>  <hr>  <p>For the new version of the question: define <span class=""math-container"" id=""18794576"">\|x\|_* = \|Ax\|</span>.  Notably, we have  <span class=""math-container"" id=""18794577""> \|x\|_*^2 = \langle Ax, Ax \rangle= \langle x,A^2 x \rangle\\ \|x\|_A^2 = \langle A^{1/2}x, A^{1/2}x \rangle= \langle x,A x \rangle </span> With that in mind: if we make the substitution <span class=""math-container"" id=""18794578"">y = A^{1/2}x</span>, then it suffices to show that for all <span class=""math-container"" id=""18794579"">y</span>, we have <span class=""math-container"" id=""18794580""> \lambda_{min}(A^{1/2})\|x\|_A = \lambda_{min}(A^{1/2}) \|y\|_2 \leq\\ \|y\|_{A^{1/2}} = \|x\|_* = \|Ax\| \leq\\ \lambda_{max}(A^{1/2}) \|y\|_2 = \lambda_{max}(A^{1/2})\|x\|_A </span></p> "	Hint:  Show that  \lambda_{min}(A) \|x\|_2 \leq \|x\|_A \leq \lambda_{max}(A) \|x\|_2 .  It is helpful to start with the case in which  A  is diagonal.       For the new version of the question: define  \|x\|_* = \|Ax\| .  Notably, we have    \|x\|_*^2 = \langle Ax, Ax \rangle= \langle x,A^2 x \rangle\\ \|x\|_A^2 = \langle A^{1/2}x, A^{1/2}x \rangle= \langle x,A x \rangle   With that in mind: if we make the substitution  y = A^{1/2}x , then it suffices to show that for all  y , we have   \lambda_{min}(A^{1/2})\|x\|_A = \lambda_{min}(A^{1/2}) \|y\|_2 \leq\\ \|y\|_{A^{1/2}} = \|x\|_* = \|Ax\| \leq\\ \lambda_{max}(A^{1/2}) \|y\|_2 = \lambda_{max}(A^{1/2})\|x\|_A   	0.9560655951499939
1309431	"<p><strong>Hint:</strong> note the following:</p>  <ul> <li><span class=""math-container"" id=""12178567"">\textrm{Cond}_2(A) = \|A\| \|A^{-1}\|</span></li> <li>Because <span class=""math-container"" id=""12178568"">A</span> is positive semidefinite, <span class=""math-container"" id=""12178569"">\|A\| = \max_{\|x\| = 1} x^TAx</span></li> <li>Because <span class=""math-container"" id=""12178570"">A</span> is positive semidefinite, <span class=""math-container"" id=""12178571"">\|A^{-1}\| = \max_{\|x\| = 1} \frac{1}{x^TAx}</span></li> </ul>  <p>For an lower bound of each of these maxima, plug in the <span class=""math-container"" id=""12178572"">j</span>th standard basis vector for <span class=""math-container"" id=""12178573"">x</span>.</p>  <p>Let <span class=""math-container"" id=""12178574"">L_j</span> denote the <span class=""math-container"" id=""12178575"">j</span>th row of <span class=""math-container"" id=""12178576"">L</span>.  We have <span class=""math-container"" id=""12178577""> e_j^T(LDL^T)e_j = d_{jj}\cdot \|L_j\|^2 \geq d_{jj} </span></p> "	Hint:  note the following:      \textrm{Cond}_2(A) = \|A\| \|A^{-1}\|   Because  A  is positive semidefinite,  \|A\| = \max_{\|x\| = 1} x^TAx   Because  A  is positive semidefinite,  \|A^{-1}\| = \max_{\|x\| = 1} \frac{1}{x^TAx}      For an lower bound of each of these maxima, plug in the  j th standard basis vector for  x .    Let  L_j  denote the  j th row of  L .  We have   e_j^T(LDL^T)e_j = d_{jj}\cdot \|L_j\|^2 \geq d_{jj}   	0.9560604095458984
1945004	"<p>I take it that you want to confirm that the operator norm is indeed a norm, and in particular that the triangle inequality is satisfied. Following the approach of Kreyzwig, let's show that <span class=""math-container"" id=""18029898"">\|T\|=\underbrace{\sup \|Tx\|}_{\|x\|=1, x \neq 0}.</span></p>  <p>Well, this is not so bad, let <span class=""math-container"" id=""18029899"">\|x\|=a</span>, and then let <span class=""math-container"" id=""18029900"">y=\frac{1}{a}x</span> for <span class=""math-container"" id=""18029901"">x \neq 0</span>. Then <span class=""math-container"" id=""18029902"">\|y\|=1</span>, and by linearity, we obtain:</p>  <p><span class=""math-container"" id=""18029903"">\|T\|=\sup\frac{1}{a}\|Tx\|=\sup\|T(\frac{1}{a}x)\|=\sup\|Ty\|</span> where <span class=""math-container"" id=""18029904"">\|y\|=1</span>. From this point of view, the result should seem a little more obvious, since we are taking a supremum over the values of functions applied to vectors of unit length. We get that</p>  <p><span class=""math-container"" id=""18029905"">\sup_{\|x\|=1}\|(T_1+T_2)x\|=\sup_{\|x\|=1}\|T_1x+T_2x\| \leq \sup_{\|x\|=1}\|T_1x\|+\sup_{\|x\|=1} \|+T_2x\|.</span></p>  <p>The last inequality follows from basic properties of the supremum.</p> "	I take it that you want to confirm that the operator norm is indeed a norm, and in particular that the triangle inequality is satisfied. Following the approach of Kreyzwig, let's show that  \|T\|=\underbrace{\sup \|Tx\|}_{\|x\|=1, x \neq 0}.    Well, this is not so bad, let  \|x\|=a , and then let  y=\frac{1}{a}x  for  x \neq 0 . Then  \|y\|=1 , and by linearity, we obtain:    \|T\|=\sup\frac{1}{a}\|Tx\|=\sup\|T(\frac{1}{a}x)\|=\sup\|Ty\|  where  \|y\|=1 . From this point of view, the result should seem a little more obvious, since we are taking a supremum over the values of functions applied to vectors of unit length. We get that    \sup_{\|x\|=1}\|(T_1+T_2)x\|=\sup_{\|x\|=1}\|T_1x+T_2x\| \leq \sup_{\|x\|=1}\|T_1x\|+\sup_{\|x\|=1} \|+T_2x\|.    The last inequality follows from basic properties of the supremum.  	0.9560231566429138
2100325	"<p><span class=""math-container"" id=""19422195"">\bf{Hint}</span>:</p>  <p>If you know the general Least squares approach to <span class=""math-container"" id=""19422196"">\|Ax-b\|_2^2</span>, which is \begin{equation} \hat{x}=(A^{\top}A)^{-1}A^{\top}b \end{equation} Then maybe you can rewrite your problem just as general single objective Least Squares problem, remember that the matrix <span class=""math-container"" id=""19422197"">A</span> does not need to be square, but can also be skinny, i.e. <span class=""math-container"" id=""19422198"">A\in \mathbb{R}^{m\times n}</span> with <span class=""math-container"" id=""19422199"">m&gt;n</span>. </p>  <p>Do you have a clue what I am hinting at? Otherwise, just let me know.</p>  <p><strong>Edit:</strong></p>  <p>If we go back to your problem of minimizing <span class=""math-container"" id=""19422200"">\|Ax-a\|_2^2+\|Bx-b\|_2^2</span> then see that we can rewrite this as \begin{equation} \left\Vert\begin{bmatrix} A\\ B \end{bmatrix}x - \begin{bmatrix} a\\ b \end{bmatrix}\right\Vert_2^2=\|Cx-c\|_2^2 \end{equation} Now our Least Squares solution becomes(assuming <span class=""math-container"" id=""19422201"">C</span> is full rank) \begin{equation} \hat{x}=(C^{\top}C)^{-1}C^{\top}c= (A^{\top}A+B^{\top}B)^{-1}(A^{\top}a+B^{\top}b) \end{equation}</p> "	\bf{Hint} :    If you know the general Least squares approach to  \|Ax-b\|_2^2 , which is \begin{equation} \hat{x}=(A^{\top}A)^{-1}A^{\top}b \end{equation} Then maybe you can rewrite your problem just as general single objective Least Squares problem, remember that the matrix  A  does not need to be square, but can also be skinny, i.e.  A\in \mathbb{R}^{m\times n}  with  m>n .     Do you have a clue what I am hinting at? Otherwise, just let me know.    Edit:    If we go back to your problem of minimizing  \|Ax-a\|_2^2+\|Bx-b\|_2^2  then see that we can rewrite this as \begin{equation} \left\Vert\begin{bmatrix} A\\ B \end{bmatrix}x - \begin{bmatrix} a\\ b \end{bmatrix}\right\Vert_2^2=\|Cx-c\|_2^2 \end{equation} Now our Least Squares solution becomes(assuming  C  is full rank) \begin{equation} \hat{x}=(C^{\top}C)^{-1}C^{\top}c= (A^{\top}A+B^{\top}B)^{-1}(A^{\top}a+B^{\top}b) \end{equation}  	0.9559687376022339
1126484	"<p><strong>Hint:</strong> it suffices to show that the lowest eigenvalue of <span class=""math-container"" id=""10614441"">A - B</span> is positive.  Note that for a symmetric matrix <span class=""math-container"" id=""10614442"">M</span>, the lowest eigenvalue can be expressed as <span class=""math-container"" id=""10614443""> \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx </span> and that <span class=""math-container"" id=""10614444""> \|M\| = \max_{\|x\| = 1} \left|x^TMx \right| </span></p>  <hr>  <p>I've combined several ideas to get that last equality.  From the usual results regarding the <a href=""http://en.wikipedia.org/wiki/Rayleigh_quotient"" rel=""nofollow"">Rayleigh quotient</a>, we have <span class=""math-container"" id=""10614445""> \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx \\ \lambda_{max}(M) = \max_{\|x\| = 1} x^TMx </span> For arbitrary matrices, we have <span class=""math-container"" id=""10614446""> \|A\| = \max_{\|x\| = 1}\|Ax\|= \sqrt{\max_{\|x\| = 1} x^TA^TAx} </span> So, for symmetric <span class=""math-container"" id=""10614447"">M</span>, we have <span class=""math-container"" id=""10614448""> \|M\| = \sqrt{\max_{\|x\| = 1} x^TM^2x} = \sqrt{\lambda_{max}(M^2)} </span> Note, however, that the eigenvalues of <span class=""math-container"" id=""10614449"">M^2</span> are simply <span class=""math-container"" id=""10614450"">\lambda^2</span> for all eigenvalues <span class=""math-container"" id=""10614451"">\lambda</span> of <span class=""math-container"" id=""10614452"">M</span>.  Thus, we have <span class=""math-container"" id=""10614453""> \|M\| = \sqrt{\lambda_{max}(M^2)} = \max\{|\lambda|: \lambda \text{ is an eigenvalue of } M\} = \max_{\|x\| = 1} \left|x^TMx \right| </span></p> "	Hint:  it suffices to show that the lowest eigenvalue of  A - B  is positive.  Note that for a symmetric matrix  M , the lowest eigenvalue can be expressed as   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx   and that   \|M\| = \max_{\|x\| = 1} \left|x^TMx \right|        I've combined several ideas to get that last equality.  From the usual results regarding the  Rayleigh quotient , we have   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx \\ \lambda_{max}(M) = \max_{\|x\| = 1} x^TMx   For arbitrary matrices, we have   \|A\| = \max_{\|x\| = 1}\|Ax\|= \sqrt{\max_{\|x\| = 1} x^TA^TAx}   So, for symmetric  M , we have   \|M\| = \sqrt{\max_{\|x\| = 1} x^TM^2x} = \sqrt{\lambda_{max}(M^2)}   Note, however, that the eigenvalues of  M^2  are simply  \lambda^2  for all eigenvalues  \lambda  of  M .  Thus, we have   \|M\| = \sqrt{\lambda_{max}(M^2)} = \max\{|\lambda|: \lambda \text{ is an eigenvalue of } M\} = \max_{\|x\| = 1} \left|x^TMx \right|   	0.9558638334274292
1219652	"<p>First, note that <span class=""math-container"" id=""11405745""> \left\|\,f\vphantom{\hat{f}}\,\right\|_{A(\mathbb{T})}=\left\|\,\hat{f}\,\right\|_{\ell^1} </span> Suppose <span class=""math-container"" id=""11405746"">g(x)=1</span>. Then <span class=""math-container"" id=""11405747"">\hat{g}(n)=[n=0]</span>, and therefore, <span class=""math-container"" id=""11405748"">\|g\|_{A(\mathbb{T})}=1</span>. Then, your question asks if <span class=""math-container"" id=""11405749""> \left\|\,f\,\right\|_{A(\mathbb{T})} \le C\left\|\,f\,\right\|_{L^2} </span> which is impossible. For counterexample, we can look at <span class=""math-container"" id=""11405750""> \widehat{f_n}(k)=\left\{\begin{array}{cl} \frac1{|k|}&amp;\text{if }1\le|k|\le n\\ 0&amp;\text{otherwise} \end{array}\right. </span> where <span class=""math-container"" id=""11405751"">\|f_n\|_{A(\mathbb{T})}=2H_n</span>, yet <span class=""math-container"" id=""11405752"">\|f_n\|_{L^2}\le\frac{\pi^2}3</span>, where <span class=""math-container"" id=""11405753"">H_n</span> is a <a href=""http://en.wikipedia.org/wiki/Harmonic_number"" rel=""nofollow"">Harmonic number</a>.</p> "	First, note that   \left\|\,f\vphantom{\hat{f}}\,\right\|_{A(\mathbb{T})}=\left\|\,\hat{f}\,\right\|_{\ell^1}   Suppose  g(x)=1 . Then  \hat{g}(n)=[n=0] , and therefore,  \|g\|_{A(\mathbb{T})}=1 . Then, your question asks if   \left\|\,f\,\right\|_{A(\mathbb{T})} \le C\left\|\,f\,\right\|_{L^2}   which is impossible. For counterexample, we can look at   \widehat{f_n}(k)=\left\{\begin{array}{cl} \frac1{|k|}&\text{if }1\le|k|\le n\\ 0&\text{otherwise} \end{array}\right.   where  \|f_n\|_{A(\mathbb{T})}=2H_n , yet  \|f_n\|_{L^2}\le\frac{\pi^2}3 , where  H_n  is a  Harmonic number .  	0.9558252096176147
746656	"<p>For the unit circle <span class=""math-container"" id=""7405112"">U=\{x\in\mathbb{R}^2\,\colon\, |x|&lt;1\}</span>, a routine example is something like that <span class=""math-container"" id=""7405113""> f(x)= \begin{cases} \ln{\ln{\frac{1}{|x|}}},\quad |x|&lt;e^{-2},\\ \ln{2}, \quad |x|\geqslant e^{-2}. \end{cases} </span> To show that <span class=""math-container"" id=""7405114"">f\in L^2(U)</span>, take a <em>remarkable limit</em> <span class=""math-container"" id=""7405115""> \lim_{|x|\to 0}|x|^{\mu}\!\!\cdot\!\ln{\frac{1}{|x|}}=0\quad \forall\,\mu&gt;0,\tag{1} </span> and notice that <span class=""math-container"" id=""7405116"">(1)</span> yields <span class=""math-container"" id=""7405117""> \lim_{|x|\to 0}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)=0\quad \forall\,\mu&gt;0.\tag{2} </span> But <span class=""math-container"" id=""7405118"">(2)</span> implies that <span class=""math-container"" id=""7405119""> C_{\mu}\overset{\rm def}=\sup_{x\in U}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)&lt;\infty\quad\forall\, \mu&gt;0, </span> whence follows inequality <span class=""math-container"" id=""7405120"">  \int_{U}|f(x)|^2dx\leqslant C^2_{\mu}\int_{U}\frac{dx}{|x|^{2\mu}}= \frac{\pi}{1-\mu} C^2_{\mu} </span> with some <span class=""math-container"" id=""7405121"">\mu\in (0,1)</span>, while <span class=""math-container"" id=""7405122""> \int_{U}|\nabla f(x)|^2 dx=\int\limits_{|x|&lt;e^{-2}}\frac{dx}{|x|^2 \bigl(\ln{\frac{1}{|x|}}\bigr)^2}= 2\pi\!\!\int\limits_0^{e^{-2}}\frac{dr}{r(\ln{r})^2}=-\frac{2\pi}{\ln{r}}\biggr|_0^{e^{-2}}=-\frac{2\pi}{-2}+\frac{2\pi}{-\infty}=\pi. </span></p> "	For the unit circle  U=\{x\in\mathbb{R}^2\,\colon\, |x|<1\} , a routine example is something like that   f(x)= \begin{cases} \ln{\ln{\frac{1}{|x|}}},\quad |x| 0,\tag{1}   and notice that  (1)  yields   \lim_{|x|\to 0}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)=0\quad \forall\,\mu>0.\tag{2}   But  (2)  implies that   C_{\mu}\overset{\rm def}=\sup_{x\in U}|x|^{\mu}\!\!\cdot\!\ln\Bigl(\ln{\frac{1}{|x|}}\Bigr)<\infty\quad\forall\, \mu>0,   whence follows inequality    \int_{U}|f(x)|^2dx\leqslant C^2_{\mu}\int_{U}\frac{dx}{|x|^{2\mu}}= \frac{\pi}{1-\mu} C^2_{\mu}   with some  \mu\in (0,1) , while   \int_{U}|\nabla f(x)|^2 dx=\int\limits_{|x|	0.955813467502594
1073279	"<p>Note that trivially <span class=""math-container"" id=""10142455"">\|g\|_\infty \le \|g\|_\infty + \underbrace{\|g'\|_\infty}_{\ge 0}</span> You must now see if there is some constant <span class=""math-container"" id=""10142456"">C</span> such that <span class=""math-container"" id=""10142457"">\|g\|_\infty + \|g'\|_\infty \le C\|g\|_\infty</span></p>  <p>Note that on <span class=""math-container"" id=""10142458"">C[0,1]</span>, <span class=""math-container"" id=""10142459"">g'</span> need not even exist, let alone be continuous. This leads to a very simple counter-example: <span class=""math-container"" id=""10142460"">g(x) = \sqrt x</span> with <span class=""math-container"" id=""10142461"">\|g'\|_\infty = \|\frac1{2\sqrt \cdot}\|_\infty = \infty</span></p>  <hr>  <p><strong>EDITED QUESTION</strong><br> For <span class=""math-container"" id=""10142462"">C^1[0,1]</span> (i.e. <span class=""math-container"" id=""10142463"">g'</span> is continuous as well) we can see that The spline functions defined by <span class=""math-container"" id=""10142464"">g_n(x) = \cases{0 &amp; $x&lt;\frac12 - \frac1n$ \\ \ast &amp; $x\in \frac12 + [-\frac1n, \frac1n]$\\ 1 &amp; $x &gt; \frac12 + \frac1n$}</span> Where <span class=""math-container"" id=""10142465"">\ast</span> is the <span class=""math-container"" id=""10142466"">C^1</span> (degree <span class=""math-container"" id=""10142467"">2</span>) spline interpolation are all <span class=""math-container"" id=""10142468"">C^1</span> and <span class=""math-container"" id=""10142469"">\|g_n\|_\infty = 1</span> but <span class=""math-container"" id=""10142470"">\|g_n'\|_\infty \ge \frac n2 \qquad \text{by MVT}</span> So there cannot be a bounding constant on <span class=""math-container"" id=""10142471"">C^1</span> either.</p> "	Note that trivially  \|g\|_\infty \le \|g\|_\infty + \underbrace{\|g'\|_\infty}_{\ge 0}  You must now see if there is some constant  C  such that  \|g\|_\infty + \|g'\|_\infty \le C\|g\|_\infty    Note that on  C[0,1] ,  g'  need not even exist, let alone be continuous. This leads to a very simple counter-example:  g(x) = \sqrt x  with  \|g'\|_\infty = \|\frac1{2\sqrt \cdot}\|_\infty = \infty       EDITED QUESTION  For  C^1[0,1]  (i.e.  g'  is continuous as well) we can see that The spline functions defined by  g_n(x) = \cases{0 & $x<\frac12 - \frac1n$ \\ \ast & $x\in \frac12 + [-\frac1n, \frac1n]$\\ 1 & $x > \frac12 + \frac1n$}  Where  \ast  is the  C^1  (degree  2 ) spline interpolation are all  C^1  and  \|g_n\|_\infty = 1  but  \|g_n'\|_\infty \ge \frac n2 \qquad \text{by MVT}  So there cannot be a bounding constant on  C^1  either.  	0.9557976722717285
181306	"<p>Another way to look at it, though no really different in essence, is to consider the operator norm on <span class=""math-container"" id=""2105336"">{\rm M}_{n}(\mathbb{C})</span> induced by the Euclidean norm on <span class=""math-container"" id=""2105337"">\mathbb{C}^{n}</span> (thought of as column vectors). Hence <span class=""math-container"" id=""2105338"">\|A \| = {\rm max}_{ v : \|v \| = 1} \|Av \|.</span> Since the unitary transformations are precisely the isometries of <span class=""math-container"" id=""2105339"">\mathbb{C}^{n},</span> we see that conjugation by a unitary matrix does not change the norm of a matrix. If <span class=""math-container"" id=""2105340"">A</span> can be diagonalized by a unitary matrix, then it is clear from this discussion that <span class=""math-container"" id=""2105341"">\| A \| = {\rm max}_{\lambda} |\lambda |,</span> as <span class=""math-container"" id=""2105342"">\lambda</span> runs over the eigenvalues of <span class=""math-container"" id=""2105343"">A</span>. Hence as soon as we find a diagonalizable matrix <span class=""math-container"" id=""2105344"">B</span> with <span class=""math-container"" id=""2105345"">\| B \| \neq {\rm max}_{\lambda} |\lambda|,</span> we know that <span class=""math-container"" id=""2105346"">B</span> is not diagonalizable by a unitary matrix. For example, the matrix <span class=""math-container"" id=""2105347"">B = \left( \begin{array}{clcr} 3 &amp; 5\\0 &amp; 2 \end{array} \right)</span> has largest eigenvalue <span class=""math-container"" id=""2105348"">3,</span> but <span class=""math-container"" id=""2105349"">\|B \| &gt; 5</span>  because <span class=""math-container"" id=""2105350"">B \left( \begin{array}{cc} 0 \\1 \end{array} \right) = \left( \begin{array}{cc} 5 \\2 \end{array} \right).</span> Also, <span class=""math-container"" id=""2105351"">B</span> is diagonalizable, but we now know that can't be achieved via a unitary matrix.</p> "	Another way to look at it, though no really different in essence, is to consider the operator norm on  {\rm M}_{n}(\mathbb{C})  induced by the Euclidean norm on  \mathbb{C}^{n}  (thought of as column vectors). Hence  \|A \| = {\rm max}_{ v : \|v \| = 1} \|Av \|.  Since the unitary transformations are precisely the isometries of  \mathbb{C}^{n},  we see that conjugation by a unitary matrix does not change the norm of a matrix. If  A  can be diagonalized by a unitary matrix, then it is clear from this discussion that  \| A \| = {\rm max}_{\lambda} |\lambda |,  as  \lambda  runs over the eigenvalues of  A . Hence as soon as we find a diagonalizable matrix  B  with  \| B \| \neq {\rm max}_{\lambda} |\lambda|,  we know that  B  is not diagonalizable by a unitary matrix. For example, the matrix  B = \left( \begin{array}{clcr} 3 & 5\\0 & 2 \end{array} \right)  has largest eigenvalue  3,  but  \|B \| > 5   because  B \left( \begin{array}{cc} 0 \\1 \end{array} \right) = \left( \begin{array}{cc} 5 \\2 \end{array} \right).  Also,  B  is diagonalizable, but we now know that can't be achieved via a unitary matrix.  	0.9557799696922302
2809735	"<p>Following Jalex Stark's hint, let \begin{align*} v \doteq (1/1,1/2,1/3,\dots), \end{align*} so that <span class=""math-container"" id=""25936406"">v \notin \ell^1</span> and <span class=""math-container"" id=""25936407"">v \in \ell^2</span>. Now define the bounded operator <span class=""math-container"" id=""25936408"">A : \ell^2 \to \ell^2</span> as, \begin{align*} Au \doteq \langle v,u \rangle v. \end{align*} Indeed, <span class=""math-container"" id=""25936409"">A</span> is bounded, since by Cauchy Schwartz, \begin{align*} \|A\| = \sup_{u \neq 0}\frac{\|Au\|}{\|u\|} = \sup_{u \neq 0}\frac{|\langle v,u\rangle|\|v\|}{\|u\|} \leq \sup_{u \neq 0} \|v\|^2 = \|v\|^2. \end{align*} Now take <span class=""math-container"" id=""25936410"">u = (1/1^2,1/2^2,1/3^2,\dots) \in \ell^1</span>, and we will have \begin{align*} Au = \langle v,u \rangle v = \left(\sum_{n=1}^{\infty}1/n^3 \right)v \notin \ell^1. \end{align*}</p> "	Following Jalex Stark's hint, let \begin{align*} v \doteq (1/1,1/2,1/3,\dots), \end{align*} so that  v \notin \ell^1  and  v \in \ell^2 . Now define the bounded operator  A : \ell^2 \to \ell^2  as, \begin{align*} Au \doteq \langle v,u \rangle v. \end{align*} Indeed,  A  is bounded, since by Cauchy Schwartz, \begin{align*} \|A\| = \sup_{u \neq 0}\frac{\|Au\|}{\|u\|} = \sup_{u \neq 0}\frac{|\langle v,u\rangle|\|v\|}{\|u\|} \leq \sup_{u \neq 0} \|v\|^2 = \|v\|^2. \end{align*} Now take  u = (1/1^2,1/2^2,1/3^2,\dots) \in \ell^1 , and we will have \begin{align*} Au = \langle v,u \rangle v = \left(\sum_{n=1}^{\infty}1/n^3 \right)v \notin \ell^1. \end{align*}  	0.9557787179946899
571805	"<p>Let <span class=""math-container"" id=""5879260"">\| v \|_{p}</span> denotes the <span class=""math-container"" id=""5879261"">p</span>-norm of <span class=""math-container"" id=""5879262"">v \in \Bbb{R}^{n}</span>. In particular,</p>  <p><span class=""math-container"" id=""5879263""> \| v \|_{2} = [ v_{1}^{2} + \cdots + v_{n}^{2} ]^{1/2} \quad \text{and} \quad \| v \|_{\infty} = \max \{ |v_{1}|, \cdots, |v_{n}| \}. </span></p>  <p>Then the following map</p>  <p><span class=""math-container"" id=""5879264""> F : D^{n} \to [-1, 1]^{n} : v \mapsto \frac{\|v\|_{2}}{\|v\|_{\infty}} v </span></p>  <p>gives the homeomorphism with the inverse</p>  <p><span class=""math-container"" id=""5879265""> G : [-1, 1]^{n} \to D^{n} : w \mapsto \frac{\|w\|_{\infty}}{\|w\|_{2}} w. </span></p>  <p>(Of course, we set <span class=""math-container"" id=""5879266"">F(0) = 0 = G(0)</span>.)</p>  <p>The idea is simple:</p>  <p><span class=""math-container"" id=""5879267""> D^{n} = \{ \| v \|_{2} \leq 1 \} \quad \text{and}  \quad [-1, 1]^{n} = \{ \| v\|_{\infty} \leq 1 \}. </span></p>  <p>So the map <span class=""math-container"" id=""5879268"">F</span> rescales the vector so that <span class=""math-container"" id=""5879269"">\| F(v) \|_{\infty} = \| v \|_{2}</span>. Checking that both <span class=""math-container"" id=""5879270"">F</span> and <span class=""math-container"" id=""5879271"">G</span> are indeed continuous is not theoretically hard, though it may be somewhat cumbersome.</p>  <p>The following graph may help you what is actually going on in <span class=""math-container"" id=""5879272"">n = 3</span>. The sphere (above) is mapped into the cube (below) by the mapping <span class=""math-container"" id=""5879273"">F</span>. (The seams between the faces of the cube are software artifacts.)</p>  <p><img src=""https://i.stack.imgur.com/5oS1j.png"" alt=""enter image description here""></p> "	Let  \| v \|_{p}  denotes the  p -norm of  v \in \Bbb{R}^{n} . In particular,     \| v \|_{2} = [ v_{1}^{2} + \cdots + v_{n}^{2} ]^{1/2} \quad \text{and} \quad \| v \|_{\infty} = \max \{ |v_{1}|, \cdots, |v_{n}| \}.     Then the following map     F : D^{n} \to [-1, 1]^{n} : v \mapsto \frac{\|v\|_{2}}{\|v\|_{\infty}} v     gives the homeomorphism with the inverse     G : [-1, 1]^{n} \to D^{n} : w \mapsto \frac{\|w\|_{\infty}}{\|w\|_{2}} w.     (Of course, we set  F(0) = 0 = G(0) .)    The idea is simple:     D^{n} = \{ \| v \|_{2} \leq 1 \} \quad \text{and}  \quad [-1, 1]^{n} = \{ \| v\|_{\infty} \leq 1 \}.     So the map  F  rescales the vector so that  \| F(v) \|_{\infty} = \| v \|_{2} . Checking that both  F  and  G  are indeed continuous is not theoretically hard, though it may be somewhat cumbersome.    The following graph may help you what is actually going on in  n = 3 . The sphere (above) is mapped into the cube (below) by the mapping  F . (The seams between the faces of the cube are software artifacts.)     	0.9557642936706543
