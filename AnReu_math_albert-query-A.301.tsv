pid	doc	score
268234	Not true for the spectral norm you use. Simple example:   A = \begin{bmatrix}1 & 1 \\ -1 & 1\end{bmatrix}, \quad \tilde{A} = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix},     \|A\|_2 = \sqrt{2}\approx 1.4142, \qquad  \|\tilde{A}\|_2 = \sqrt{\frac{\sqrt{5}+3}{2}}\approx 1.6180.   On the other hand, zeroing out entries of  A  to get  \tilde{A}  can give:      strict inequality for the Frobenius norm (2-norm of the vector of the singular values):  \|\tilde{A}\|_F < \|A\|_F ,   non-strict inequalities for some other norms, e.g.:  \|\tilde{A}\|_{\star} \leq \|A\|_{\star} , where  \star = 1,\infty  (usual matrix  p -norms) or  \star=M , where  \|A\|_M=\max_{i,j}|a_{ij}| .    	0.9896482229232788
729924	Let  A  be some  m \times n  real matrix. Let  \|A\|_1  be the  maximim absolute column sum norm ,  \|A \|_\infty  be the  maximim absolute row sum norm  and  \|A \|_2 = \sigma_{\max}  be the  2-norm  or  spectral norm  ( \sigma_{\max}  is the maximum singular value). We have the following fundamental inequalities:  (i) \quad \frac{1}{\sqrt{n}} \|A \|_\infty \leq \|A \|_2 \leq \sqrt{m} \|A \|_\infty   (ii) \quad \frac{1}{\sqrt{m}} \|A \|_1 \leq \|A \|_2 \leq \sqrt{n} \|A \|_1    \|A \|_2 \leq \sqrt{\|A \|_1 \|A \|_\infty} \quad (\text{some kind of Hölder's inequality}) .    So we see that if you impose additionally that also  \|A \|_\infty \leq 1  (i.e. row sum is also small) then your assertion holds.    However it's easy to construct (quadratic) matrices with small column sum but big row sum (and vice versa). And inequality  (i)  (or  (ii)  whatever you are starting with) forces the largest singular value to be big. More precisely the following matrix   \begin{pmatrix} 0.99 & 0.99 & 0.99\\ 0 & 0 & 0\\ 0 & 0 & 0\\ \end{pmatrix}   fulfilles your condition but has singular value  1.71473 .  	0.9892391562461853
590577	What you are doing is actually using Schwarz inequality to prove Schawarz inequality.    In the last step you need to use the Holder's Inequality as follows \begin{align} |\langle x,y\rangle|& = \biggl{|}\sum_\limits{n=1}^{\infty}\langle x,e_1\rangle \overline{\langle y,e_i\rangle}\biggr{|}\\ &\leq \sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|| \overline{\langle y,e_i\rangle}|\\ &\leq \sqrt{\sum_\limits{n=1}^{\infty}|\langle x,e_1\rangle|^2} \sqrt{\sum_\limits{n=1}^{\infty}|\langle y,e_1\rangle|^2} && \text{ (Holder's Inequality)}\\ &\leq \|x\|\|y\| && \text{(Bessel's Inequality)}. \end{align}    \textbf{EDIT:}  Sorry for this not helping answer. An easy way out is as follows:    Let  x\in X  and let  y\neq 0  be given. Then just take  e=\frac{y}{\|y\|} . This  \{e\}  only makes an orthonormal system. So, by Bessel's Inequality it will follow that  |\langle x,e\rangle|^2 \leq \|x\|^2 . Now just multiply both sides by  \|y\|  to get  |\langle x,y\rangle|^2\leq \|x\|\|y\| .  	0.987310528755188
1197878	This is the Holder's inequality for  q=1  and  p=\infty  i.e.     \begin{align*} \|v u\|_2 &=\sqrt{\|v^2 u^2 \|_1}\\ &\leq    \sqrt{\| v^2\|_\infty}\sqrt{ \| u^2\|_1}\\ &=\|v\|_\infty \|u\|_2 \end{align*} And for the proof someone can see Martin's answer.    In certain cases the previous version of Holder's can be, easily, derived from the more popular one, i.e. when  1	0.9865486025810242
1282943	As @Alonso Delfín mentioned, the desired conclusion is wrong. Actually this question is related to the so-called dual norm. By Hölder inequality, for  a,x\in\mathbb{R}^n , there holds   \sum |a_ix_i|\leq\|a\|_{\ell^p}\|x\|_{\ell^q},   where  p^{-1}+q^{-1}=1  for  1\leq p,q\leq\infty  and for each  p,q  and  a  there exists  x  establishing the equality. Therefore we have   \|\langle a,\cdot\rangle\|_{\ell^p}=\|a\|_{\ell^q},   and that's why it is called the dual norm. As your case, it actually should be   \|\langle a,\cdot\rangle\|_{\ell^\infty}=\|a\|_{\ell^1}.   	0.9864407777786255
1115939	If  natural norm  means a matrix norm induced by a vector norm, then it is generally not true.     For any induced matrix norm and a diagonal  D , we have   \|D\|=\max_{x\neq 0}\frac{\|Dx\|}{\|x\|}\geq\max_{1\leq i\leq n}\frac{\|De_i\|}{\|e_i\|}=\max_{1\leq i\leq n}|D_{ii}|   obtained by picking  x  to be columns of the identity matrix. So the maximum of  |D_{ii}|  is at least a lower bound on  \|D\| .    There are cases, where the inequality becomes an equality. This is true for any matrix  p -norm with  1\leq p\leq \infty .    In order to construct a counterexample, consider a vector norm   \|x\|_M:=\|Mx\|_\infty   with a nonsingular matrix  M . The induced matrix norm is related to the  \infty -norm by   \|D\|_M=\|MDM^{-1}\|_\infty.   For example, in  \mathbb{R}^2  and with   D=\begin{bmatrix}1&0\\0&2\end{bmatrix}, \quad M=\begin{bmatrix}2&1\\1&2\end{bmatrix},   we get   \|D\|_M=3> 2=\max_{1\leq i\leq2}|D_{ii}|.   	0.9861908555030823
221753	Since  \|f(x)\| = \|Ax\| \leq \|A\| \|x\|,  by the definition of the operator norm:   \|A\| = \sup_{x\in \mathbb{R}^3}\frac{\|Ax\|}{\|x\|}.   In case of Euclidean norm and  A: \mathbb{R}^3\to  \mathbb{R}^3  is a linear transformation:    \|A\|  = \sqrt{\lambda_{\mathrm{max}}(A^T A)},   where  \lambda_{\mathrm{max}}(A^T A)  is the largest eigenvalue of  A^TA , and you wanna check this is less than  1 .    In your case:    A^T A = \begin{pmatrix}5/16 & 0 & 0 \\ 0 & 1/9 & 0\\ 0 & 0 & 5/16 \end{pmatrix},   which gives the  \lambda_{\mathrm{max}} = 5/16 , and  \|A\| = \sqrt{5}/4 < 1 , thus    \|f(x)\| = \|Ax\| \leq \|A\| \|x\| < \frac{\sqrt{5}}{4}\|x\|   and  f  is a contraction.  	0.9861837029457092
609030	The answer by @MotylaNogaTomkaMazura shows why this holds if  \|\cdot\|  is submultiplicative and satisfies  \|I\|=1 . However, if it is not submultiplicative, this does not need to be true.    Consider, e.g., the  \max -norm   \|X\|:=\max_{i,j}|x_{ij}|.   Clearly,  \|\cdot\|  is a matrix norm (matrix analogue of the vector  \infty -norm). Also,  \|X\|=1 . But with   X=\pmatrix{0&1\\1&1},\quad Y=\pmatrix{1&1\\0&1},     \|XY\|=2\not\leq 1=\|X\|\|Y\|,   so  \|\cdot\|  is not submultiplicative.    With   A=\frac{1}{5}\left(\begin{array}{rr}-3&4\\-4&-3\end{array}\right),   we have  \|A\|=3/5<1 . But   \frac{1}{1+\|A\|}=\frac{5}{9}\not\leq\frac{1}{2}=\|(I-A)^{-1}\|.   	0.9859060049057007
1223990	If you read  Boyd  in chapter six there is regularization and least squares problems. Regularization follows the following problem like this.     \textrm{ minimize w.r.t }R_{+}^{2} (\| Ax -b\|,\|x \|)     this is called the bi-criterion problem which is a convex optimization problem.    Regularization has a general pattern which looks like this    \textrm{ minimize}  \| Ax -b\| +  \gamma \|x \|     Where we have a parameter   \gamma \in (0,\infty)    which is our regularization parameter. In the case of  \ell_{2}  regularization we have      \textrm{ minimize}  \| Ax -b\|_{2} +  \delta \|x \|_{2}     where our 2-norm  here  \|x \|_{2} = \left( \sum_{i=1}^{m} |x_{i} |^{2} \right)^{\frac{1}{2}}    The superscript simply means     \| x \|_{2}^{2} =  \sum_{i=1}^{m} |x_{i} |^{2}   	0.9856680035591125
727344	A possible solution steps:      Prove that    \int_0^1\frac{1}{|x-t|^{1/2}}\,dt=2\sqrt{x}+2\sqrt{1-x}\le 2\sqrt{2}.    Prove that  \|g\|_\infty\le 2\sqrt{2}\|f\|_\infty  (simple estimation by 1).   Prove that  \|g\|_1\le 2\sqrt{2}\|f\|_1  (using e.g.  Tonelli's theorem  and 1).   Conclude that  \|g\|_2\le 2\sqrt{2}\|f\|_2  by the  Riesz-Thorin theorem .    	0.9855940341949463
764213	I'm afraid your intition is leading you in a wrong direction. Consider  \mathbb R^2  equipped with the  1- norm  \left\|\begin{pmatrix}x_1\\x_2\end{pmatrix}\right\|_1=|x_1|+|x_2| .     Let  n\in \mathbb N  and  a=\begin{pmatrix}n\\0\end{pmatrix} ,  b=\begin{pmatrix}0\\n\end{pmatrix} ,  c=\begin{pmatrix}n\\n\end{pmatrix} . Then  \|a\|_1=\|b\|_1=\frac{1}{2}\|c\|_1 = n,  but  \|a-b\|_1 = 2n,  i.e. there is no upper bound.    I'm not sure for which norms you can generalize this kind of argument.  	0.9855416417121887
615535	Partial results:    We define   \DeclareMathOperator{\tr}{trace} \|A\|_1 = 1'|A|1 = \sum_{i,j}|a_{ij}|\\ \|A\|_2 = \sqrt{\tr(A'A)} = \sqrt{\sum_{i,j}|a_{ij}|^2}    These are  the entrywise  1 -norm and entrywise  2 -norm (AKA Frobenius norm) respectively.  Both are submultiplicative matrix norms.    We note that for any matrix  A ,   \|A\|_2 \leq \|A\|_1 \leq n \|A\|_2   By the information given, we note that  \|A\|_2^2 = \tr(A'A) = o(n) . In particular, this means that  \|A\|_2 = o(\sqrt{n}) .  Now, because the norms are submultiplicative, we also have   \|A^3\|_2 \leq \|A\|_2^3 = o(n^{3/2})   From there, we have   1'A^31 = \|A^3\|_1 \leq n \|A^3\|_2 = o(n^{5/2})=o(n^{2.5})   Which may be the best we can do.       The information is  1'A1 = O(n)  doesn't let us do better directly.  In particular, we have   \|A^3\|_1 \leq \|A\|_1^3 = O(n^3)   	0.9854814410209656
698211	Hint:    \frac{1}{\|A^{-1}\|\|A\|}=\left|\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\right|\leq 1 \ [\text{applicable only for the normal matrices}]      \frac{1}{\|A^{-1}\|\|A\|}=\frac{\sigma_{min}(A)}{\sigma_{max}(A)}\leq 1 \ [\text{true only when, } \|\cdot\| \text{ denotes } \|\cdot\|_2]  For any matrix  A, \ \kappa(A) \geq 1 .    [Please check the comments below for another approach to solve this problem.]  	0.9854452013969421
304661	Note that  \rho(A)\le\|A\|  for every matrix norm, and  \|A\|_b:=\|S^{-1}AS\|_a  is a matrix norm whenever  \|\cdot\|_a  is a matrix norm and  S  is nonsignular. By combining known matrix norms with similarity transform, one can always obtain a sufficient condition for  \rho(A)<1  in the form of  \|S^{-1}AS\|<1 .    For instance, suppose  A  satisfies the condition  \color{red}{\|\pmatrix{P&Q}\|_\infty<1} , where  \|X\|_\infty=\max_i\sum_j|x_{ij}|  is the maximum row sum norm. Then we get   \|A\|:=\left\|S^{-1}AS\right\|_\infty<1\ \text{ with }\ S=\pmatrix{I\\ &(1+\epsilon)I}.     Alternatively, if  \color{red}{Q \text{ is positive definite and } \operatorname{trace}(P^2)+2\operatorname{trace}(Q)<1} , then  A  is similar to  B=\pmatrix{P&Q^{1/2}\\ Q^{1/2}&0}  and  \|B\|_F^2<1 . Hence  \rho(A)=\rho(B)\le\|B\|_F<1 .  	0.9854231476783752
1247670	"Though not an answer to the question, I would like to mention that the inequality  \|B\|_F \leq \|B\|_{\mathrm{op}}  is not correct, it should be  \|B\|_{\mathrm{op}}\leq\|B\|_{F} . A quick way to see this, is to use the equations   \|B\|^2_{\mathrm{op}}  = \lambda_{\max}(B^TB)= \sigma^2_{\max}(B)   and   \|B\|_F^2 = \operatorname{trace}(B^T B) = \sum_{i=1}^{\min\{m,n\}} \sigma^2_i (B),  where  \lambda_i  and  \sigma_i  stand for an eigenvalue and singular value respectively.    Reference:  ""Matrix norm"" on Wikipedia .  "	0.9853920340538025
446096	Any induced norm satisfies  \|A\|\ge \rho(A) , where  \rho(A)  is the spectral radius of  A ,  i.e.  the largest absolute value of its eigenvalues. This fact is stated without proof  in Wikipedia ; here is a proof.        Any induced norm is of the form    \|A\|=\max_x\frac{\|Ax\|}{\|x\|}.  Let  x^*  be an eigenvector corresponding to the eigenvalue  \lambda^*  with the largest absolute value. Then  \|A\|\ge\frac{\|Ax^*\|}{\|x^*\|}=\frac{\|\lambda^*x^*\|}{\|x^*\|}=|\lambda^*|\frac{\|x^*\|}{\|x^*\|}=|\lambda^*|.      Let  A=\begin{bmatrix}1&0\\0&0\end{bmatrix} . Then your norm is  \|A\|=\dfrac1{\sqrt2}\|A\|_F=\dfrac1{\sqrt2}  while  \rho(A)=1 , contradicting the above inequality.  	0.9852996468544006
1068908	It is valid but as noted the Gershgorin theorem is quite an overkill. A more elementary approach follows.    Let  A=D-B , where  D  is the diagonal part of  A . Let  A  be row diagonally dominant, that is,  \tag{1} |a_{ii}|>\sum_{j\neq i}|a_{ij}|\;\iff\; 1>\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|, \quad i=1,\ldots,n.   The latter inequality is equivalent to  \tag{2}1>\|D^{-1}B\|_\infty.    If  A  was singular,  Ax=0  and hence  x=D^{-1}Bx  for some nonzero  x . We would have   \|x\|_\infty=\|D^{-1}Bx\|_\infty\leq\|D^{-1}B\|_\infty\|x\|_\infty   and, dividing by  \|x\|_\infty ,   1\leq\|D^{-1}B\|_\infty,   which contradicts (2).  	0.9852674603462219
1060956	I'm posting this as an answer, since it doesn't fit in a comment, and seems to be pretty darn close to what is required.    I get the following bound: \begin{align} \|f-g\|_p^p  &= \| |f-g|^p \|_1 \\  &\le \| |f-g| |f-g|^{p-1} \| \\  &= \| f-g \|_2 \| |f-g|^{p-1} \|_2 \tag{by Hölder} \\  &= \|f-g\|_2 \left( \int |f-g|^{2p-2} \right)^{\frac{1}{2}} \\  &\le \|f-g\|_2 \left( \int (|f|-|g|)^{2p-2} \right)^{\frac{1}{2}} \tag{triangle inequality} \\  &\le \|f-g\|_2 \left( \int (2C)^{2p-2} \right)^{\frac{1}{2}} \\  &= (2C)^{p-1} \sqrt{\mu(X)} \|f-g\|_2 \end{align} This differs from the desired bound in three ways:  (1) the exponent on  C  is  p-1 , not  p-2  (this could be a clerical error on my part), (2) the extra 2 in the constant, and (3) it appears to me that the inequality also depends on the measure of the space.  	0.9851417541503906
105367	This got too long for a comment:    Using the definition of the  vecto  p  norm        \|\mathbf{x}\|_p := \bigg( \sum_{i=1}^n |x_i|^p \bigg)^{1/p},   you can combine 3.) and 4.) like the following:    Let    \left \| A \right \| _p = \max \limits _{x \ne 0} \frac{\left \| A x\right \| _p}{\left \| x\right \| _p}.    So you get back 3.) with  p=2 . In the case of  p=1  and  p=\infty , the norms can be computed as:       \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} |,    which is simply the maximum absolute column sum of the matrix.    \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} |,    which is simply the maximum absolute row sum of the matrix      Here I'm not sure, which of both you mean, but none of them is a vector as already pointed out in the comments (taken from  Matrix norm/Induced_norm , which also provides some examples, that might help with the interpretation).  	0.9848872423171997
882704	You can also prove it by relating spectral radius with matrix norm. For a linear operator  A ,  \operatorname{spr}(A) \le \|A\|  where  \begin{align} \|A\| &= \underset{\|x\|=1}{\sup} \|Ax\| \\ \operatorname{spr}(A) &= \max\left\lbrace |\lambda|: \lambda \text{ is an eigenvalue of } A \right\rbrace \end{align}   The spectral radius  \operatorname{spr}(A)  and  \|A\|  are equal iff the operator  A  is  normal .    So  \|A\| < 1 \Rightarrow \operatorname{spr}(A) < 1 \Rightarrow   all of the eigenvalues of  A  have absolute value smaller than 1.    See Exercise I.2.6 of  Rajendra Bhatia's Matrix Analysis  where he calls it the  Neumann Series .   	0.9848735332489014
577698	Do you remember how we can prove Cauchy-Schwarz through interpolation?     Since  |xy|\leq\frac{x^2+y^2}{2}  we have:   \| f\cdot g\|_1 \leq \frac{\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2} \tag{1}   but the LHS is just the same if we replace  f  with  \lambda f  and  g  with  \frac{1}{\lambda}g , so:   \| f\cdot g\|_1 \leq \frac{\lambda^2\|f\|_2^2}{2}+\frac{\|g\|_2^2}{2\lambda^2} \tag{2}   and by choosing  \lambda  in such a way the two terms in the RHS of  (2)  are equal, i.e.  \lambda=\sqrt{\frac{\|g\|_2}{\|f\|_2}} , we get:   \| f\cdot g\|_1 \leq \|f\|_2 \cdot \|g\|_2 \tag{3}  that is the usual Cauchy-Schwarz inequality.  If we start with the Young inequality:   |xy|\leq \frac{|x|^p}{p}+\frac{|y|^q}{q}\tag{4}   and follow exactly the same interpolation steps, we end with:   \| f\cdot g\|_1 \leq \|f\|_p\cdot \|g\|_q \tag{5}   that is the wanted Holder's inequality.  	0.9847512245178223
575617	"I have question about the same proof. In the book it mentioned     ""Set  σ_{1}=\|A\|_{2} .By a compactness argument, there must be vectors  v_{1} \in C^n  and  u_{1} \in C^{m}  with  \|v_{1}\|_{2}=\|u_{1}\|_{2}=1  and  Av_{1}=σ_{1} .""  Since it was defined earlier that  \|A\|_{2}=sup_{\|v\|_{2}=1} \|Av\|_{2}  and   σ_{1}\geqslantσ_{2}\geqslant…  , as semi-axis principle, I can follow why  σ_{1}=\|A\|_{2}  but how it came to the conclusion that there is  u_{1}\in C^{m}  with  \|u_{1}\|_{2}=1  and  Av_{1}=σ_{1}u_1 .  "	0.984725832939148
1127545	Indeed! Let  S  be the matrix for which  A=SJS^{-1},  where  J  is the Jordan canonical form of  A.  Let  E=\mathrm{diag}(1,\varepsilon,\varepsilon^{2},\ldots,\varepsilon^{n-1}).  Then  A=SEJ'E^{-1}S^{-1},  where  J'  is like the Jordan canonical form of  A , but wherever  J  had a one on its first superdiagonal,  J'  has a  \varepsilon.  Let  V=SE.  Then if we let  \|x\|:=\|V^{-1}x\|_{\infty},  we see that \begin{align*} \|A\|&=\sup_{\|x\|=1}\|Ax\|\\ &=\sup_{\|V^{-1}x\|_{\infty}=1}\|V^{-1}Ax\|_{\infty}\\ &=\sup_{\|y\|_{\infty}=1}\|V^{-1}AVy\|_{\infty}\\ &=\sup_{\|y\|_{\infty}=1}\|J'y\|_{\infty}=\rho(A)+\varepsilon. \end{align*}    I leave it to you to show that  \|V^{-1}\cdot\|_{\infty}  defines a valid vector norm on  \mathbb{C}^{n} , and the last equality.  	0.9846723079681396
572722	"This is related with  the Frobenius norm  and it is defined as  \|A\|_F =\sqrt{\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|^2}\;.  It can be proved that this norm is not induced by any vector norm, hence there does  not exist  a vector norm such that the following holds:    \|T\|_F = \sup_{x \ne 0} \frac{\|Tx\|}{\|x\|}.      It's really easy to prove. An operator (matrix) norm, in order to be a norm induced by a vector norm, has to be  1  for the identity operator:  \|I\| = \sup_{x \ne 0} \frac{\|Ix\|}{\|x\|} = \sup_{x \ne 0} \frac{\|x\|}{\|x\|} = 1.    The norm of the identity operator (matrix) is not unitary in Frobenius norm:  \|I_n\|_F = \sqrt{n}.    Since  1 \ne \sqrt{n} , this norm is not induced by any vector norm with the ""sup"" formula.  "	0.9846426844596863
1050732	For the identity map  I      \|I\|_{op}=\sup \{|L(x)| : |x|=1 \}=\sup \{|x| : |x|=1 \}=1.     \|I\|_{tr} = \sum_{i=1}^{n}\sum_{j=1}^{n}(a_{i,j})^2= \sum_{i=1}^{n}\sum_{j=1}^{n}(\delta_{i,j})^2=n    (I recall that  \delta_{i,j}=1  if  i=j  and  0 , otherwise).    I guess that there should be taken a root from  \|L\|_{tr}  in order to make it norm, that is to assure that  \|\lambda L\|_{tr}=|\lambda|\|L\|_{tr}  for each real  \lambda , because in the present state we have  \|\lambda L\|_{tr}= |\lambda|^2\|L\|_{tr} . In particular,  \|\lambda I\|_{op}=|\lambda| , whereas  \|\lambda I\|_{tr}=|\lambda|^2 , so  0<|\lambda|<1  the left part inequality fails too. But for corrected norm  \|L\|_{tr}=\sqrt{\sum_{i=1}^{n}\sum_{j=1}^{m}(a_{i,j})^2}  the inequality   \|L\|_{op} \leq \|L\|_{tr}   indeed follows from Cauchy-Schwartz inequality.  	0.984616756439209
366865	Note first that your definition of the operator norm is not correct. It should say that  \|T\|_{op}  is the least  c  such that  \|T\vec v\|\leq c\,\|\vec v\|  for all  \vec v .    Note also that the operator norm depends on which norms you give to  \mathbb R^n  and  \mathbb R^m . The canonical choice is to take the Euclidean norm in both, but that choice is by no means the only one.     As for those  A  such that  \|A\|_{op}\leq1 , you cannot expect any algebraic property to characterize them. Indeed, given any matrix  B , the matrix  A=B/\|B\|_{op}  satisfies  \|A\|_{op}=1 .     Of course there are some necessary conditions. If  \|A\|_{op}\leq1 , then for all  k,j   |A_{kj}|=|\langle Ae_j,e_k\rangle|\leq\|A\|_{op}\,\|e_j\|\,\|e_k\|\leq1.  More than that, if  e  is the vector all entries equal to 1, then   1\geq\|A\|_{op}\geq\,\frac1n\,|\langle Ae,e\rangle|=\frac1n\,\left|\sum_{k=1}^m\sum_{j=1}^nA_{kj}\right|.   Or, for each  j ,   1\geq\|A\|_{op}^2\geq\|Ae_j\|^2=\sum_{k=1}^m|A_{kj}|^2   	0.9845547676086426
571530	The operator norm is defined (equivalently) as any of :   \|A\| = \max_{\|x\|=1}\|Ax\|   \|A\| = \max_{\|x\|\leq1}\|Ax\|   \|A\| = \max_{\|x\|\neq0}\frac{\|Ax\|}{\|x\|}    Above, you test against  k = (0,1,1)  (I'm not sure what you mean by  e_1k_2+e_2k_3)  - you're multiplying vectors here, which makes little sense in context, and none at all without specifying if you are using the cross or dot product), which has (Euclidian) norm  \sqrt{2} , so if you wish to use it to test your operator norm, you need to use the third formula above, and  \frac{\|Ak\|_2}{\|k\|_2} = \frac{\|(1,1,0)\|_2}{\|(0,1,1)\|_2} = \frac{\sqrt{2}}{\sqrt{2}} = 1 , so you have a witness that  \|A\| \geq 1 . In order to prove that  \|A\| = 1 , you now need to show that  \|Ax\|_2 \leq \|x\|_2  for all  x \in\mathbb{R}^3  (or possibly faster, using the first formula,  \|Ax\|_2 \leq 1  for all  x \in \mathbb{R}^3  with  \|x\|_2 = 1 .    (NB: Throughout (except in the definition of the operator norm, which applies on any normed linear space) I've made the use of the Euclidian norm explicit by denoting it  \|\cdot\|_2  - this is not actually important).  	0.9844218492507935
310217	In  this answer , it is shown that in  \mathbb{R}^n  and using the same Fourier Transform that saz mentions,   \|\xi\hat{f}\|_2\|xf\|_2\ge\frac{n}{4\pi}\|\hat{f}\|_2\|f\|_2\tag{1}   Therefore,   \begin{align} \frac12\|xf(x)\|_2\|\xi\hat{f}(\xi)\|_2 &\le\|xf(x)\|_{L^2(I_1)}\|\xi\hat{f}(\xi)\|_{L^2(I_2)}\tag{2}\\ &\le\frac{|I_1||I_2|}{4}\|f(x)\|_{L^2(I_1)}\|\hat{f}(\xi)\|_{L^2(I_2)}\tag{3}\\ &\le\pi|I_1||I_2|\|xf(x)\|_2\|\xi\hat{f}(\xi)\|_2\tag{4} \end{align}    (2) : Given   (3) :  |x|\le\frac{|I_1|}2  and  |\xi|\le\frac{|I_2|}2   (4) :  (1)  in  \mathbb{R}^1    Thus, we get that  |I_1||I_2|\ge\frac1{2\pi}  	0.9843502044677734
13090	For  p \geq 1  the generalized mean defines a norm, because it is the  \ell^{p} -norm only up to a factor  \sqrt[p]{n} .    However, if  p \lt 1 , the generalized mean (and also the  \ell^{p} -expression) don't define a norm because the set  \|x\|_{p} \leq 1  is not convex: if  x_{i} \geq 0  and  y_{i} \geq 0  for all  i  then  \|x + y\|_{p} \geq \|x\|_{p} + \|y\|_{p} !    If  p \leq q  then  \|x\|_{q} \leq \|x\|_{p} . To see this, note that both sides of the inequality are invariant by multiplication with a positive real number, so we may take without loss of generality an  x  with  \|x\|_{p} = 1 . Then  \|x\|_{q}^{q} = \sum_{j = 1}^{n} |x_{j}|^{q} \leq \sum_{j = 1}^{n} |x_{j}|^{p} = 1  this is because for  t \leq 1  and  p \leq q  we have  t^{q} \leq  t^{p} .    I don't understand what you ask about the sequence space.  	0.984289288520813
696673	It is not true in general.    Let  S = \begin{bmatrix} 1 & 100 \\ 1 & 0 \end{bmatrix}  and define  \|x\|_* = \|Sx\|_2 . Since  S  is invertible it is easy to check that  \|\cdot \|  is a norm.    Let  A=\begin{bmatrix} 1 & 100 \\ 0 & 1 \end{bmatrix} , then we have \begin{eqnarray} \|A\|_* &=& \sup_{\|x\|_* \le 1} \|Ax\|_* \\ &=& \sup_{\|x\|_* \le 1} \|SAx\|_2 \\ &=& \sup_{\|Sx\|_2 \le 1} \|SAx\|_2 \\ &=& \sup_{\|x\|_2 \le 1} \|SAS^{-1}x\|_2 \\ &=& \|SAS^{-1}\|_2 \end{eqnarray} Since  \| SAS^{-1}\|_2 = \| \begin{bmatrix} 2 & -1 \\ 1 & 0 \end{bmatrix} \|_2 = \sqrt{2} , we see that  |[A]_{12}| = 100 > \sqrt{2} = \|A\|_* .    Addendum : If the vector norm is the Euclidean norm, then we have  \|A\|_2 = \sup_{\|u\|_2 \le 1, \|v\|_2 \le 1 } | u^T A v | , hence choosing  u=e_i, v=e_j  gives the required result.  	0.9842754006385803
480640	I guess it holds for  \|X\|_{1}\leq \frac{1}{\|A\|_{1}} . Let  A  be a  m\times n  matrix. Then  A^{T}:\mathbb{R}^{m}\to\mathbb{R}^{n}  is a linear map. Now if we consider the  \ell_{\infty}  norm on both  \mathbb{R}^{m}  and  \mathbb{R}^{n} , then  \|A\|_{1}=\|A^{T}\|_{op} , where  \|\cdot\|_{op}  stands for the  operator norm . Therefore \begin{eqnarray} \|XA\|_{1}=\|A^{T}X^{T}\|_{op}\leq\|X^{T}\|_{op}\|A^{T}\|_{op}=\|X\|_{1}\|A\|_{1}. \end{eqnarray}  If  \|X\|_{1}\leq \frac{1}{\|A\|_{1}} , then we have  \|XA\|_{1}\leq 1 .  	0.9841947555541992
269094	Notice that from  Hölder's inequality , valid for  p \geq 1 ,     \|f\|_1 = \|f \cdot 1\|_1 \leq \|f\|_p \|1\|_q = \|f\|_p,   where  \frac{1}{p} + \frac{1}{q} = 1 , since  \|1\|_q = 1 .    But this implies that the identity     \begin{array}{rrl}     \mathrm{id}: &(C([0,1], \|\cdot\|_p) &\rightarrow &(C([0,1], \|\cdot\|_1)     \\                  &x &\mapsto &x   \end{array}   is continuous. And since  A = \mathrm{id}^{-1}(B(0,1)) , and  B(0,1)  is open in the  1 -norm, it follows that  A  is open.       Edit:  added observation from Pedro Tamaroff that Hölder is valid for  p \geq 1 .  	0.9840380549430847
850252	"Here is a proof for the Euclidean norm. This approach is buried in Schäffer. J., "" Norms and determinants of linear mappings "", Technical report, CMU, Department of Mathematical Sciences, 1970. He does not use the SVD, but the idea is essentially the same.    Let  A=U \Sigma V^*  be a singular value decomposition of  A , with  \Sigma=\operatorname{diag} (\sigma_1,...,\sigma_n) , and  \sigma_1\ge ... \ge\sigma_n .    Then  \|A\| = \sigma_1, \|A^{-1}\| = {1 \over \sigma_n} , and  |\det A| = \sigma_1 \cdots \sigma_n .    Hence  |\det A| \le \sigma_1 \cdots \sigma_{n-1} {1 \over \|A^{-1} \|} \le \|A\|^{n-1} {1 \over \|A^{-1} \|} , from which we obtain  |\det A| \|A^{-1} \| \le \|A\|^{n-1} .    The result is not true for general operator norms, for example, with  A=\begin{bmatrix} {1 \over 2} & 1 \\ 0 & 2 \end{bmatrix} , we have  \det A = 1 ,  \|A^{-1} \|_\infty = 3, \|A\|_\infty  = 2  and so  3=\|A^{-1} \|_\infty \not< \|A\|_\infty^{2-1} = 2 .    As an aside, it is worth noting the related  Hadamard's inequality ,  |\det A| \le \|Ae_1\| \cdots \|A e_n\|  (Euclidean norm).  "	0.9839881062507629
1271971	With some small modifications, your proof can be converted into a working proof.    Instead of using  |g(x)|<\frac M2 , we use  |g(x)|<\frac Ma , where  a>1  is arbitrary. Then, the rest of the proof can be repeated as you did.    We have to calculate  \|f\|_p :   \begin{aligned} \|f\|_p^p & = \int_{X\setminus N} \frac{M^p}{|g(x)|^p \mu(X\setminus N)} \mathrm d\mu  \\ & \leq   \int_{X\setminus N} \frac{M^p}{(M/a)^p \mu(X\setminus N)} \mathrm d\mu  \\ & \leq   \mu(X\setminus N) \frac{M^p}{(M/a)^p \mu(X\setminus N)}  \\ & = a^p \end{aligned}     So  \|f\|_p\leq a .    Then we have   \frac{\|M_g(f)\|_p}{\|f\|_p}  = \frac{M}{\|f\|_p}  \geq \frac{M}{a}.     Because  a  can be arbitrarily close to  1 , this completes the proof.    Note that because of the  \sup  in the definition of the operator norm, you do not need to show that   \frac{\|M_g(f)\|_p}{\|f\|_p} = M , only that you can find functions  f  such that   \frac{\|M_g(f)\|_p}{\|f\|_p}   is arbitrarily close to  M .  	0.9838987588882446
847777	This is more or less true for square unitary matrices, but if  V  is not square, then this isn't true.      You haven't specified what matrix norm you want to use.  As it happens, both the Frobenius norm ( \| A \|_{F} ), and the spectral or 2-norm ( \| A \|_{2} ) are invariant under unitary transformations.  For these norms, if  V  is unitary, then  \| A \|=\| V^{H}A \|  and  \| A \|= \| AV \| .    To see this for the 2-norm, recall that     \| A \|=\sqrt{\lambda_{\max}(A^{H}A)}    Thus     \| V^{H}A \|_{2}=\sqrt{\lambda_{\max}(A^{H}VV^{H}A)}=\sqrt{\lambda_{\max}(A^{H}A)}=\| A \|_{2}  .    For the second part, recall that eigenvalues are invariant under unitary similarity transformations.  Thus     \| AV \|_{2}=\sqrt{\lambda_{\max}(V^{H}(A^{H}A) V)}=\sqrt{\lambda_{\max}(A^{H}A)}=\| A \|_{2} .    	0.9838905930519104
199079	As many others have pointed out, the matrix norm of  I  is not necessarily equal to  1 . In fact, if  \|\cdot\|  is a matrix norm  c\|\cdot\|  is also a matrix norm for any  c\ge1 .    Yet, if your matrix norm is submultiplicative, we have  $0<\|I\|=\|I^2\|\le\|I\|^2$  and hence  $\|I\|\ge1$ , but strict inequality may still occur. E.g. the Frobenius norm  $\|\cdot\|_F$  is submultiplicative, but  $\|I_n\|_F=\sqrt{n}>1$  when  $n>1$ .    If your textbook is not erred, it is perhaps talking about an  induced  matrix norm, i.e. one defined by  $\|A\|=\sup_{x\neq0}\frac{\color{red}{\|}Ax\color{red}{\|}}{\color{red}{\|}x\color{red}{\|}}$  for some vector norm  $\color{red}{\|\cdot\|}$  defined on  K^n . In this case, it is straight from the definition that  \|I\|=1 .  	0.98375004529953
439850	Define  f:[0,2\pi]\to\mathbb{C}  as:  f(\theta)=\sum_{n=1}^{+\infty}x_n e^{ni\theta}.  Then  f  is a  L_2 -function with zero mean over  (0,2\pi) , and we have:  \|f\|_1 \leq \sqrt{2\pi}\,\|f\|_2 \tag{1}  due to the Cauchy-Schwarz inequality and   \|f\|_2 \leq \|f'\|_2\tag{2}  due to  Wirtinger's inequality , or just Parseval's identity.  (1)  and  (2)  give:  \frac{1}{2\pi}\int_{0}^{2\pi}\left|\sum_{n=1}^{+\infty}x_n e^{ni\theta}\right|\,d\theta\leq\frac{1}{\sqrt{2\pi}}.\tag{3}  Probably this can be improved a little, since equality cannot hold both in  (1)  and in  (2) .  	0.9837123155593872
376776	From properties of the determinant, for square matrices  A  and  B  of equal size we have   |AB|=|A||B|,   which means determinants are distributive. This means that the determinant of a matrix inverse can be found as follows:   \begin{align} |I|&=\left|AA^{-1}\right|\\ 1&=|A|\left|A^{-1}\right|\\ \left|A^{-1}\right|&=\frac{1}{|A|}, \end{align}   where  I  is the identity matrix.    \\       \Large\color{blue}{\text{# }\mathbb{Q.E.D.}\text{ #}}  	0.9836406707763672
945202	Consider  L^1[0, 1]  and  f(x) = 1+x , then we see that \begin{align} \tilde f = \|f\|_1=\int^1_0 1+x\ dx = \frac{3}{2} \end{align} which means \begin{align} \|f-\tilde f\|_1 = \int^1_0 \left|x-\frac{1}{2}\right|\ dx = \frac{1}{4}. \end{align} Hence the stated inequality doesn't hold.     However, a better question would be: Does there exists  C>0  depending only on  \Omega=[0, 1]  such that \begin{align} \|f\|_1 \leq C\|f-\tilde f\|_1? \end{align}  	0.9835517406463623
1077694	"I wouldn't call  your norm ""the"" operator norm: the operator norm given by the 2-norm is way more natural than the one you are using.     For your norm  \|A\|_{\rm op}=\sup\left\{\|Ax\|_\infty:\ \|x\|_\infty=1\right\},  it is very easy to see that    \|A\|_{\rm op}=\max\left\{\sum_{j=1}^n|A_{kj}|:\ k=1,\ldots,n\right\}.   So  \|A\|_{\rm op}  is the maximum of the 1-norms of the rows of  A .        For what is very commonly named as ""the"" operator norm, that is    \|A\|=\sup\left\{\|Ax\|_2:\ \|x\|_2=1\right\},   one can show (not that easily) that    \|A\|=\max\{\lambda^{1/2}:\ \lambda\in\sigma(A^*A)\},   where  \sigma(A^*A)  denotes the spectrum of  A^*A , i.e., its list of eigenvalues.   "	0.9834703207015991
298231	Let  \| v \|_{p}  denotes the  p -norm of  v \in \Bbb{R}^{n} . In particular,     \| v \|_{2} = [ v_{1}^{2} + \cdots + v_{n}^{2} ]^{1/2} \quad \text{and} \quad \| v \|_{\infty} = \max \{ |v_{1}|, \cdots, |v_{n}| \}.     Then the following map     F : D^{n} \to [-1, 1]^{n} : v \mapsto \frac{\|v\|_{2}}{\|v\|_{\infty}} v     gives the homeomorphism with the inverse     G : [-1, 1]^{n} \to D^{n} : w \mapsto \frac{\|w\|_{\infty}}{\|w\|_{2}} w.     (Of course, we set  F(0) = 0 = G(0) .)    The idea is simple:     D^{n} = \{ \| v \|_{2} \leq 1 \} \quad \text{and}  \quad [-1, 1]^{n} = \{ \| v\|_{\infty} \leq 1 \}.     So the map  F  rescales the vector so that  \| F(v) \|_{\infty} = \| v \|_{2} . Checking that both  F  and  G  are indeed continuous is not theoretically hard, though it may be somewhat cumbersome.    The following graph may help you what is actually going on in  n = 3 . The sphere (above) is mapped into the cube (below) by the mapping  F . (The seams between the faces of the cube are software artifacts.)     	0.9834470748901367
834405	Yes; apply Holder's inequality (you can also use Jensen if you like): \begin{align*} \|f\|_{L^p}=\|f\cdot1\|_{L^p}\leq\|f\|_{L^q}\|1\|_{1/(1/p-1/q)}=\|f\|_{L^q}\mu(X)^{1/(1/p-1/q)}\leq\|f\|_{L^q} \end{align*} since we assumed  \mu(X)<1 , and  1/(1/p-1/q)\geq0  from  p\leq q .    Edit:  Please note I used a slightly generalized version of the Holder inequality for the case where a product is being estimated in  L^p  for  p  not necessarily  1 . This is described  here .  	0.9834405183792114
1216245	Suppose   A, B \in \mathbb{R}^{n \times n}  then we know the singular value decomposition exists for both of these. So, the following is true.     A = U \Sigma V^{T}        with   U,V, \Sigma \in \mathbb{R}^{n \times n}   then we know  \| A \|_{F} \leq   \|U \| \|\Sigma\| \|V^{T} \|    but  U, V   are orthogonal which means their norm is 1. So, we have    \|A \|_{F}  \leq \|\Sigma\|_{F}    and   \Sigma   is diagonal matrix. The norm of   \Sigma   is   \| S \|_{F} = max_{i} |\sigma_{i}|   that is it is the maximum singular value which is the top entry incidently.    So, you have some bounds. From here then      \|A+B\| \leq \| A \| +\|B\|       \sigma_{A+B} \leq \sigma_{A} + \sigma_{B}   	0.9833580851554871
658046	( \Longleftarrow ) Suppose there exists a function  r: \mathbb{R} \rightarrow \mathbb{R}  such  that  \underset{t \rightarrow +\infty}{\text{lim }}r(t) = +\infty  and  f(x) \ge r(\|x\|_2)  for sufficiently large  x . Then given any  d \in \mathbb{R}^n , the existence of  \delta > 0  such that  f(x) \ge r(\|x\|_2) \ge \|d\|_2\|x\|_2  whenever  \|x\|_2 > \delta , is guarenteed. Thus by the Cauchy-Schwarz inequality,  d^Tx - f(x) \le \|d\|_2 \|x\|_2 - f(x) \le 0  if  \|x\| > \delta . Hence \begin{eqnarray} \begin{split}  f^*(d) = \text{max }\left\{\underset{\|x\|_2 \le \delta}{\text{sup }}d^Tx - f(x), \underset{\|x\|_2 > \delta}{\text{sup }}d^Tx - f(x)\right\} &\le \text{max }\left\{\underset{\|x\|_2 \le \delta}{\text{sup }}d^Tx - f(x), 0\right\} \\ &< +\infty. \end{split} \end{eqnarray}  	0.9833507537841797
160023	We can assume  A=1  so that your equation is    -\Delta u +bu= f   with  b=A^{-1}B  and  f=A^{-1}F . Your bilinear form, which I'll call  B , then satisfies   |B(u,v)|=\left| \int_{\mathbb{R}^3} \nabla u \cdot \nabla v dx +b \int_{\mathbb{R}^3} uvdx \right| \leq \| \nabla u\| _{L^2} \|\nabla v \|_{L^2} +|b| \| u\|_{L^2}\| v\|_{L^2}\\ \leq \| u\|_{H^1}\| v\| _{H^1} +|b| \| u\|_{H^1}\| v\|_{H^1}   where the first inequality is Cauchy-Schwarz applied to each summand, and the second follows from the definition of the  H^1  norm.    For coercivity, I can only prove it when  b> 0  (when it is obvious since in this case  B(u,u)=\| \nabla u\|_{L^2}^2 +b\| u\|_{L^2}^2 ).     Edit: A variant of  f_k(x)=|x|^{-\frac{3}{2}-\frac{1}{k}}  shows that when  b\leq0  the bilinear form is not coercive.  	0.9833288192749023
1198702	Note that for any matrix, the  induced 2-norm  of that matrix is defined as  \|A\|_2:=\sup_{x\neq0}\frac{\|Ax\|_2}{\|x\|_2},  and thus by the  spectral theorem for normal matrices , there exists a basis of eigenvectors of  A  for  \mathbb{C}^{n\times n} , and thus  \|A\|_2=\rho(A) . Similarly  \|B\|_2=\rho(B) . For  AB , which is not necessarily normal, we hace  \|AB\|_2=\sigma_{\mathrm{max}}(AB) , the maximal singular value of  AB , see also  this post .    Now using this, we can see directly that  \rho(AB)\leq\sigma_{\mathrm{max}}(AB)=\|AB\|_2\leq\|A\|_2\|B\|_2=\rho(A)\rho(B),  where the second inequality holds because the induced  2 -norm  \|\cdot\|  is indeed a norm.    For the first inequality, observe that for any induced matrix  p -norm, we have  for any  v\in\mathbb{C}^n , and therefore also for  v  the eigenvector with norm  1  corresponding to  |\lambda|=\rho(A) :  \|A\|_p:=\sup_{x\neq0}\frac{\|Ax\|_p}{\|x\|_p}=\sup_{\|x\|_p=1}\|Ax\|_p\geq\|Av\|_p=\|\lambda v\|_p=\rho(A)\|v\|_p=\rho(A).  	0.9832457304000854
421322	This is not possible, because         Claim:    For a  A  being an  n\times n  matrix  A  we have    1/n\cdot \|A\|_1\le \|A\|_\infty \le n\cdot \|A\|_1      Proof:    \begin{align} \|A\|_\infty &= \max_i \sum_j |a_{ij}| \le \sum_j\sum_{i} |a_{ij}|\le \sum_j \max_{k} \sum_i|a_{ik}|=n\|A\|_1 \end{align}   This shows the first inequality. The second follows by interchanging the roles of rows and columns in this calculation.  \square    	0.9832440614700317
27213	If we can show that  A  doesn't increase the 1-norm, i.e.,  \|Ax\|_1\leq\|x\|_1  Then  \|Ax\|_1=\|\lambda x\|_1=|\lambda|\|x\|_1\leq\|x\|_1  which is  |\lambda|\leq 1 , we are done, but how to show above inequality? For convenience, let's set stochastic matrix  A=\begin{pmatrix}a_{11}& a_{12}\\a_{21}& a_{22}\end{pmatrix}  Then \begin{eqnarray*}\|Ax\|_1&=&|a_{11}x_1+a_{12}x_2|+|a_{21}x_1+a_{22}x_2|\\&\leq& a_{11}|x_1|+a_{12}|x_2|+a_{21}|x_1|+a_{22}|x_2|\\&=&|x_1|+|x_2|\\&=&\|x\|_1\end{eqnarray*} For n-dimensional matrix, it can be shown in same manner.  	0.9832308292388916
784172	No, it is not true. It can be confusing because they are usually indicated with the same symbol.    One is the Frobenius norm   \|A\|_F = \left(\sum_{i,j} |a_{ij}|^2 \right)^{1/2} = \sqrt{\mathrm{Tr}(A^\dagger A)} = \left(\sum_i \sigma_i^2\right)^{1/2},   where  \sigma_i^2  are the eigenvalues of the positive matrix  A^\dagger A .    The Frobenius norm is the Euclidean norm of the vector of the singular values  \sigma_i  of  A .    The other is the induced norm   \|A\|_2 = \mathrm{sup}_{x\neq 0} \frac{|A x|_2}{|x|_2} = \mathrm{max}_i \sigma_i,   the largest singular value of  A .     \|A\|_F = |\boldsymbol{\sigma}|_2 \leq |\boldsymbol{\sigma}|_\infty = \|A\|_2  	0.9832026958465576
62319	Any matrix norm induced by a norm on your vector space (over  \mathbb{R}  or  \mathbb{C} ) that also satisfies  \|A^{*}\|=\|A\|  will be greater than or equal to the spectral norm. Let  \lambda  denote the largest singular value of A (the square root of the largest eigenvalue of ( A^*A ) ) and  v  the corresponding eigenvector. Let  \|A\|  denote the matrix norm induced by a norm on the vector space:   \|A\|^2=\|A^{*}\|\cdot\|A\|\geq\|A^{*}A\|=\max\frac{\|A^{*}Ax\|}{\|x\|}\geq\frac{\|A^{*}Av\|}{\|v\|}=\lambda   and so  \|A\|\geq\sqrt{\lambda}    For the 2-norm you actually have equality, which you can show by singular value decomposition. We can take an orthonormal basis of eigenvectors for  A^*A  (with respect to the usual scalar product that also induces the 2-norm). Denote this basis by  v_1,\ldots,v_n  with eigenvalues  \lambda=\lambda_1,\ldots,\lambda_n . For any vector  x=\sum x_i v_i  we have   \|Ax\|_2^2=\overline{x}^TA^{*}Ax\leq\overline{x}^T\sum\lambda_i x_i v_i=\sum\lambda_i |x_i|^2\leq \lambda \|x\|_2^2   So  \|A\|_2\leq \sqrt{\lambda}  and both inequalities together show  \|A\|_2=\sqrt{\lambda} .  	0.9831899404525757
952796	HINT: we must show that if  q\neq p  then these norms are not equivalent, that is doesnt exists  K\in(0,\infty)  such that    K\|f\|_q\le\|f\|_p,\quad\forall f\in C(I,\Bbb K)    or equivalently    \frac{\|f\|_q}{\|f\|_p}\le K^{-1}    for  f\neq 0 . Choosing  f(x):=a^x ,  q=2  and  p=1  and  I:=[0,1]  we have that it must be true that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}\le K^{-1},\quad\forall a>1    but the above is equivalent to say that    \frac{\sqrt{\int_0^1 a^{2x}\mathrm dx}}{\int_0^1 a^x\mathrm dx}=\frac{\sqrt{\frac{a^2-1}{2\ln a}}}{\frac{a-1}{\ln a}}=\sqrt{\frac{(a+1)\ln a}{(a-1)2}}\le K^{-1},\quad\forall a>1    what cannot be possible, hence  \|{\cdot}\|_2  and  \|{\cdot}\|_1  are not equivalent. We can generalize this result easily for any  q>p  and any interval  [\alpha,\beta] .  	0.9830652475357056
885240	First, you are missing a square root:  \|x\|_2=\sqrt{\int_{a}^{b}\|x(t)\|^2\ dt}    Since for all  t \in [a, b], \|x(t)\| \le \|x\|_\infty = \sup\{\|x(t)\| : t\in[a,b]\} , we have that  \int_{a}^{b}\|x(t)\|^2\ dt \le \int_{a}^{b}\|x\|_\infty^2\ dt = (b-a)\|x\|_\infty^2  and so  \|x\|_2 \le (\sqrt{b-a})\|x\|_\infty    This inequality cannot be improved upon, as it is an equality when  \|x(t)\|  is constant. So the question of which is larger depends on how far away  a  and  b  are from each other. When  b - a \ge 1 , then the supremum norm is always larger. when  b - a < 1 , then either norm can be larger, depending on the map  x .    For a more general comparison, see Hölder's inequality.  	0.9830181002616882
877760	If  A  is positive definite then the eigenvalues of  A  are all positive. In particular, the smallest eigenvalue, say  \lambda_s , will be nonzero. Next observe \begin{align} \frac{1}{2}x^TAx-b^Tx=\frac{1}{2}x^TQ^TDQx-b^Tx=\geq\frac{\lambda_s}{2}\| Qx\|^2-\|b\|\|x\|=\frac{\lambda_s}{2}\|x\|^2-\|b\|\|x\|. \end{align} Thus as  \|x\|\rightarrow \infty  we see that  f  will also go to infinity.    For the converse direction, assume  A  is not positive definite then there is an eigen-direction corresponding to a non-positive eigenvalue, say  x . Then we see that if  \lambda<0  then \begin{align}  f(\alpha x)= \frac{\lambda\alpha^2}{2}\|x\|^2-\alpha b^Tx\leq \frac{\lambda\alpha^2}{2}\|x\|^2+|\alpha|\|b\|\| x\| \end{align} which goes to negative infinity as  |\alpha|\rightarrow \infty . For the case  \lambda=0 , we will leave it as an exercise for the reader.  	0.9829994440078735
71529	You could of course generalize your current measure    \begin{align}  S(X) = \frac{\frac{k^{(1/m)}}{k^{(1/n)}} -\frac{\|X\|_m}{\|X\|_n} } {\frac{k^{(1/m)}}{k^{(1/n)}}-1} \end{align}    while preserving your properties you specified.    An interesting special case could be  m = 1, n \to \infty , in which case the expression simplifies to    \begin{equation}  S(X) = \frac{k-\frac{\|X\|_1}{\|X\|_c}}{k-1} \end{equation}    where  c = \infty , (for some reason, mathjax refused to render when I inserted  \infty  directly in the fraction)  	0.9829953908920288
664197	The answer to your question is yes.   Golub and Van Loan’s book, as a book on numerical linear algebra, tends to focus on real matrices but this result holds for complex matrices too.    To see this, let  A \in \mathbb{C}^{m \times n} .  Since the matrix 2-norm is an induced matrix norm, we have that  \left\| A \right\|_2  = \mathop {\max }\limits_{\left\| x \right\|_2  = 1} \left\| {Ax} \right\|_2    or, equivalently,  \left\| A \right\|_2^2  = \mathop {\max}\limits_{\left\| x \right\|_2  = 1} \left\| {Ax} \right\|_2^2 .    Suppose  \left| {a_{r,s} } \right| = \mathop {\max}\limits_{1 \le i \le m \atop 1 \le j \le n} \left| {a_{ij} } \right|  , where we’re letting  a_{ij}  denote the element of  A  in the  i -th row and  j -th column.  Then if we let  x = e_s , we have that      \left\| A \right\|_2^2  \ge \left\| {Ae_s } \right\|_2^2  = \sum\limits_{k = 1}^m {\left| {a_{ks} } \right|^2 }  \ge \left| {a_{rs} } \right|^2    which establishes the result.    As for your other request, recall that the 2-norm of a Hermitian matrix  A  is equal to the spectral radius  \rho \left( A \right)  of  A .  So, from above, we have that     \mathop {\max}\limits_{1 \le i \le m \atop 1 \le j \le n} \left| {a_{ij} } \right| \le \left\| A \right\|_2  = \rho \left( A \right) .  	0.9829819202423096
465997	It is maybe a too big cannon to use (or prove) the continuity of eigenvalues. It is well known that the (matrix) norms are continuous. This follows from the triangle inequality:   \|A\|\leq\|B\|+\|A-B\|, \quad \|B\|\leq\|A\|+\|A-B\|\quad\Rightarrow \quad |\|A\|-\|B\||\leq\|A-B\|.   Therefore, if  A  and  B  are close then  \|A\|  and  \|B\|  are close as well.    Then use the fact that  \|A\|=\rho(A)  when  \|\cdot\|:=\|\cdot\|_2  is the spectral norm and  A  is SPD.    You can relate the componentwise closeness to the normwise closeness by realizing that   \|A\|_2=\max_{\|x\|_2=1}\|Ax\|_2\leq n\max_{1\leq i,j\leq n}|a_{ij}|=:n\|A\|_{\max}   (or by simply showing that  \|A\|_{\max}  is a norm and use the fact that norms on a finite-dimensional space are equivalent). Therefore, if  |a_{ij}-b_{ij}|<\epsilon  for all  i,j=1,\ldots,n  and some  \epsilon>0 , we have  |\|A\|_2-\|B\|_2|<\delta  with  \delta:=\epsilon/n .  	0.9829585552215576
160133	Presumably  A  is a real  n\times n  matrix with all its eigenvalues being real. The inequality is false in general (see chaohuang's answer for a counterexample), but it is true in each of the following circumstances (exercises):      A  is real symmetric (in this case,  A  is guaranteed to have only real eigenvalues),   A  is a  doubly stochastic matrix  (hint:  Perron-Frobenius theorem ),   the norm of every column of  A  does not exceed  \frac1{\sqrt{n}}  (hint: for any unit vector  x , we have  \|Ax\|\le\sum_i|x_j|\|a_{\ast j}\|\le\|x\|\ \left\|\left(\|a_{\ast 1}\|,\ldots,\|a_{\ast n}\|\right)\right\|  by Cauchy-Schwarz inequalty),   the norm of every row of  A  does not exceed  \frac1{\sqrt{n}} ,   \|A\|_1\|A\|_\infty\le1 , where  \|A\|_1  and  \|A\|_\infty  are respectively the maximum absolute column sum norm and the maximum absolute row sum norm of  A  (hint:  \rho(A^TA)\le\|A^TA\|_1\le\|A^T\|_1\|A\|_1 ).    	0.9829268455505371
665279	No, why should they be equal? It is easy to generate a counterexample. E.g.   A=\pmatrix{1&0},\ B=\frac1{\sqrt{2}}\pmatrix{1\\ 1},\ AB=\frac1{\sqrt{2}}.   We have  \|AB\|_1=\frac1{\sqrt{2}}\ne1=\|A\|_1 .  	0.9829211235046387
1168432	About Hölder and your upper bound, OK:   \|Tx\|_1 = \sum_{k=1}^\infty\left|\frac{x_k}{2^k}\right|\le \left(\sum_{k=1}^\infty 2^{-2k}\right)^{1/2} \left(\sum_{k=1}^\infty x_k^2\right)^{1/2} = \frac1{\sqrt 3}\|x\|_2.   About finding  x  with  \|Tx\|_1\approx\frac1{\sqrt 3}\|x\|_2  (equality  can  be impossible), see the conditions for equality in the Hölder inequality.  	0.9829199314117432
612600	Hint:  note the following:      \textrm{Cond}_2(A) = \|A\| \|A^{-1}\|   Because  A  is positive semidefinite,  \|A\| = \max_{\|x\| = 1} x^TAx   Because  A  is positive semidefinite,  \|A^{-1}\| = \max_{\|x\| = 1} \frac{1}{x^TAx}      For an lower bound of each of these maxima, plug in the  j th standard basis vector for  x .    Let  L_j  denote the  j th row of  L .  We have   e_j^T(LDL^T)e_j = d_{jj}\cdot \|L_j\|^2 \geq d_{jj}   	0.9829195737838745
827463	The standard  Holder's inequality  (for points in  \mathbb{R}^n ) can be written as     \|fg\|_p \leq \|f\|_q \|g\|_r     where  p^{-1} = q^{-1} + r^{-1} ; note that this requires  q, r \geq p .     The inequality you wrote above follows by taking  r^{-1} = p^{-1} - q^{-1}  and  g = \mathbf{1} .        The version above can be proved by taking  \tilde{f} = |f|^p  and  \tilde{g} = |g|^p  and applying the standard version quoted in Wikipedia.        In arbitrary measure spaces, what you have is the estimate     \|x\|_p \leq |\mathrm{supp}(x)|^{\frac1p -\frac1q} \|x\|_q     for every  q \geq p , provided the support of the function  x  has finite measure. This is sometimes useful in probability theory.   	0.9828877449035645
773110	Note that  \|M_{f}Vx\| \le \frac{1}{2}\|x\| ,  \|(M_fV)(M_fV)x\|\le \frac{1}{2}\frac{1}{3}\|x\| . By induction,             \|(M_{f}V)^{k}x\| \le \frac{1}{(k+1)!}\|x\|,\;\;\; k=1,2,3,\cdots.   Therefore, if  M_fV x = \lambda x  for some  \lambda ,                         |\lambda|^k\|x\|=\|(M_{f}V)^{k}Vx\| \le \frac{1}{(k+1)!}\|x\|,\\             \|x\| \le \frac{|\lambda|^{-k}}{(k+1)!}\|x\|.   The right side of the last inequality tends to  0  as  k\rightarrow\infty  because  \sum_{k=0}^{\infty}\frac{|\lambda|^{-k}}{k!} =e^{1/|\lambda|}  converges. So  M_fV  has no non-zero eigenvectors for any  \lambda \ne 0 . And  \mathcal{N}(M_{f}V)=\{0\}  is easily verified. So  M_{f}V  has no eigenvalues. If  P_{k}x  is the projection onto the first  k  components of  x , then                \|M_{f}VP_{k}-M_{f}V\| \le \frac{1}{k}.   So  M_{f}V  is the norm limit of a sequence of finite-rank operators.  	0.9828671813011169
596636	Let  Q\in\mathbb{C}^{n\times k} ,  n\geq k , such that  Q^*Q=I  and consider the partitioning   Q=\left(\begin{array}{c}Q_1\\Q_2\end{array}\right)\begin{array}{l}\}\;k\\\}\;n-k\end{array}.   Since  Q  has orthonormal columns, for any  x\in\mathbb{C}^k  such that  \|x\|_2=1 ,   1=\|x\|_2^2=\|Qx\|_2^2=\|Q_1x\|_2^2+\|Q_2x\|_2^2.   Hence  \tag{1} \sigma_{\max}(Q_1)=\max_{\|y\|_2=1}\|Q_1y\|_2=\left(1-\min_{\|y\|_2=1}\|Q_2y\|_2^2\right)^{1/2}.   So if  Q_2  has a nontrivial nullspace, the minimum on the right-hand side is zero and hence the maximal singular value of  Q_1  is equal to 1. A sufficient condition for this is that  Q_2  has more columns than rows, that is,  k>n-k , or,  2k>n . By a more careful use of the variational characterization of singular values instead of (1), one can show that the multiplicity of this singular value is given by the dimension of the nullspace of  Q_2 .  	0.982831597328186
958762	The necessary and sufficient condition is that the  spectral radius  of  A  is less than  1 . Equivalently, there exists a positive integer  n  and a submultiplicative matrix norm  \|\cdot \|  such that  \|A^n\|<1 .     If you are looking for an easy-to-check sufficient condition, it's reasonable to consider the  induced norms  for either  \ell^1 - or  \ell^\infty - vector norms. These are    \|A\|_1 = \max_{j}\sum_{i}|a_{ij}|\quad\text{ and }\quad \|A\|_\infty = \max_{i}\sum_{j}|a_{ij}|   If either of these norms is less than  1 , then the inequality  \|Ax\|_p\le \|A\|_p\|x\|_p  (with  p  being  1  or  \infty ) yields convergence to zero.  	0.9828178286552429
879031	\begin{align} \||A|\|_2  &= \sup_{\|x\|_2 \le 1} \||A|x\|_2 \quad \mbox{(by definition of  \|\cdot\|_2  for matrices)} \\  &= \sup_{\|x\|_2 \le 1} \sqrt{\sum_{i=1}^m ||A_i|^Tx |^2} \quad \mbox{( A_i^T  is  i -th row of  A )} \\ &\le \sup_{\|x\|_2 \le 1} \sqrt{\sum_{i=1}^m \|A_i\|_2^2\|x\|_2^2} \quad \mbox{(by Cauchy-Schwarz inequality)} \\ &\le \sqrt{\sum_{i=1}^m \|A_i\|_2^2} \\ &= \sqrt{\sum_{i=1}^m\sum_{j=1}^n |A_{ij}|^2} \\ &= \sqrt{\mbox{trace}(A^*A)} \\ &= \sqrt{\sum_{i=1}^r \sigma_i^2} \quad \mbox{(by properties of trace,  \sigma_i  is singular value of  A )} \\ &\le \sqrt{r} \, \sigma_{\max} \quad \mbox{( \sigma_{\max}  is maximum singular value of  A )} \\ &= \sqrt{r} \, \|A\|_2, \quad \mbox{(follows from definition of  \sigma_{\max}  and  \|\cdot\|_2 )} \end{align} where  r  is the rank of  A , which in this case satisfies  r \le n .   	0.9827717542648315
183624	As copper.hat already pointed out, the inequality  \|f\|_p \leq \mu(E)^{1-\frac{1}{p}} \cdot \|f\|_{\infty}  does not hold for all  p \geq 1 . Instead, it should read  \|f\|_p \leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty}    This follows from  \|f\|_p \leq \mu(E)^{\frac{1}{p}-\frac{1}{r}} \cdot \|f\|_r \qquad (r>p) by letting  r \to \infty , using that  \lim_{r \to \infty} \|f\|_r = \|f\|_{\infty}  (see for a  proof  here).       Edit: Actually, there is a rather quick (and direct) proof of the inequality:  \begin{align} \|f\|_p^p &= \int_E \underbrace{|f|^p}_{\stackrel{p \geq 1}{\leq} \|f\|_{\infty}^p} \, d\mu \leq \mu(E) \cdot \|f\|_{\infty}^p \\ \Rightarrow \|f\|_p &\leq \mu(E)^{\frac{1}{p}} \cdot \|f\|_{\infty} \end{align}  	0.9827667474746704
1048185	Note that  A  has full column-rank, so its singular values are all positive.    Let  \sigma_1(A),\dots,\sigma_n(A)  denote the singular values of  A  from largest to smallest.  We note that  \|A\|_2 = \sigma_1(A)  and  \|A^+\| = 1/\sigma_{n}(A) .  Similarly,  \|A_1\|^{-1} = 1/\sigma_n(A_1) .    Thus, your statement amounts to proving that  1/ \sigma_n(A) \leq 1/\sigma_n(A_1) .  That is,    \sigma_{n} \left(\begin{bmatrix}     A_1 \\A_2 \end{bmatrix}\right) \geq \sigma_n(A_1)   It is easy to prove that this is the case using the Rayleigh-Ritz formula formula for the singular value, namely   \sigma_n(M) = \min_{x \neq 0}\frac{\|Mx\|}{\|x\|}   perhaps now you can see why this works.       We have   \sigma_{n} \pmatrix{A_1\\ A_2} = \min_{x \neq 0} \frac{\left\|\pmatrix{A_1\\A_2}x\right\|}{\|x\|} \geq \min_{x \neq 0} \frac{\|A_1x\|}{\|x\|} = \sigma_n(A_1)   	0.9827336668968201
1277426	HINT :    Consider the Sequence  D_n=\sum_{k=1}^n(BA^{-1})^n .  Since the series  \sum_{k=1}^\infty\|(BA^{-1})^n\|  converges by the  root test , the sequence  \{D_n\}  converges to some  D\in\mathcal A . Then the  C  you write in your question is  A^{-1}D .  Can you finish?    Side note:  It is not true that  r^{\text{Gelf}}(A) < 1  implies  \| A\| < 1 .  For example, consider  \mathcal A=M_2(\mathbb C)  and     A=\begin{pmatrix}0&\frac{3}{2}\\\frac{3}{8}  &0\end{pmatrix}.    Then  r^{\text{Gelf}}(A)=\frac{3}{4}<1   while  \|A\|=\frac{3}{2}>1 .  	0.9826838970184326
718457	This is an example of the  Cauchy-Schwarz inequality :    \begin{align*} \|u\|_1 &= \sum_{i = 1}^n |u_i|\cdot 1 \\ &\le \left(\sum_{i = 1}^n |u_i|\right)^{1/2} \left(\sum_{i = 1}^n 1 \right)^{1/2} \\ &= \|u\|_2 \sqrt{n} \end{align*}    To improve the result to  \sqrt{q} , use a mix of  1 's and  0 's rather than a constant sequence  1  in the second sum. I'll leave it to you to work out the details.  	0.9826732873916626
967785	Another answer is using Holder's inequality. Since  l1  in the Holder's inequality is paired with  l\infty , we can use this  to our advantage. We have  x^Ty\le ||x||_q||y||_p \quad: \quad \frac{1}{p}+\frac{1}{q}=1  by setting  p=1  we have  q=\infty  and the infinity norm is the largest value of the vector. So  ||x||_{\infty} = \max_{\forall i} |x_i| . Therefore we can say  f(x)=||x||_{\infty}  and for the convex conjugate function, we have     f^*(y) = \sup_{x\in\mathbb{R}^n} \left(y^Tx-f(x)\right) \le \sup_{x\in\mathbb{R}^n}\left(||x||_{\infty}||y||_{1}-f(x)\right) = \sup_{x\in\mathbb{R}^n}\left(f(x)||y||_{1}-f(x)\right)=\sup_{x\in\mathbb{R}^n}\left(f(x)\times(||y||_{1}-1)\right)  Apparently this function is unbounded above for an arbitrary choice of  x  unless the factored term be equal to zero. so we have this solution:  f^*(y) = \begin{cases} 0 \qquad  ||y||_{1}=1 \\ \infty \qquad O.W.\end{cases}  	0.9826626777648926
483876	I guess that this could be  shown by using the inequality  \|A\|_2\leq\sqrt{\|A\|_1\|A\|_{\infty}}  and in a more general fashion.  Let  A  be such that  \|A\|_1\leq 1 ,  b  be a vector of unit 1-norm, and let  B:=\mathrm{diag}(b) . We have   \|BA\|_2^2\leq\|BA\|_1\|BA\|_{\infty}.     Since the absolute row-sums of  A  are bounded from above by one and the components of  b=(\beta_i)  satisfy  |\beta_i|\leq 1  (because  \|b\|_1=1 ), the absolute row-sums of  BA  are bounded by one as well. Hence  \|BA\|_{\infty}\leq 1 .    The  j th column of  BA  is  c_j:=[\beta_1 a_{1j},\ldots,\beta_n a_{nj}]^T . Therefore,   \|c_j\|_1=\sum_{i=1}^n|\beta_i a_{ij}|\leq\max_{1\leq i\leq n}|a_{ij}|\|b\|_1.   From  \|A\|_1\leq 1 , we have  |a_{ij}| \leq 1  for all  i  and  j  and since  \|b\|_1=1 ,  \|c_j\|_1\leq 1 . Hence   \|BA\|_1=\max_{1\leq j\leq n}\|c_j\|_1\leq 1.     Putting this together gives   \|BA\|_2\leq 1.   	0.9826399087905884
1235708	A separable Hilbert space with orthonormal basis  \{ e_n \}  is much easier to deal with than the general case. And your problem falls into that category.    For example, if                         f : [0,1]\rightarrow\mathscr{H}     is weakly measurable, meaning that  \langle f(x),h\rangle  is measurable for all  h\in\mathcal{H} , then  \|f(t)\|^2 = \sum_{n}|\langle f(t),e_n\rangle|^2  is measurable as well. Then the requirement that  \int_{0}^{1}\|f(t)\|^2dt < \infty  gives a Hilbert norm:  \|f\|^2=\int_{0}^{1}\|f(t)\|^2dt.  You can define an integral  \int_{0}^{1}f(t)dt  as the unique vector  \int_{0}^{1}f(t)dt  such that         \int_{0}^{1}\langle f(t),x\rangle dt = \left\langle \int_{0}^{1}f(t)dt,x\right\rangle, \;\;\; \tag{*}     which makes sense by the Riesz representation theorem because         \left|\int_{0}^{1}\langle f(t),x\rangle dt\right|    \le \int_{0}^{1}|\langle f(t),x\rangle|dt\\   \le \int_{0}^{1}\|f(t)\|\|x\|dt \\   \le \left(\int_{0}^{1}\|f(t)\|^2dt\right)^{1/2}\|x\| \\    = \|f\|\|x\|.   Using this weak definition of integral, you can derive the relevant properties of the integral by reducing to the scalar case. And you automatically have property (*) above.  	0.9826149940490723
135506	It is not true. Choose  y=0  and  a_{1,m} = \frac{1}{m} . Then  y_m \to y , but it is never the case that  \|y_m\| \leq C \|y\| .    Elaboration : This is because  \|y_m\| = \frac{1}{m} \|w_1\| , the  w_i  are linearly independent, hence non-zero. Hence  \|y_m\| = \frac{1}{m} \|w_1\| > 0  for all  m . There is  no  choice of  C  that will satisfy the equation  \|y_m\| \leq C \|y\| .     The above is true even if  w_i  are orthonormal.     (I think you need to be more explicit about your choice of  a_{im} . A 'nice' choice would be to let  y_m  be the closest point to  y  in  \text{sp}\{w_i\}_{i=1}^m . This is what the first part of the answer by Berci does below.)  	0.9825807213783264
776275	If  a = \{a_n\} \in \ell^\infty , then  \|a\| = \sup\{|a_n|\} . Now   \begin{align}|f(a)| &=  \left|\sum_{n=0}^\infty \frac{(-1)^na_{n+1}}{\sqrt{n!}}\right| \\&\le \sum_{n=0}^\infty \frac{|a_{n+1}|}{\sqrt{n!}}\\&\le \sum_{n=0}^\infty \frac{\|a\|}{\sqrt{n!}} = M\|a\|\end{align}  where according to Wolfram Alpha  M \approx 3.46951  (maybe someone knows a closed form for this value, but I don't).  \sum_{n=0}^\infty (n!)^{-1/2}  can be proven to converge by the ratio test.    Obviously when  a = (1, -1, 1, -1, \ldots)  we have  |f(a)| = M\|a\| . Therefore  \|f\| = M .  	0.982578694820404
539503	I discovered my mistake. Since  \widehat{f}_{\lambda} (\xi) = \widehat{f}(\lambda \xi).  This means that I cannot apply (3) on the left hand side of (4). Instead, I have to apply,     \begin{equation} \tag{6} \label{6} \|\widehat{g}_{\lambda} \|_{s} = \lambda^{-\frac{n}{s}} \| \widehat{g} \|_{s}. \end{equation}     So that (5) becomes     \lambda^{- \frac{n}{q}} \| \widehat{f} \|_{q} \le \left( \lambda^{1/p - 1} \right)^{n} \| f \|_{p}.     Hence,     \| \widehat{f} \|_{q} \le \left( \lambda^{\frac{1}{p} + \frac{1}{q} - 1} \right)^{n} \| f \|_{p},     resulting in  q = p^{\prime}  as desired.    Let this serve as a warning to abuse of notation.  	0.9825637340545654
991649	Let  A=\begin{bmatrix} a & b/2\\  b/2 & c \end{bmatrix}  and define  \left \langle x,y  \right \rangle_{s}=x^tAy , for  x,y\in\mathbb{R}^2 . Note that  \|x\|_{s}^{2}=\left \langle x,x  \right \rangle_{s} .    Then,  \|\cdot\|_{s}  is norm iff ( \|\cdot\|_{s}\geq0   and  \|x\|_{s}=0  iff  x=0 ) iff  A  is positive-definite.  	0.9825553894042969
983643	True for the edited version.    One can assume that  \mu  is a unitary measure since one can divide by  \mu(\Omega)  and obtain a new measure where all norms will be multiplied by  \dfrac{1}{\mu(\Omega)}.  So, I shall assume  \mu  is unitary.    In this case, a direct application of Holder inequality allows concluding at once  \|f\|_1 \leq \|f\|_2 \|1\|_2 = \|f\|_2;  by translation invariance of the norm, convergence in  \mathscr{L}^2  implies convergence in  \mathscr{L}^1.  (Plus, if you divide a function  f  in  \mathscr{L}^2  as  |f| = |f|\mathbf{1}_{\{|f|\leq 1\}}+|f|\mathbf{1}_{\{|f|>1\}} \leq |f|^2+1,  once gets that  \mathscr{L}^2 \subset \mathscr{L}^1 ).  	0.9825412034988403
991609	We may scale  \mathcal{B}  so that it has radius 1, by considering  \mathcal{B}/r.  Now if  A=U\Sigma V^{*}  is the singular value decomposition of  A,  then the largest singular value of  A,   \sigma_{1}(A)=\max_{x\in\mathcal{B}/r}\|Ax\|_{2}=\max_{y\in\mathcal{Y}/r}\|y\|_{2}.  Then  \max_{z\in\mathcal{Y}}\|z\|_{2}=\max_{ry\in\mathcal{Y}}\|ry\|_{2}=r\sigma_{1}(A).    Note that this does not depend on the invertibility of  A .  	0.9825394749641418
555291	You can just complete the square in the objective function.    This optimization problem is equivalent to finding a minimizer of  \|x\|_2^2 - \frac{2}{\mu} a^T x  subject to  \|x\|_2 \leq 1 , which is in turn equivalent to finding  a minimizer of  \|x\|_2^2 - \frac{2}{\mu} a^T x + \frac{\|a\|_2^2}{\mu^2}  (subject to the given constraint). (Adding a constant term to the objective function does not change the minimizer.)    But notice that \begin{equation} \|x\|_2^2 - \frac{2}{\mu} a^T x + \frac{\|a\|_2^2}{\mu^2} = \left \|x - \frac{a}{\mu} \right \|_2^2. \end{equation} So, our optimization problem is equivalent to \begin{align} \text{minimize} & \quad \|x - \frac{a}{\mu} \|_2^2 \\ \text{subject to} & \quad \|x\|_2 \leq 1. \end{align} The solution to this is just the projection of  \frac{a}{\mu}  onto the  2 -norm unit ball.  	0.9825177788734436
215298	Since  \max_{1\leq i\leq d}|x_i|^2\leq\sum_{i=1}^d |x_i|^2\leq d\ \max_{1\leq i\leq d}|x_i|^2  one has  \|{\bf x}\|_\infty\leq\|{\bf x}\|_2\leq\sqrt{d}\>\|{\bf x}\|_\infty\qquad(x\in{\mathbb R}^d)\ .  Therefore the identity map  {\rm id}:\quad\bigl({\mathbb R}^d,\ \|\cdot\|_\infty\bigr)\ \to\ \bigl({\mathbb R}^d, \ \|\cdot\|_2\bigr)  is Lipschitz in both directions. This means that distances between points measured one way or the other differ by a factor at most  \sqrt{d} . It follows that a sequence  ({\bf x}_n)_{n\geq0}  in  {\mathbb R}^d  is Cauchy with respect to  \|\cdot\|_\infty  iff it is Cauchy with respect to  \|\cdot\|_2 .  	0.9824702739715576
1264203	The inequality is trivially true with  C=0 . What we need to prove is that there is a  C>0  for with the inequality holds. For simplicity I will assume the non-centered maximal function. Let  R>0  be such that   \int_{|x| R/2 , then   Hf(a)\ge\frac{1}{m(B_{2|a|}(0))}\int_{B_{2|a|}(0)}|f(x)|\,dx\ge C\,\|f\|_1\,|a|^{-n},   where  C>0  depends only on  n .    From here you get a lower bond on  m(\{x:Hf(x)>\alpha\})  which is of the same order of the upper bound given by the maximal theorem:   \frac{c}{\alpha}\,\|f\|_1\le m(\{x:Hf(x)>\alpha\})\le\frac{C}{\alpha}\,\|f\|_1.   This is what is meant by  essentially sharp .  	0.9824569225311279
758234	Following the hint, we have that   \|f^n\|_1 = \langle e^{i\varphi_n} , f^n \rangle_{L^2}=\langle g, \widehat{f}^n\rangle_{\ell^2} .   Here,  g\in\ell^2(\mathbb Z)  is (the Fourier transform of  e^{i\varphi_n}  and) an element of norm  1 , and  \widehat{f}^n  is a genuine  n th power. Thus, by Cauchy-Schwarz,   \|f^n\|_1^{1/n} \le \|\widehat{f}\|_{2n} ,   and we obtain the other inequality from the fact that if  a\in\ell^p  for some  p<\infty , then  \lim_{p\to\infty} \|a\|_p=\|a\|_{\infty}  (justifying the notation).    This is easy to prove (and will be in many texts). We can assume that  \|a\|_{\infty}=1 . Then   \sum_{|a_n|<1} |a_n|^p \to 0   by dominated convergence, so   1 \le \sum |a_n|^p \le C   for all large  p .  	0.9824405908584595
581003	You need to cut the study in two :    On  A=[-1,1] , you have  \|f\|_{1,A}^2 \leq 2\|f\|_{2,A}^2 \leq 2\|f\|_2^2  (Jensen inequality)    On  A^c=[-1,1]^c , you have     \|f\|_{1,A^c} = \left\| \frac{g}{x} \right\|_{1,A^c} \leq \|g\|_{2,A^c}\|\frac{1}{x}\|_{2,A^c} \leq \sqrt{2} \|g\|_2    Hence     \|f\|_1 \leq \sqrt{2}(\|f\|_2+\|g\|_2)    	0.9824376106262207
368028	As with most things the proof isn't tricky when you know how!    Define two norms for  x \in \mathbb{R^n}  as follows:     \|x\|_1 := \sum_{i=1}^n |x_i|    and      \|x\|_2 := (\sum_{i=1}^n |x_i|^2)^{1/2}.    For  x,y \in \mathbb{R^n}  I will define  xy:= x \cdot y  as a convenient short hand.     To prove your required inequality, it suffices (by the triangle inequality) to show that     \forall x,y \in \mathbb{R^n} \ \ \|xy\|_1 \leq \|x\|_2\|y\|_2.    To this end, note that     \forall a,b \in \mathbb{R}_{+} \quad 0 \leq \|(ax+by)^2\|_1 = a^2\|x^2\|_1 + 2ab\|xy\|_1 + b^2\|y^2\|_1.    Dividing through by  b^2 , and setting  \lambda:= a/b  results in this inequality:    \forall \lambda>0 \quad 0 \leq \lambda^2\|x^2\|_1 + 2\lambda\|xy\|_1 + b^2\|y^2\|_1.    We can conclude that there are no positive real roots of this quadratic in  \lambda . Therefore, we know that the determinant must be non-positive, i.e.     (2\cdot \|xy\|_1)^2 \leq 4\|x^2\|_1 \|y^2\|_1.    Dividing both sides by  4  and taking square roots gives:    \|xy\|_1 \leq \|x^2\|_1^{1/2} \|y^2\|_1^{1/2} = \|x\|_2\|y\|_2,      as required.   	0.9824094176292419
288879	When  A  and  B  are square matrices, the inequality is true for every matrix norm (one that satisfies  \|AB\|\le \|A\|\,\|B\| .) Indeed,    \operatorname{cond}(AB)=\|AB\|\,\|(AB)^{-1} | \le \|A\|\,\|B\|\,\|B^{-1}\|\,\|A^{-1} \| =\operatorname{cond}(A)\,\operatorname{cond}(B)    If  A  and  B  are non-square, then  A^{-1}  is not meaningful, and the condition number has to be defined differently. The one definition I know for this case (which agrees with the above when the operator norm is used), is    \operatorname{cond}(A)=\frac{\sigma_1(A)}{\sigma_n(A)} = \frac{\max\{|Ax|:|x|=1\}}{\min \{|Ax| : |x|=1\}}   (Here  \sigma_1  and  \sigma_n  are the greatest and smallest singular values of  A , defined in the quotient on the right). This definition is of interest only when the kernel is trivial. The submultiplicative inequality still holds, because  \sigma_1(AB)\le \sigma_1(A)\sigma_1(B)  and  \sigma_n(AB)\ge \sigma_n(A)\sigma_n(B)  .  	0.9823948740959167
691685	I am assuming that  x \in \mathbb{R}^n .    \|\Sigma x\|_\infty \ge {1 \over \sqrt{n}}\|\Sigma x\|_2 \ge {1 \over \sqrt{n}}  \sigma_\min(\Sigma) \|x\|_2 \ge {1 \over \sqrt{n}}  \sigma_\min(\Sigma) \|x\|_\infty .    Here  \sigma_\min(\Sigma)  is the smallest eigenvalue of  \Sigma  (which is positive semi definite).  	0.9823891520500183
806138	The answer to this is yes.  In order to prove it, note that   \|Ax\|_\infty = \left \| \pmatrix{(e_1^H A)x\\ \vdots \\ (e_m^H A)x} \right\|_\infty = \max_{i=1,\dots,m} \left|(e_i^H A)x\right| \leq \\ \max_{i=1,\dots m } \|(e_i^H A)\|_1 \cdot \|x\|_\infty   The inequality comes from the general result  |u^Tv| \leq \|u\|_1 \|v\|_\infty , which you may consider as an instance of Hölder's inequality.  In any case, the proof of the inequality is pretty quick (but it requires expanding the dot-product as a sum, which I'm too lazy to do).    From there, it is necessary to show that this upper bound is attained.  That is, it is necessary to find an  x  with  \|x\|_\infty = 1  and  \|Ax\|_\infty = \|A\|_\infty . To that effect, let  i  be the row of  A  with the highest  1 -norm, and consider  x  to be given by   x_j = \frac{|e_iAe_j|}{e_iAe_j} = \frac{|A_{ij}|}{A_{ij}} \quad j = 1,\dots,n   (in the case that  A_{ij} = 0 , set  x_j = 1 ).  	0.9823685884475708
698265	Denote  B=AA_1^{-1}=\left[\matrix{I\\A_2A_1^{-1}}\right] . A proof strategy (among several others) could be      Show that  B^*B-I  is positive-semidefinite. It would mean that the eigenvalues  \lambda_i(B^*B)\ge 1  and, hence, the singular values  \sigma_i(B)\ge 1 .   Show (e.g. via SVD) that  \|B^{+}\|_2=\frac{1}{\sigma_\min(B)}\le 1 .   Show that  B^{+}=A_1A^{+} .   Estimate   \|A^{+}\|_2=\|A_1^{-1}A_1A^{+}\|_2\le\|A_1^{-1}\|_2\|A_1A^{+}\|_2\le \|A_1^{-1}\|_2.     	0.9823487401008606
537410	One way of proving that norms give the same topology is to find inequalities     c||x||_1\leq ||x||_2\leq C||x||_1\ \ \ \ \text{ with }c>0.    \text{If }\ \ q>p\geq1\ \ \ \ \ \ \ \ \ \text{ then }\ \ \ \ \ \ \ \ \ \frac{n^{1/q}}{n^{1/p}}||x||_p\leq||x||_q\leq||x||_p.    The first inequality is the  generalized mean inequality  and the second is because     \frac{||x||_p}{||x||_q}=\underbrace{\left|\left|\frac{1}{||x||_q}x\right|\right|_p\geq\left(\sum_i\frac{|x_i|^q}{||x||_q^q}\right)^{1/p}}_{q>p\implies a^q\leq a^p\text{ for }|a|<1.}=1^{1/p}=1\ \ \ \ \text{ because }\ \ \ \ \frac{|x_i|}{||x||_q}\leq1.    Now, such a pair of inequalities allows you to, for every open ball in one norm to put an open ball from the other norm inside. Suppose  B_p(a,r)  is a ball of radius  r  in the  p -norm, then  B_p(a,\frac{n^{1/p}}{n^{1/q}}r)\supset B_q(a,r)\supset B_p(a,r).  	0.9823240637779236
993075	Let's denote  H=\partial^2 f  and  v=x-a . Your question why (I assume you are talking about the spectral norm)  \|H\|\le\lambda_{\max}  is true can be answered as, in fact,   \|H\|=\lambda_{\max}   for  Hermitian matrices . It can be proved via unitary diagonalisation of Hermitian matrix   H=UDU^*   where  D=\text{diag}\{\lambda_1,\lambda_2,\ldots,\lambda_n\}  is the diagonal matrix of eigenvalues and  U  is unitary, because  \|H\|=\|D\|=\lambda_{\max}  (the spectral norm is unitary invariant and the norm of the diagonal matrix is an easy exercise).    It makes the answer to your second question trivial as  \|H\|=\lambda_{\max}\ge\lambda_{\min} . What is more interesting is to take a bit better estimations for   \langle Hv,v\rangle.    Using the property of  Rayleigh quotient  it is easy to conclude that   \lambda_{\min}\|v\|^2\le\langle Hv,v\rangle\le\lambda_{\max}\|v\|^2,   which fits your needs better.  	0.9823108911514282
712355	I hope you don't mind I rename your matrix  A , for notational laziness. :)    ||A||=\max_{||x||=1}{||Ax||}    Now, for  x=\left(\begin{array}{c}x_1\\\vdots\\ x_k\end{array}\right) , we have:  Ax=\left(\begin{array}{c}0\\x_2\\\vdots\\ x_k\end{array}\right),  Now comparing  ||Ax||  and  ||x|| , or  ||\left(\begin{array}{c}0\\x_2\\\vdots\\ x_k\end{array}\right)||\ \mbox{ and }\ ||\left(\begin{array}{c}x_1\\\vdots\\ x_k\end{array}\right)||,  it follows that  ||Ax||\leq||x|| , such that   ||A||=\max_{||x||=1}{||Ax||}\leq\max_{||x||=1}{||x||}=1.    Note that whitout specifying which norm  ||\cdot||  is, we cannot arrive at  ||A||<1 , but only at  ||A||\leq1 , for example  ||A||_1=1 .  	0.9822985529899597
886897	You have the Kato-Ponce inequality. Here's a reference paper  https://arxiv.org/pdf/1303.5144v1.pdf .     Basically, you have     \begin{align} \|D^s(fg)\|_{L^r} \lesssim \|D^sf\|_{L^{p_1}} \|g\|_{L^{q_1}}+\|f\|_{L^{p_2}} \|D^s g\|_{L^{q_2}} \end{align} where \begin{align} \frac{1}{r} = \frac{1}{p_i}+\frac{1}{q_i}. \end{align}    Note: You have fractional derivative.   	0.9822973608970642
929907	Consider a QR factorization of  A  in the form   A=\begin{bmatrix} A_1\\A_2 \end{bmatrix} = \begin{bmatrix} Q_1\\Q_2 \end{bmatrix} R=:QR   where  Q  has orthogonal columns and is partitioned in the same way as  A . The matrix  A  has full column rank so  R  is nonsingular.    We have  A^+=R^{-1}Q^*  and hence  \|A^+\|_2=\|R^{-1}\|_2 . We have to show that  \|R^{-1}\|_2\leq\|A_1^{-1}\|_2 . From  A_1=Q_1R , we get  R^{-1}=A_1^{-1}Q_1  and    \|R^{-1}\|_2\leq\|Q_1\|_2\|A_1^{-1}\|_2.   But  Q_1  is a submatrix of  Q  so  \|Q_1\|_2\leq\|Q\|_2=1 .  	0.9822332859039307
549755	First, if  P  is a  nontrivial  projection ( P\neq 0 ), any submultiplicative norm of  P  is bounded from  below  by one. This follows simply from the fact that  P  is idempotent and  \|P\|=\|P^2\|\leq\|P\|^2  which gives  \|P\|\geq 1 . Consequently, if  \|\cdot\|  is the matrix norm induced by the vector norm induced by the scalar product w.r.t. which  P  is orthogonal, then either  \|P\|=0  or  \|P\|=1  with the first option possible if and only if  P=0 .    The only way how to make a norm of a nontrivial projection smaller than one is to use a matrix norm which is not submultiplicative. This excludes, e.g.,  p -norms, Frobenius norm, and any matrix norm induced by a vector norm. On the other hand, some matrix norms are not submultiplicative; e.g., the  \max -norm  \|P\|_\max:=\max\limits_{i,j}|p_{ij}| .    E.g., with (stealing the  P  from Omnomnomnom)   P=\frac{1}{5}\pmatrix{1&2\\2&4},     \|P\|_2=1, \quad \|P\|_1=\|P\|_\infty=\frac{6}{5}>1, \quad \|P\|_\max=\frac{4}{5}<1.   	0.9822291731834412
1116274	"Using H\""older's inequality, we can prove that these norms are equivalent. Let  1\le p\le q\le +\infty . Then   \|Ax\|_p \le n^{1/p-1/q} \|Ax\|_q \le n^{1/p-1/q} \|A\|_q\|x\|_q\le n^{1/p-1/q} \|A\|_q\|x\|_p,    hence   \|A\|_p \le n^{1/p-1/q}\|A\|_q.   Similarly,   \|A\|_q \le m^{1/p-1/q}\|A\|_p.   Equality holds in one of the inequalities if  A  contains a column (row) of all ones, with the remaining entries zero:   A=\pmatrix{1& 0 & \dots & 0 \\ \vdots &\vdots&&\vdots\\1& 0 & \dots & 0}.   Take  x\ne0 , then  \|A\|_p = |x_1| n^{1/p} . Taking the supremum over  \|x\|_p\le 1  yields  \|A\|_p = n^{1/p} . And the first inequality holds with equality.    Equality holds in both inequalities for  p\ne q  only if  n=m  for the same matrix  A  as above.  "	0.9822152853012085
538214	Hint:  it suffices to show that the lowest eigenvalue of  A - B  is positive.  Note that for a symmetric matrix  M , the lowest eigenvalue can be expressed as   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx   and that   \|M\| = \max_{\|x\| = 1} \left|x^TMx \right|        I've combined several ideas to get that last equality.  From the usual results regarding the  Rayleigh quotient , we have   \lambda_{min}(M) = \min_{\|x\| = 1} x^TMx \\ \lambda_{max}(M) = \max_{\|x\| = 1} x^TMx   For arbitrary matrices, we have   \|A\| = \max_{\|x\| = 1}\|Ax\|= \sqrt{\max_{\|x\| = 1} x^TA^TAx}   So, for symmetric  M , we have   \|M\| = \sqrt{\max_{\|x\| = 1} x^TM^2x} = \sqrt{\lambda_{max}(M^2)}   Note, however, that the eigenvalues of  M^2  are simply  \lambda^2  for all eigenvalues  \lambda  of  M .  Thus, we have   \|M\| = \sqrt{\lambda_{max}(M^2)} = \max\{|\lambda|: \lambda \text{ is an eigenvalue of } M\} = \max_{\|x\| = 1} \left|x^TMx \right|   	0.982191801071167
1181665	"The shortest proof I can think of is the following:      Prove that  \|\cdot\|_{[k]}  and  \|\cdot\|_{[k]}^*  are norms (easy be definition).   Use the LP problem suggested by Michael Grant that    \|x\|_{[k]}=\max\{x^Tz\colon \|z\|_1\le k,\,\|z\|_\infty\le 1\}   and since  \|z\|_1\le k   \Leftrightarrow   \frac{1}{k}\|z\|_1\le 1  rewrite it as   \|x\|_{[k]}=\max\{x^Tz\colon \|z\|_{[k]}^*\le 1\}.\tag{1}    The relation (1) means by definition that  \|\cdot\|_{[k]}  is the dual of  \|\cdot\|_{[k]}^* . Taking dual once again: the dual of  \|\cdot\|_{[k]}  is the second dual of  \|\cdot\|_{[k]}^* , which is the same as  \|\cdot\|_{[k]}^* .         P.S. The last fact ""the second dual norm is the norm itself"" is a known result in finite dimensional spaces (even in reflexive Banach spaces). A proof normally uses separation theorems of convex sets (alternatively Hahn-Banach theorem). It can be found in e.g. Horn, Johnson, Matrix Analysis, Ch. 5, Sec. 5.5.  "	0.9821901321411133
1019709	We can get a quick upper bound as follows: let  \|x\|  denote the norm (Euclidean length) of a vector, and denote   \|(x_1,\dots,x_n)\|_\infty = \max_{i=1,\dots,n}|x_n|   It is well known that    \max_{x \neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty} = \max_{i=1,\dots,n}\sum_{j=1}^n |a_{ij}|    Thus, for a stochastic matrix  A , we have   \sigma_1(A) = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|} \leq \max_{x \neq 0} \frac{\sqrt{n} \|Ax\|_\infty}{\frac 1{\sqrt{n}}\|x\|_\infty} = n \max_{i=1,\dots,n}\sum_{j=1}^n |a_{ij}|  = n   I'm not sure if this bound is tight.       We can see that it is possible to get a maximum singular value of at least  \sqrt{n} .  In particular, it suffices to consider  A = \mathbf 1 e_1^T  where  \mathbf 1  is the column-vector of  1 s and  e_1  is the standard basis vector  (1,0,\dots,0)^T .   	0.9821845293045044
504483	My proof below is incomplete: for operators, positivity does not mean invertibility.  I believe, however, that that case in which the infimum is zero can be handled separately.       I'm not sure how you proved the statement about the sup without the spectral theorem, but that'll be enough for the opposite statement.    Note that since  A  is positive, it is invertible, and  A^{-1}  is positive.  We have   \sup_{\|x\| = 1} \|A^{-1}x\| = \sup_{\|x\| = 1}|\langle x,A^{-1}x\rangle|   from there, we note that   \inf_{\|x\| = 1} \|Ax\| = \left[\sup_{\|x\| = 1} \frac{1}{\|Ax\|}\right]^{-1} = \left[\sup_{x \neq 0} \frac{x^*x}{x^*AAx}\right]^{-1/2} =(\text{set }y = Ax)\\ \left[\sup_{x \neq 0} \frac{y^*A^{-1}A^{-1}y}{y^*y}\right]^{-1/2} = \left[\sup_{\|y\|=1}\|A^{-1}y\|\right]^{-1}   Similarly, show that   \inf_{\|x\| = 1}|\langle x,A x\rangle| = \left[\sup_{\|y\| = 1}|\langle y,A^{-1}y \rangle| \right]^{-1}   the statement follows.  	0.9821799993515015
1075848	Okay, I got both parts:    Invertibility follows since if there is some  w \neq 0  such that  Lw = 0 , then  \|Lw \| = 0  but  \|w\| \neq 0 , which contradicts the given inequality.  To show the bound, write the definition of the operator norm:     \| L^{-1} \| = \sup_{v \neq 0} \frac{\|L^{-1}v\|}{\|v\|} = \sup_{Lw \neq 0} \frac{\|L^{-1}(Lw)\|}{\|Lw\|} = \sup_{Lw \neq 0} \frac{\|w\|}{\|Lw\|} \leq \sup_{Lw \neq 0} \frac{1}{C} = \frac{1}{C}.      The only trick is to notice that we can write any  v  in the range of  L  as  v = Lw  (by definition).  Then we just crank away until we can apply the given inequality to get the inverse bound.  	0.9821761846542358
